text_id,text_level,text
1,C2,"Some time ago, a website talked about the dangers of public check-ins, which are online announcements of where you are. The website's message was clear: you might think you are just telling people, ""Hey, I'm here,"" but you are also letting everyone know you are not at home. This made people more aware that sharing too much online can have negative effects. The internet gives us many chances to share our lives with a global audience, offering possibilities like wealth and fame. So, we dive into the internet, sharing personal stories and photos. But soon, we realize that the online world can be dangerous and confusing.
This might sound depressing, but don't lose hope. There is a guide to this future, created by early internet users. In the early days of the web, these pioneers explored the internet and faced many challenges. They lost jobs, made and lost friends, and dealt with the risks of being famous, all before social media existed. These early bloggers have experienced what many of us are going through now. We should learn from their experiences to avoid repeating their mistakes.
In January 1994, Justin Hall, a 19-year-old student, started posting on the ""WWW,"" which was mostly used by students and scientists. The web was invented at CERN in Switzerland to help researchers share their work. Hall saw it as a chance to share his life. He created a detailed online autobiography with text, photos, and art. In January 1996, he started a daily blog, and many people were interested in his bold use of this new medium. Hall shared everything, and no topic was off-limits. While some saw him as an exhibitionist, others recognized the artistic value of his work.
One day, visitors to Hall's site found it replaced with a video called ""Dark Night."" He had fallen in love, but when he wrote about it online, he was told, ""either the blog goes, or I do."" He realized that sharing his life online made people not trust him. He stopped blogging, but the problem of sharing too much online remains. Sharing online is fun, but if you expect it to make people like you, you might be disappointed.
In 2002, Heather Armstrong, a web worker in Los Angeles, had a blog called Dooce. She sometimes wrote about her job at a software company. One day, a colleague sent her blog to the company's vice presidents, including some she had criticized, and she lost her job. Experts call this the ""online distribution effect,"" where people feel they can say things online they wouldn't say in person. But the internet is not a separate reality where we can say anything without consequences. Our online and real lives are connected, and ignoring this can lead to serious mistakes.
Armstrong's story ended well. Although she was upset and stopped blogging for a while, she got married and restarted her blog, focusing on her family. Now, she is a successful ""mommy blogger,"" and her writing supports her family. She learned that just because the internet allows us to say anything, it doesn't mean we should."
2,C2,"Some time ago, scientists started a campaign to warn people about a low-budget film called ""What the Bleep Do We Know?"" This film mixed documentary and drama to show that there is a lot about our universe we don't understand. Most scientists agree that we don't have all the answers about the universe. However, some scientists felt the need to warn the public about the film, calling it ""atrocious"" and ""a very dangerous piece of work."" This made the film seem interesting to watch. 
At first, the film seemed harmless, with scientists saying that new discoveries show the universe is stranger than we thought. But then the film talked about discoveries like water molecules being affected by thoughts. It mentioned a Japanese researcher who claimed that the shape of a water molecule could change just by the thoughts of people around it. The film showed pictures of ice crystals looking nice after being talked to by someone happy and looking bad after being exposed to someone in a bad mood. Some people find this evidence convincing, but many scientists are skeptical and say, ""Give me a break."" 
The idea that thoughts can affect water is amazing and could mean new forces are at work in the universe. But we need strong proof that this effect is real, and pretty pictures of crystals are not enough. The real issue is that the film's claims were not strange enough. Consider this: water molecules might have properties due to energy that comes from nowhere, linked to a force driving the universe's expansion. This idea is supported by decades of research in labs and observatories worldwide.
Recent discoveries show that the universe is stranger than we believed. Astronomers found that the universe is made of unknown matter and is driven by a mysterious force called ""dark energy."" On a more everyday level, neuroscientists found that our perception of events is delayed by about half a second, but our brains edit this out. Anthropologists think they have found where modern humans originated and how they spread across the world. Some theorists even suggest links between life on Earth and the universe's design.
Science is far from complete. We are further from knowing everything than ever before. Concepts like chaos and quantum uncertainty limit what we can know. Some top physicists are working on a Theory of Everything to explain all forces and particles in the universe with one equation. Despite theories, many people think the universe can be described in one word: incredible."
3,C2,"'In simple terms, I find writing novels challenging and writing short stories enjoyable. If writing novels is like planting a forest, then writing short stories is like planting a garden. These two activities complement each other, creating a complete landscape that I value. The trees provide shade, and the wind moves the leaves, which sometimes turn a bright gold. In the garden, flowers bloom, and their colorful petals attract bees and butterflies, showing the change of seasons. Since I started my career as a fiction writer, I have alternated between writing novels and short stories. My pattern is this: after finishing a novel, I want to write short stories; after completing short stories, I focus on a novel. I never write short stories while working on a novel, and vice versa. These two types of writing might use different parts of the brain, and it takes time to switch from one to the other. I began my career with two short novels in 1975, and from 1984 to 1985, I started writing short stories. I knew little about writing short stories then, so it was difficult, but the experience was memorable. I felt my fictional world expand. Readers seemed to appreciate this side of me as a writer. One of my early works, ‘Breaking Waves’, was in my first short-story collection, Tales from Abroad. This was my start as a short-story writer. One joy of writing short stories is that they don’t take long to finish. It usually takes me about a week to shape a short story (though revisions can be endless). It’s not like the long commitment needed for a novel, which can take a year or two. You go into a room, finish your work, and leave. That’s it. Writing a novel can feel like it goes on forever, and I sometimes wonder if I’ll make it through. So, writing short stories is a necessary change of pace. Another nice thing about short stories is that you can create them from small things – an idea, a word, an image. It’s often like jazz improvisation, with the story leading me. Also, with short stories, you don’t have to worry about failing. If the idea doesn’t work, you just move on. Even great writers like F. Scott Fitzgerald and Raymond Carver didn’t always write masterpieces. This is comforting. You can learn from your mistakes and use that in the next story. When I write novels, I try to learn from the successes and failures of my short stories. In this way, short stories are like an experimental lab for me as a novelist. It’s hard to experiment in a novel, so without short stories, writing novels would be more difficult. My short stories are like soft shadows I have set out in the world, faint footprints I have left behind. I remember where I wrote each one and how I felt. Short stories are like guideposts to my heart, and it makes me happy to share these feelings with my readers.'"
4,C2,"Science can be very abstract, like philosophy, or very practical, like curing diseases. It has made our lives easier but also posed threats. Science tries to understand everything from tiny atoms to the vast universe, but it often struggles. It influences poets, politicians, philosophers, and even tricksters. Its beauty is clear to experts, but its dangers are often misunderstood. People sometimes overestimate or underestimate its importance, and its mistakes are either ignored or exaggerated. Science is full of conflicts. Old theories are often changed or replaced by new ones, similar to how new music styles are first mocked but later accepted. Scientists can be competitive and emotional. This book will explore how scientific ideas have changed not just science but also human thought. We will focus on ideas, not practical inventions like non-stick pans. Science is always changing. Scientists often challenge each other's ideas. Usually, these changes don't affect society much, but sometimes they cause big shifts in our beliefs. For example, in the 17th century, science described the universe as a giant clock. Later, physics showed that our observations can affect the universe. Some people think science can't fully explain the universe, but scientific changes usually help us understand and predict nature better. Isaac Newton explained more than Aristotle, and Albert Einstein explained more than Newton. Science makes mistakes but keeps progressing. At the end of the 19th century, many thought physics was complete, but then came many new discoveries like radioactivity and quantum mechanics. Biology has also made many advances. Some now claim we are close to a complete theory of everything. Science is not just a hobby. In the last 200 years, we have started to control nature, sometimes upsetting its balance. We must pay attention to science. Non-scientists need to understand scientific advances because they affect our future and our children's future. Science is now part of how we plan and shape our future. This is not just a philosophical question; it affects national budgets, health, and even life on Earth."
5,C2,"From around 2015, after years of growth, major publishers noticed that ebook sales had stopped increasing, or even decreased in some cases. This raised new doubts about the long-term future of ebooks in the publishing industry. One publishing executive admitted that the excitement about ebooks might have led to poor investments, with his company losing faith in 'the power of the printed word'. Despite the clear idea that digital and print can exist together, people still wonder if ebooks will 'kill' print books. Whether predicting or dismissing this, the idea of books disappearing continues to capture our imagination and spark debate. Why is this idea so strong? Why do we see ebooks and print books as being in conflict, even if evidence suggests otherwise? The answers go beyond ebooks and reveal our mixed feelings of excitement and fear about innovation and change. In my research, I have shown that the idea of one medium 'killing' another often follows new technologies. Even before digital tech, people predicted the end of existing media. For example, after TV was invented, many thought radio would die, but radio survived by finding new uses, like in cars and factories. The idea of books disappearing isn't new either. As early as 1894, people thought the phonograph would replace books with what we now call audiobooks. This pattern has repeated with movies, radio, TV, and smartphones, all thought to destroy print books at some point. Some feared this would lead to cultural decline, while others exaggerated the benefits of ebooks. The idea of the book's death often appears during technological change, reflecting our hopes and fears about such changes. We form emotional bonds with media as they become part of our lives. Studies show we connect closely with objects like books, TVs, and computers, sometimes even naming our cars or getting angry at our laptops. So, new technology like e-readers doesn't just mean economic and social change; it also affects our bond with familiar things. As technology advances, we miss what we used to know. This is why industries develop around retro products and old technologies. For example, the printing press in 15th-century Europe made people seek original manuscripts. The move from silent to sound movies in the 1920s created nostalgia for silent films. The same happened with the shift from analog to digital photography and from vinyl records to CDs. E-readers have made people appreciate the physical quality of 'old' books, even their smell. This should reassure those worried about print books disappearing. Yet, the idea of a disappearing medium will continue to be a compelling story about technology's power and our resistance to change. We use familiar stories, like tragedy and endings, to understand change. The story of the book's death reflects our excitement for the future and our fear of losing parts of our world and ourselves."
6,C2,"'For nearly a year and a half, I woke up at 5:30 every weekday, brushed my teeth, made coffee, and wrote about how some of the greatest minds of the past 400 years managed their daily routines. I wanted to see how they found time to be creative and productive. By looking at the simple details of their daily lives—when they slept, ate, worked, and worried—I aimed to show a new side of their personalities and careers. The French gastronome Jean Anthelme Brillat-Savarin once said, ‘Tell me what you eat, and I shall tell you what you are.’ I say, ‘Tell me what time you eat, and if you take a nap afterward.’ This book is about the conditions for creativity, not the final product. It’s also personal. The novelist John Cheever believed that even a business letter reveals something about your inner self. My book explores questions I face in my own life: How do you do meaningful creative work while earning a living? Is it better to focus entirely on a project or to work on it a little each day? When time is short, do you give up things like sleep or income, or can you learn to do more in less time? I don’t claim to answer these questions, but I provide examples of how successful people have faced similar challenges. I wanted to show how big creative ideas come from small daily efforts and how work habits affect the work itself. The book is called Daily Rituals, but it’s really about routines. A routine might seem ordinary, but it can help you use your limited resources like time, willpower, and optimism. A good routine can guide your mental energy and help you avoid being controlled by your moods. Psychologist William James thought that having good habits could free your mind for more interesting things. Ironically, James himself was a procrastinator and couldn’t stick to a schedule. This book started from my own procrastination. One Sunday, I was at work trying to write a story due the next day. Instead of working, I was tidying my desk and making coffee. I’m a ‘morning person’ and can focus well in the morning but not after lunch. To feel better, I searched online for other writers’ schedules. I found them interesting and thought someone should collect these stories. That’s how I started the Daily Routines blog and later, this book. The blog was simple; I posted descriptions of people’s routines from biographies and articles. For the book, I expanded and researched more, keeping the variety of voices. I let my subjects speak through quotes from letters, diaries, and interviews. In other cases, I summarized their routines from other sources. This book wouldn’t be possible without the work of many biographers, journalists, and scholars. I’ve listed all my sources in the Notes section, which can guide further reading.'"
7,C1,"Howard became a palaeontologist because of a change in interest rates when he was six years old. His father, who was careful with money and had a big mortgage, decided they couldn't afford a holiday to Spain. Instead, they rented a chalet on the English coast. There, on a rainy August day, Howard found an ammonite* on the beach. From that moment, he knew he wanted to be a palaeontologist. By the end of his university studies, he knew what kind of palaeontologist he wanted to be. He was interested in the earliest times, not in dinosaurs or the Jurassic period. He studied ancient, delicate creatures found in grey rocks. 
When he finished his doctoral thesis, he worried about finding a job, especially in the type of institution he wanted. He was confident in his abilities but knew that deserving something doesn't always mean getting it. When a job as an Assistant Lecturer at Tavistock College in London was advertised, he applied, though he wasn't very hopeful. On the day of his interview, the professor who was supposed to support another candidate couldn't attend because of an accident. The replacement professor disliked the absent professor and made sure Howard got the job. Howard was surprised by the unexpected support but later learned the real reason. He was a little disappointed but was mainly happy to have the job he wanted.
Howard often thought about how his professional life was orderly, with clear plans and actions, unlike the chaos of personal life. He realized how strangers could change your life, like when his briefcase with lecture notes was stolen at an Underground station. Angry, he returned to the college, postponed his lecture, and reported the theft. While having coffee, he met a visiting curator from the Natural History Museum in Nairobi. He learned about a new collection of fossils that needed study, which would be his biggest challenge and secure his career. If not for the theft, he wouldn't have known about this opportunity. He quickly changed his plans, deciding not to attend a conference in Stockholm or take students on a field trip to Scotland. Instead, he focused on finding funds to visit the museum in Nairobi."
8,C1,"Charles Spence is willing to try almost any food. ""We have ice cream made from bee larvae at home,"" says the Professor of Experimental Psychology at Oxford University in the UK. Although they look like maggots, they have a ""slightly nutty, floral"" taste. Making bug-eating acceptable is just one of the challenges Spence and his team are working on. Through his research on how our senses combine to create our perception of flavor, Spence is quietly influencing what we eat and drink. This includes the products of large food companies (he advises one well-known multinational and receives funding from another) and the menus of top restaurants. Spence and his colleagues study how we experience food and drink, a field informally known as gastrophysics. Factors like who we eat with, how food is presented, and even background noise can affect taste. Spence's book, ""The Perfect Meal,"" co-written with Betina Piqueras-Fiszman, offers many interesting insights for anyone who enjoys eating. Did you know that the first person to order in a restaurant usually enjoys their meal the most? Or that we eat about 35% more when dining with one other person, and 75% more with three others? Spence's lab in Oxford is simple and low-tech. There are soundproof booths and old audio-visual equipment. By keeping costs low, he can work creatively with chefs who can't afford academic research. Much of his work is funded by a major food company. In the past, research funded by industry was seen as less serious in universities. But now, since the government wants universities to show their work has an impact, this type of research is valued. Spence is helping famous brands reduce salt and sugar, often due to government rules. Companies do this gradually so customers don't notice the change. ""Research shows that if you tell people about the change, they focus on the taste and don't like it as much,"" he says. Spence first met Heston Blumenthal, a famous experimental chef, while working on a project for a major food producer. ""At the time, people thought science and food didn't mix, but most food is scientific. Who better to change that view than Heston?"" Their collaboration led to the ""Sound of the Sea"" dish at Blumenthal's restaurant. Interestingly, Spence notes that the Italian futurists were experimenting with sound and food a century ago, but it didn't become popular. Now, the food industry is using Spence's sensory science widely. For example, his research shows that high-pitched music enhances sweetness, while low-pitched sounds make food taste bitter. ""It's surprising when shapes affect taste, or when music changes how you perceive flavor,"" he says. An airline will soon match music with food served to passengers. Last year, a well-known brand released a smartphone app that played music while your ice cream softened, but they didn't match the music to the taste, which Spence says happens often. One wonders what dinner parties are like at Spence's home. Once, they ate rabbit with the fur wrapped around the cutlery. Another time, they used remote-controlled, multi-colored light bulbs. ""We've had dinner parties with a tone generator, headphones, and 10 different drinks to see if they have different pitches."" For Spence, home, shops, food conventions, and international gastronomy conferences are all extensions of his lab."
9,C1,"Our brains are busier than ever. We are bombarded with facts, false facts, chatter, and rumours, all pretending to be information. We have to sort through this to find what we need and what we can ignore. At the same time, we are doing more tasks ourselves. Thirty years ago, travel agents booked our flights, and salespeople helped us in shops. Now, we do most things ourselves. We are doing the work of many people while trying to manage our lives, families, careers, hobbies, and TV shows. Smartphones help us fit as much as possible into every spare moment. But there is a problem. We think we are multitasking, doing several things at once, but this is a dangerous illusion. Earl Miller, a neuroscientist at MIT, says our brains are not designed to multitask well. When we think we are multitasking, we are actually switching quickly from one task to another, which has a mental cost. We are not expert jugglers; we are more like amateur plate spinners, quickly switching tasks and worrying about dropping one. Multitasking makes us less efficient. It increases stress hormones, which can cause mental fog. Multitasking also creates a craving for new things, distracting us. The prefrontal cortex, the part of the brain we need to focus, is easily distracted by new things. Just having the chance to multitask can hurt our thinking. Glenn Wilson, a former professor of psychology, calls it info-mania. His research shows that trying to focus on a task while an email is unread can lower your IQ by almost 10 points. This loss is greater than the loss from being tired. Russ Poldrack, a neuroscientist at Stanford, found that learning while multitasking sends information to the wrong part of the brain. For example, if students do homework and watch TV, the information goes to the striatum, which stores new skills, not facts. Without TV, the information goes to the hippocampus, where it is better organized. Multitasking also involves decision-making, which is hard on our brains. Small decisions use the same brain resources as big ones, leading to poor decisions. In discussions with business leaders and scientists, email is often mentioned as a problem. It is not the email itself, but the overwhelming amount of communication. A colleague's 10-year-old son said his father ""answers emails"" for a living, which is not far from the truth. We feel we must reply to emails, but it seems impossible to do so and get other work done."
10,C1,"In a Swedish zoo, a chimpanzee named Santino spent his nights breaking concrete into pieces to throw at visitors during the day. Was he being spiteful? In caves in the US, female bats help unrelated fruit bat mothers if they can’t find the right birthing position. Are they being caring? Fifty years ago, these questions would have been seen as irrelevant. Animals had behaviors, these behaviors produced measurable outcomes, and science recorded those outcomes. The idea that animals have consciousness, feelings, and moral systems was considered sentimental. But recently, this has started to change. Research into the behavior of bats, chimps, rats, dolphins, and chickens has begun to explore animal emotions. This change has reached popular science books, like Mark Bekoff’s ""Wild Justice"" and Victoria Braithwaite’s ""Do Fish Feel Pain?"". This has started a debate: can animals be said to have consciousness? This debate leads to another one: not about consciousness, but conscience – a sense of right and wrong that guides behavior. In a recent experiment with cows that had to open a locked gate to get food, those that opened the gate themselves showed more pleasure – by jumping and kicking their legs – than those that had the gate opened for them. If cows enjoy problem-solving, what does it mean for beef production and consumption? The observations may not be disputed, but their interpretation is. Dr. Jonathan Balcombe, author of ""Second Nature,"" believes the only logical response to this research is to stop eating meat. He thinks humanity is on the verge of a major ethical revolution. Aubrey Manning, Professor Emeritus at Edinburgh University, believes we should at least re-evaluate our view of animal cognition. He thinks animals have a simpler theory of mind than humans. Professor Euan MacPhail believes we should stop anthropomorphizing. The three may never agree because the issue is not just scientific or moral, but philosophical. Given that defining consciousness is difficult, can we ever know, as philosopher Thomas Nagel asks, what it is like to be a bat? Balcombe describes an experiment that seems to show starlings – a type of bird – can get depressed. At Newcastle University, starlings were split into two groups. Half were in luxurious cages, with plenty of space and water. The other half were in small, barren cages. Initially, both groups were fed tasty worms from one box and unpleasant worms from another, and soon learned to take only from the tasty box. But when offered only unpleasant worms, only the birds in luxurious cages would eat. Balcombe concluded that being in a bad cage made the starlings pessimistic. Balcombe, who has worked with animal rights groups, has a clear bias. ‘We look back with horror on an era of racism,’ he says. ‘Our view about animals will someday be the same. We can’t support animal rights while eating a cheeseburger.’ If he were the only one with this view, it might be easy to dismiss him as extreme. But Professor Aubrey Manning shares his view. Manning has written a textbook, ""An Introduction to Animal Behaviour."" ‘What we are seeing is a pendulum swing,’ he says. ‘At the turn of the 20th century, some assumed animals thought like us, and there was a reaction against that. Now we are going the other way. But it is a contentious subject, and you want to avoid the sound of academics with personal grievances and strong opinions.’"
11,C1,"
Critical thinking is a way of understanding what we read or hear more deeply. Adrian West, from the Edward de Bono Foundation U.K., says that argument can help find the truth. Technology helps us store and process information, but it might also change how we solve complex problems, making it harder to think deeply. West notes that we are exposed to a lot of poor thinking and opinions, which can overwhelm our ability to reason. More data does not always mean better knowledge or decisions. 
According to the National Endowment for the Arts, reading literature has decreased by 10%, and this decline is speeding up. Patricia Greenfield, a psychology professor, believes that focusing more on visual media has a cost. She suggests that less reading might lead to less critical thinking because people focus more on real-time media and multitasking. However, there is no clear answer yet on how technology affects critical thinking. 
Technology has changed how we think, says Greenfield, who studied over 50 research papers on learning and technology. She explains that reading helps develop imagination, reflection, and critical thinking, unlike visual media like video games and TV. Visual media can improve some information processing, but they often do not allow time for reflection or analysis. As a result, many people, especially younger ones, may not reach their full potential.
How society views technology affects how it sees critical thinking. This is clear in the debate about video games and thinking skills. James Paul Gee, an educational psychology professor, says that video games are not just entertainment; they can be good learning tools. Evidence shows that games like Sim City and Civilization help build reasoning skills and teach decision-making in realistic virtual environments. These games allow players to explore ideas that might be hard to access otherwise. In the digital age, as reading and math scores drop, it's important to understand how technology affects thinking and analysis."
12,C1,"Matthew Crawford is a motorcycle mechanic who used to work in an office. He became unhappy with office life and decided to write about the benefits of manual work. His latest book is about dealing with modern life. He got the idea when he noticed ads on a credit card machine while shopping. Crawford believes these ads are hard to avoid and distract us from our own thoughts. This makes it difficult to think or remember past conversations. To avoid constant interruptions, people are less likely to talk to strangers. Crawford says we often experience the world through things like video games and phone apps, which can control our desires. Many people are worried about this, like office workers who complain about emails but still check them in their free time. Studies show that just seeing a phone can distract us. There is no proof yet that our attention spans are shorter, but we are more aware of other things we could be doing. Crawford thinks technology has made it easy for us to focus on ourselves. This affects society because we prefer texting to talking. By only interacting with digital versions of people, we might lose important social skills. Crawford gives an example from his gym, where people used to share music but now listen to their own with ear buds. This change has made the gym less social. Crawford suggests two solutions. First, we need rules to reduce noise and distractions in public. More importantly, he believes in engaging with the world through skilled activities. He mentions cooks, ice-hockey players, and motorbike racers as examples of people who work with real things. These activities require good judgment and interaction with others. Crawford says that when we engage with the world this way, digital experiences seem less important. He doesn't mean everyone should become a chef, but that we should find ways to use our judgment. This can help us focus better and resist distractions."
13,C1,"""‘What do you do for a living?’ is a common question. We often define ourselves by our jobs. Usually, we answer this question in a few words. But when you are a philosopher, like me, it is a bit harder. Saying you are a philosopher can sound pretentious. People might think you claim to have special knowledge or wisdom. This is not true; philosophers are just like everyone else. So, why do people have this idea about philosophers? One reason is that philosophers are seen as people who judge others and value intellectual life. The Greek philosopher Aristotle (384 – 322 BCE) said that a life of thinking, or a philosophical life, was the best life. Few modern philosophers agree, but philosophy is still linked with thinking deeply. Another Greek philosopher, Socrates, said ‘the unexamined life is not worth living’. He meant that just accepting what society says is not satisfying. Our ability to think about the world helps us control our lives and make decisions. But living an examined life does not mean reading many philosophical books or spending all your time thinking. It means looking closely at everyday experiences to see if they are important. It does not mean living like a wise person away from society. The examined life is practical and involves sharing knowledge, which is important for a good life. Another reason people misunderstand philosophers is that academic philosophy has become distant from real life, especially for those without formal training. This is partly because of how universities work, with funding and evaluations tied to expensive journals. Philosophers often only talk to other experts in their field. To outsiders, philosophy can seem disconnected from reality. If philosophers once tried to change this view, many have stopped. The university system created this isolated environment, and academics have supported it. As some areas of philosophy focus on specific, technical debates, explaining them to non-experts is very hard. Philosophy, in some cases, has moved away from society. This needs to change. I sometimes call myself an ‘ethicist’ because I work in ethics, which is about evaluating human actions. But I feel this title is not enough because ethics is often linked to rules and laws. Recently, ethics has come to mean applied ethics, which looks at the fairness of social practices. Ethicists today decide if an action is ‘ethical’ or acceptable. These are important questions, and I often discuss them. However, philosophy is more than this. A discussion might start with whether downloading films illegally is wrong (it is) and then explore ideas about responsibility, art, and consumerism. Philosophy helps people examine the actions and beliefs that shape their lives. Sometimes we find what we already know; other times, we see our beliefs are hard to justify. By examining these ideas, we help everyone."""
14,C1,"'People who love food, like foodies, chefs, and gourmands, might think that thinking deeply about what and how we eat can make eating more enjoyable. However, in history, philosophers have often used food to talk about other ideas. When they mentioned eating, it was usually a metaphor for something else, like learning. Sometimes, discussions that seem to be about food are actually about other, loosely related topics. An interesting example is the ancient Greek philosopher Epicurus. He talked about seeking pleasure and avoiding pain in many areas of life. Yet, his name is now linked with a love for eating and drinking. We see his name used in restaurants and food shops, likely because businesses think it will attract customers. Food is social and cultural. It comes from history and is shared with others in our communities. These communities give us people to eat with and the systems to grow and distribute food. We interact with food more often and in more basic ways than any other product, making it a good topic for philosophical study. Once food is made, critics start discussing it. But why do they have such a special role? One philosopher says that tasting food is not a special skill, but that famous food critics are just better at describing tastes. This area has been overlooked because philosophers have mostly studied vision, not taste. Another part of food is its beauty. We say paintings or music are beautiful, but not food. Some philosophers argue that with modern cooking, food should be part of discussions about beauty, like art or poetry. But food is eaten and disappears, unlike art, which lasts over time. So, some think food cannot be considered truly beautiful. There are many ethical questions about food. We can ask what we should eat: organic, free-range, local, vegetarian, or non-GMO foods? Our choices often show our ethical beliefs. Philosophers ask why we make these choices, and discussing this is important. Another topic is how cooking at home is different from cooking in restaurants. Home cooks have a duty to their guests because of personal relationships. At home, everyone shares food and friendship. Professional cooks have responsibilities to their employers and the food, so their kitchens are not open to everyone, only to qualified people. Their relationships are professional, not personal. A recent essay called ‘Diplomacy of the Dish’ looks at how food can help bridge cultural gaps. This happens in two ways. First, we can appreciate other cultures by enjoying their food. Second, we build personal connections with people from other cultures by eating together, a practice with a long history in diplomacy. The essay includes many examples and stories, making this part of food philosophy interesting for readers.'"
15,C1,"Rob Daviau, from the US, designs 'legacy' board games. He felt that the board games he played as a child were not exciting or challenging anymore. He wondered if board games could have a storyline or if decisions made in one game could affect the next. He changed a classic board game called Risk to create a new version: Risk Legacy. In this version, decisions made during the game have a lasting impact. Players might have to tear up cards, write on the board, or open packets with new rules. The game is played over a set number of sessions, and past actions become part of the game. Daviau said: 'You could point to the board and say: ""Right here! You did this!""'
Daviau was then asked to work on a legacy version of Pandemic, a popular game where players try to cure diseases. His next project was a game called SeaFall. While Pandemic Legacy was very successful, SeaFall was seen as a real test of the legacy format because it was not based on an earlier game. Set in the age of sail (16th – mid 19th century), players explore and conquer a new world. Legacy game designers must think about all possible player choices to keep the story together. They use testers to see how the game will play out. Jaime Barriga, a tester for SeaFall, said: 'It takes a lot of time. Sometimes the first few games are great, but then it starts to fall apart, and you have to fix everything.'
Legacy board games were not expected to become popular. Even Daviau thought it would be a small interest. 'When I was working on it, I thought: ""This is different and good,""' he said. 'But it's strange and breaks many rules. I thought only a few people would like it.' However, many players, like Russell Chapman, loved the idea. He sees it as a big step forward in game design. 'It's a new level of commitment, intensity, excitement,' he said. 'There's nothing more exciting for a board gamer than learning a new game or way to play, and you get that with legacy games.'
Another fan, Ben Hogg, said the adventure was more important than worries about the game's lifespan. 'At first, I was worried about changing and ruining the board,' he said. 'But Pandemic Legacy changed that. Most people don't watch the same movie twice, do they? You're buying an experience. It's like the stories you get from video games.' The legacy format is influenced by video games and the demand for episodic entertainment from TV series. Daviau once wanted to be a TV writer but moved to advertising and then game design. The love of storytelling stayed with him. Pandemic creator Matt Leacock compares designing a legacy game to writing a novel. 'You need to know how it ends and starts,' he said. 'But you also need more than just an outline.'
While Daviau feels proud of his work, he is interested in seeing how others use the idea. But he also thinks people will soon want something new. Colby Dauch, from the publisher of SeaFall, believes the legacy format is a big change. 'It's the kind of idea that changes how you think about board games,' he said."
16,C1,"'One night, an octopus named Inky escaped from his tank at New Zealand’s National Aquarium. He moved across the floor and squeezed into a drain leading to the Pacific Ocean. This story was shared widely online and seemed like something from a children's movie. People enjoy stories like this because they like to imagine animals are like humans. This is especially true for octopuses, which are very intelligent but look very different from us. Octopuses can open jars, recognize faces, use coconut shells as armor, and even play in complex ways. 
Some people think that giving animals human traits is unscientific. However, Dr. Frans de Waal, who studies primates like gorillas and chimpanzees, believes the opposite. He says that not recognizing human-like traits in animals, which he calls anthropodenial, is more common. He has studied animal behavior for many years and found that animals can do many things we thought only humans could do, like thinking about the past and future, showing empathy, and understanding others' motives. 
In the past, people thought animals were smart. In medieval Europe, animals could even be put on trial for crimes. In the nineteenth century, naturalists looked for connections between human and animal intelligence. Charles Darwin, who developed the theory of evolution, said that the difference between humans and animals is one of degree, not kind. 
In the twentieth century, behaviorism changed how people viewed animal intelligence. Animals were seen as machines that responded to stimuli or as robots with instincts. People who thought animals had inner lives were considered unscientific. This view coincided with humans destroying animal habitats and ignoring animal welfare. 
Dr. de Waal believes we are now starting to see animal intelligence as similar to human intelligence, though not the same. He says that the best tests of animal intelligence consider the species' unique traits. For example, squirrels might not do well on human memory tests, but they can remember where they hid nuts. 
In her book, The Soul of an Octopus, naturalist Sy Montgomery suggests that if octopuses tested human intelligence, they might think we are not very smart because we can't change our skin color. 
Dr. de Waal is not sure if Inky really found his way to the ocean, but he knows that viral stories can help people appreciate animal intelligence. He once did an experiment to see if capuchin monkeys feel envy. When some monkeys got cucumbers and others got grapes, the monkeys with cucumbers got upset. The study was published in a scientific journal, but a video of the experiment convinced more people. This shows how our minds work in interesting ways.'"
17,C1,"'Robotics, once only seen in science fiction, is now becoming a major technological change, similar to the industrial revolution. Robots have been used in car and manufacturing industries for years, but experts say we are close to a big change where robots will be used in many more areas. Many people think robots will take over most jobs in the next 50 years, but they also believe their own jobs will still exist. This is not true: robotic automation will affect every industry soon. For example, an Australian company, Fastbrick Robotics, has created a robot called Hadrian X that can lay 1,000 bricks in one hour, a job that takes two human workers almost a day. In San Francisco, a startup called Simbe Robotics has made Tally, a robot that moves through supermarket aisles to check if goods are stocked and priced correctly. Supporters of robotic automation say robots will create new jobs for technicians and programmers. Critics warn that we should not ignore the importance of human skills at work. Dr. Jing Bing Zhang, an expert in robotics, studies how robots are changing the workforce. His research shows that soon, many robots will be smarter and able to work with humans. In a few years, many top companies will have a chief robotics officer, and some governments will have laws about robots. Salaries in the robotics field will rise, but there will be a lack of skilled workers. Zhang says that people with lower skills will be affected and should retrain themselves, as they cannot rely on the government to protect their jobs. New technologies will create a new type of robots for consumers, like robots that live with us at home. This is a big chance for companies, but it also brings challenges, like the need for new safety and privacy rules. With many jobs at risk, education is important to prepare for the future workforce. Developed countries need more graduates in science, technology, engineering, and maths (STEM) to stay competitive.'"
18,C1,"George Mallory, a famous mountaineer, once answered the question of why he wanted to climb Mount Everest with, ""Because it’s there."" This response reflects a common curiosity: why do people risk their lives for something that seems pointless? Mallory's answer might inspire some to follow their dreams, challenging them to face the unknown. The real value of his answer is its simplicity, suggesting that climbing might be just for adventure and fun.
In 1967, Bolivian writer Tejada-Flores wrote an important essay called ‘Games that Climbers Play’. He described seven different types of climbing activities, each with its own rules. He believed that the style of climbing, like using more or less equipment, should match the rules of the specific climbing game. Over the years, this idea has become very popular in Western climbing discussions, from magazines to debates.
Many climbers love the feeling of freedom that climbing gives them. However, climbing can also be very limiting, like being stuck in a tent during a storm. This creates a paradox: climbing can feel both freeing and restricting. But some argue that the simplicity of climbing, with its limited choices, is a form of freedom.
US rock climber Joe Fitschen offers a different perspective. He suggests asking, ""Why do people climb?"" to explore why humans are drawn to climbing. Fitschen believes climbing is in our genes; it’s natural for us to take on challenges and test our limits, even with risks. So, the joy of climbing is more about biology than logic.
US academic Brian Treanor adds another view. He thinks climbing helps develop virtues like courage, humility, and respect for nature. While not all climbers show these traits, Treanor believes they are important in our modern, risk-averse world. Climbing can help people develop qualities that are useful in everyday life.
Another idea is that climbers who rely only on themselves, without help from others or technology, must be fully committed to succeed. Experts Ebert and Robinson argued that independent climbs deserve more recognition because they are harder and riskier. However, this view has been controversial, as it might encourage unnecessary risks.
Finally, climbing can be seen from a non-Western perspective. Many climbers talk about being ""in the moment"" or ""in the zone"" during a climb. The physical effort, meditation, and intuitive problem-solving are key parts of climbing. Some say these aspects are similar to Zen philosophy, which aims for a state of perfect peace. This offers another reason why people are drawn to climbing."
79,C2,"'In recent years, a key change in higher education in the UK and around the world has been to encourage new students to get involved in research early in their studies. This change shows that research is not just for famous scholars at old universities or scientists making new discoveries. Instead, research is a natural way to build knowledge and skills. Research skills are useful not only in your studies but also in jobs, because they help you think about the world and do your work. As a student, you contribute to knowledge. You do not just learn and repeat it; you create it. Creating knowledge involves asking questions like Why? How? When? What does this mean? How might that be done? What if this were different? How does it work in that context? What do we really think about the facts, views, or beliefs we are given? Why does it matter? These questions are the basis of research. Research can range from complex, groundbreaking studies by highly trained people to everyday inquiries that involve careful work and thoughtful questions about issues, practices, and events. Most students have always been researchers in some way. You have done research for projects and answered questions at school or work. You have asked questions that led to investigations since you first became interested in studying. You have also developed research skills when planning a holiday, growing plants, fixing things, training a pet, choosing a music system, or shopping online. In college and higher education, having an enquiring mind, identifying problems and questions, critically exploring and evaluating information and ideas, and creating your own responses and knowledge are expected. Some students may find this difficult because, in some cultures, knowledge is seen as already established, and you learn by listening to teachers and texts. It might seem disrespectful to question established knowledge and authorities, and you might feel you need to be told what is important to learn. However, in the UK, US, much of Europe, and Australasia, questioning established knowledge and authorities is encouraged. This process might seem daunting. Critical thinking is very important in research. The research of others is useful to students, academics, and in jobs, but we need to do more than just repeat what we read. We need to engage with it, think about it, test it, and see if it is logical, reasoned, and supported by evidence. We should not rely uncritically on facts and information given to us by others.'"
80,C2,"Cities have always been places of intellectual activity. In the 18th century, people in London met in coffee houses to talk about science and politics. In modern Paris, artists like Pablo Picasso discussed modern art in cafés. However, living in a city is not always easy. The same London cafés that encouraged discussion also spread diseases like cholera. Picasso eventually moved to the countryside. While cities are creative hubs, they can also be overwhelming and unnatural. Scientists are now studying how city life affects our brains, and the findings are concerning. Although city life is known to be tiring, new research shows that it can also reduce our ability to think clearly. One major reason is the lack of nature. Studies show that hospital patients recover faster when they can see trees from their windows. Even brief views of nature can improve brain function because they offer a break from city life. This research is important as more people now live in cities than ever before. Instead of open spaces, we live in crowded areas with many strangers. These unnatural environments affect our mental and physical health and change how we think. Walking down a busy street, our brains must track many things: distracted people, traffic, and the confusing city layout. These tasks drain our mental energy because cities are full of stimuli, forcing us to constantly focus and refocus. This process uses a lot of brain power. In contrast, natural settings are easier on the brain. This idea is called attention restoration theory, developed by psychologist Stephen Kaplan. He suggested that being in nature helps restore our attention. Nature captures our attention without causing stress, unlike city noises like police sirens. This allows our minds to relax and recharge. Long before scientists studied this, philosophers and landscape architects warned about city life and tried to include nature in urban areas. Parks like Central Park in New York offer a break from city life and can quickly improve brain function. While people try many ways to boost brain performance, like energy drinks or office redesigns, few are as effective as a walk in nature. Despite the mental challenges of city life, cities continue to grow. Why do they remain centers of intellectual life? Research from the Santa Fe Institute shows that the same city features that cause attention problems, like crowded streets, also lead to innovation. The concentration of social interactions in cities drives creativity. Just as 18th-century London was a place of intellectual breakthroughs, modern cities like Cambridge, Massachusetts, are creative technology centers. Less crowded cities may produce less innovation over time. The challenge is to reduce the negative effects of city life while keeping its benefits. As the saying goes, sometimes people feel, ""I'm sick of the trees, take me to the city!"""
81,C2,"
Where should we look for the mind? It might seem obvious that thinking happens inside our heads. Today, we have advanced brain scans to show this. However, I believe the study of the mind should not stop at the brain. There is a lot of evidence, from ancient times to now, showing that objects, as well as neurons, are part of human thinking. Archaeology shows that stone tools, jewelry, engravings, clay tokens, and writing systems have played a role in human evolution and the development of the mind. So, what is outside the head might also be part of the mind.
It is easy to see why people think the mind and brain are the same. Most of what we know about the mind comes from studying people without the objects they usually have around them. This makes sense for neuroscientists because of the limits of brain-scanning machines. But this often hides the fact that much of our thinking happens outside our heads. I do not question the brain's role in thinking, but I suggest that the mind is more than just the brain. It is useful to explore the idea that human intelligence extends beyond the body into culture and the material world.
This is where my new theory, Material Engagement Theory (MET), comes in. MET looks at how objects become part of our thinking, like when we make numbers and symbols from clay or use a stone to make a tool. It also studies how these ways have changed since ancient times and what these changes mean for how we think. This approach gives new insights into what minds are by changing what we know about how objects help the mind.
Think of a blind person with a stick. Where does this person’s self begin? The stick and the person become one, showing how minds and objects are connected. The blind person uses the stick to turn touch into sight, and the stick plays an active role. The brain treats the stick as part of the body. This example shows that human intelligence can change by using new tools. My approach sees the human mind as always evolving.
It is important to remember that, throughout history, the 'stick' has been a pathway, not a boundary. Through the 'stick', humans explore and understand the world. This is different from a monkey using a stick to get food. For humans, 'sticks' satisfy our curiosity. This unique human trait explains why we make things and how those things shape our minds. I call this metaplasticity – our minds change as they interact with the material world.
I want to include material objects in understanding the mind. MET offers a new way to see how different forms of material culture, from stone tools to smartphones, have shaped and changed how we think. Mind-changing technology sounds futuristic, but humans have used it since they first evolved."
82,C1,"Photography is one of the few inventions that has greatly changed how we see the world. This is especially true in the United States, where photography quickly became a part of American culture. It influenced many areas, from science and industry to art and entertainment. Historians say that photography might be the greatest contribution of the US to the visual arts. Photography has become central to American intellectual and artistic discussions, so to understand its impact, we need to look at its beginnings in the mid-19th century.
Why was photography so popular? First, because it was a mechanical process, it matched the growing interest in technology in the US, where change was seen as normal. Like steam power and railroads, photography made the world feel smaller by bringing images of distant places into people's homes. Second, the camera was a tool for self-representation, important in a country where personal and national identities were always changing. Third, the camera helped families create and keep memories, even if they were idealized. Lastly, the realistic nature of photographs matched the American artists' focus on realism and everyday life.
A photograph draws attention to what the photographer wants us to see, saying, ""This, you should see!"" Because a photograph is made by a machine, it is a record of what was in front of the camera, giving it a sense of objectivity. However, since it is taken by a person, it also has a personal point of view, or subjectivity. We might think we understand a photograph because we recognize the subject, but its meaning is more complex. No image is shown to us without some context, like a caption or its placement in a magazine or gallery, which affects how we understand it.
To understand a photograph historically, we need to consider its purpose and how it was first seen. The same image can be seen in different places and times, and its meaning can change. The camera's importance to artists was once a secret, but now it is widely recognized. For decades, starting with US artist Andy Warhol, artists have used photographs in their work in many ways. Photographic vision has become an essential part of art, working alongside painting to shape our ideas of representation before the camera was invented."
83,C1,"In 1890, William James, an American philosopher and one of the founders of modern psychology, defined psychology as the ""science of mental life."" This definition is still useful today. We all have a mental life, which includes our thoughts and feelings, and we try to understand it. James was interested in human psychology, which he believed included thoughts and feelings, the physical world, and how we know about these things. Our understanding is personal and comes from our own experiences, which may or may not be influenced by scientific facts. We often act as amateur psychologists when we give opinions on why people behave in certain ways, like feeling unhappy or quitting their jobs. Problems occur when people have different views. Formal psychology tries to find the most likely explanations for these situations. Psychologists help us separate subjective thoughts from scientific facts. Although psychology is about the mind, psychologists do not fully understand the brain's role in our experiences and behavior. They study behavior to make guesses about what happens inside us. Psychology is like solving a puzzle, using clues from careful observation and analysis. Psychology aims to describe, understand, predict, and control the processes it studies. It helps us understand our experiences and apply findings to real life, such as improving teaching methods, designing safer machines, and helping people communicate their feelings. Psychological questions have been discussed for centuries, but scientific investigation began only 150 years ago. Early psychologists used introspection, or self-reflection, to study the mind, but this method has limitations. As Sir Francis Galton noted, it only shows a small part of brain activity. William James compared it to trying to see darkness by quickly turning on a light. Today, psychologists prefer to base their theories on careful observations of behavior rather than personal reflections."
84,C1,"In a warehouse in a business park in Michigan, USA, there is a place called the Museum of Failed Products. This museum shows the less successful side of consumer capitalism, where products like A Touch of Yogurt shampoo and Breakfast Cola ended up because almost nobody wanted to buy them. The museum is owned by a company called GfK, and its owner, Carol Sherry, believes each product tells a sad story of failure for the designers, marketers, and salespeople involved. Surprisingly, the museum is a successful business. Many companies do not keep their own collections of failed products, so executives visit the museum to learn from these mistakes. The museum started by accident when Robert McMath, a former marketing professional, began collecting new products in the 1960s. He wanted to create a 'reference library' of consumer products, not just failures. However, he discovered that most products fail, so his collection naturally became a museum of unsuccessful items.
Today's culture of optimism might explain why these products failed. People involved in creating them might not have realized they were doomed, or they might have continued investing in them to save face. Often, little effort is made to understand why a product failed, and people prefer not to talk about it. This focus on optimism is also common in the self-help industry, where 'positive visualization' is popular. The idea is that if you imagine things going well, they are more likely to do so. Neuroscientist Tali Sharot found that people tend to be overly optimistic about their ability to influence events, which might be linked to evolution. However, psychologist Gabriele Oettingen discovered that thinking too positively about the future can reduce motivation to achieve goals. For example, people who imagined having a successful week at work often achieved less.
Psychologist Carol Dweck suggests that our beliefs about ability affect how we handle failure. People with a 'fixed theory' mindset think ability is innate, so they see failure as proof they are not good enough. In contrast, those with an 'incremental theory' believe ability grows with effort and challenges. They see failure as a sign they are pushing their limits. Dweck compares this to weight training, where muscles grow stronger after being pushed to their limits. Having an incremental mindset is a happier way to live, whether or not it leads to success."
85,C1,"'Everyone knows that sports teams have an advantage when they play at home. But why is that? Many people think they know the answer, but professional sports are changing quickly, and what we used to believe is now being questioned. Two main reasons for this are science and money. Sports scientists want to understand what helps players perform their best, and they have many theories about home advantage. On the other hand, people who invest money in sports wonder if home advantage should matter at all. If players are paid well, shouldn't they perform well anywhere? 
What about the fans? Would it matter if Manchester United played some home games in the Far East to reach their fans there? It would matter to British fans, who believe their support is crucial for the team. Fans often think that when they cheer, it leads to goals, but they forget the times when nothing happened despite their cheering. 
However, one thing that seems true is that home fans can influence referees. In an experiment, referees watched a match with and without crowd noise. Those who heard the noise were less likely to call fouls against the home team. This suggests that referees try to avoid making decisions that would upset the home crowd.
Studies show that home advantage has decreased in all major sports, but not as much as expected. For example, in the 1890s, home football teams in England won about 70% of the points, compared to 60% today. Travel used to be difficult, but now players travel in comfort, and stadiums are more similar. Despite these changes, home advantage still varies by sport. Basketball has the most home advantage, followed by football, while baseball has the least. This might be because baseball focuses more on individual players than teamwork.
Another reason for home advantage could be psychological. Research shows that players have higher testosterone levels before home games, which might be linked to a natural instinct to defend their home ground. This was seen in the Rugby World Cup, where home teams won against stronger opponents by showing determination and aggression. As one referee said, ‘It’s the law of the jungle out there.’'"
86,C1,"""‘The more I practise, the luckier I get,’ said golfer Gary Player about fifty years ago. This saying is famous among athletes and coaches. It suggests that practice leads to improvement, not luck. This idea is part of the nature-nurture debate: is talent something you are born with, or can it be developed? The debate often seems one-sided, with many believing that practice is more important than natural talent. Psychologist Anders Ericsson is known for the idea that 10,000 hours of practice can make someone an expert in any field. This theory has inspired books like Malcolm Gladwell’s 'Outliers' and 'Talent Is Overrated,' which argue that practice is key to success.
However, in his book, Epstein challenges this idea. He points out that studies often focus only on successful people, which can be misleading. Epstein explores why some people excel while others do not, considering factors like environment, support, and determination. His research takes him around the world, even to Alaska, where he learns that sled dogs’ success might be due to both their genes and their training. This suggests that determination might also have a genetic component.
Epstein distinguishes between 'hardware' (nature) and 'software' (nurture) and believes both are important for elite athletes. He acknowledges the role of training and environment but also highlights cases where genetics play a crucial role. He discusses race and gender, questioning why men and women compete separately if practice is the only factor that matters. Sometimes, the simplest questions are the most revealing."""
87,C1,"'A recent international report shows that many children in wealthy countries feel lonely and unhappy. Jay Griffiths asks: why are children today so unhappy? In her book, Kith, Griffiths explains that children spend too much time indoors, in front of screens like TVs and computers, and have lost touch with nature. She believes this is the main problem. This idea is supported by other studies on childhood today. A follow-up study interviewed children and found that many would be happier if they could play outside more. Many adults over 40 remember their own childhoods fondly, when they spent time exploring outdoors, swimming, or building dens. They think this was healthier than the way children live now. However, they often forget that they are the ones who have made life more protective for their children, avoiding risks like playing in old sheds. Griffiths' book includes strong arguments. She talks about the fear of danger that keeps children indoors and how it benefits the toy and gadget industry. She also discusses trends like giving medication to restless children or making them wear goggles for playground games. Some of these rules might be exaggerated, but Griffiths expresses her views passionately. She also talks about childhood freedoms and rules, from fairy tales to school regulations. While her arguments are interesting, Griffiths sometimes takes them too far. She even compares modern treatment of children to racism, which seems extreme. Griffiths is very romantic about children. She mostly sees them in an ideal way, which might not match reality. For example, she says 'children are the musicians of thought,' but this might not be true for all kids. The main idea of the book is that children should have freedom to explore and take risks. Griffiths thinks children need small accidents to learn how to avoid bigger ones. However, she doesn't explain how to ensure these accidents are 'the right size.' Also, not all children might want the adventurous life Griffiths describes. Some children are naturally quiet and cautious. Maybe the real issue is forcing all children into the same lifestyle, whether keeping them inside or pushing them outside.'"
88,C1,"I am a research bio-psychologist with a PhD, so I have spent a lot of time in school. I am good at solving problems in my work and life, but this is not because of my schooling. Most life problems cannot be solved with complex formulas or memorized answers from school. They need judgment, wisdom, and creativity, which come from life experiences. For children, these experiences come from play. My recent research focuses on the value of play for children's development. All mammals, including humans, play when they are young, and those with more to learn play more. Carnivores play more than herbivores because hunting is harder to learn than grazing. Primates play more than other mammals because their way of life depends more on learning than on instincts. Children, who have the most to learn, play more than any other young primates when they can. Play is how adults and mammals have always educated themselves. The most important skills children need to live happy, productive, moral lives cannot be taught in school. These skills are learned and practiced through play. They include creativity, getting along with others, cooperating, and controlling impulses and emotions. Creativity is important for economic success. We no longer need people to follow instructions like robots or do routine calculations. We need people who can ask new questions and solve new problems. If we can develop thinkers who anticipate obstacles, we will have a strong workforce. This requires creative thinking. A creative mind is a playful mind. Geniuses are adults who keep and build on their childlike creativity. Albert Einstein said school almost destroyed his interest in math and physics, but he regained it after leaving school. He called his innovative work 'combinatorial play'. He developed his concept of relativity by imagining himself chasing a sunbeam. We cannot teach creativity, but we can suppress it with schooling that focuses on set questions and answers. More important than creativity is the ability to get along with others, care for them, and cooperate. Children are born wanting to play with others, and through play, they learn social skills, fairness, and morality. Play is voluntary, meaning players can quit anytime. To keep the game going, players must keep others happy. This makes play very democratic. School has become more demanding: breaks are shorter, homework has increased, and there is more pressure for high grades. Outside school, adult-directed sports have replaced spontaneous games. 'Play dates' with adults have replaced unsupervised neighborhood play, and adults now intervene instead of letting children solve their own problems. These changes have been gradual but significant. They are due to social factors like parents' fears, warnings from experts, less cohesive neighborhoods, and the belief that children learn more from adults than from each other. Our children do not need more school; they need more play. If we care about our children and future generations, we must reverse the trend of the past fifty years. We must give childhood back to children. They must be allowed to play and explore so they can grow into strong adults ready for an unpredictable future."
89,C1,"In many countries, more people in their twenties are using social media to find jobs. Platforms like Twitter and LinkedIn allow direct contact with potential employers, similar to standing outside an office with a ""hire me"" sign. However, this access also increases the chance of making mistakes. For example, a young jobseeker in the US contacted a senior marketing executive on LinkedIn, hoping to use her contacts to get a job. The executive was upset by this request and sent a harsh rejection note, which went viral online. Many people were shocked by the note, and the executive might regret her tone. However, this incident highlights the importance of using social media carefully for job hunting. Social media can be risky for job seekers who don't know how to use it properly, and many are making mistakes.
There is irony here because social media sites like Facebook and Twitter have been central to the social lives of twentysomethings for years. For my generation, social media was a way to escape from parents and teachers. It was a place to impress and experiment, often based on fantasy. You could have long conversations online and then ignore the person at school. By choosing certain pictures or songs for your Facebook page, you could create a different persona. But when using social media for professional networking, this experience can be a disadvantage. Professional networking is very different, and some young people don't see the difference. We first became active online by being bold and confident, which might explain why some still think this is a good approach. Just because many people liked your Facebook posts doesn't mean you can use LinkedIn to show employers you're worth hiring. We need to understand that what we learned about social networking as teenagers doesn't apply now, and we must meet employers' standards to succeed in the job market.
One common complaint from employers about young job seekers on professional networking sites is that they are too familiar and seem arrogant. This reinforces the stereotype of our generation as entitled. In reality, many young people are desperate to find jobs, which is why they turn to social media. This impression of arrogance can harm their job prospects, even if they have the skills and motivation to be valuable employees.
So, how should you contact someone on a professional networking site? First, clearly explain who you are and what you can offer them, like doing research or helping in some way. This approach increases your chances of getting a positive response. Avoid sending impersonal, mass emails, and keep your tone humble to avoid offending the recipient. Remember, social media can be a great way to make useful contacts, but it requires careful handling to avoid negative outcomes."
90,C1,"'Predicting the future of newspapers is very difficult. If we look at the numbers, newspapers seem to be in trouble. Since 2000, the circulation of most UK national newspapers has dropped by a third to a half. In the USA, the Pew Research Centre says that only 26% of people now get their news from newspapers, compared to 45% in 2001. Some people say that printed newspapers will disappear in 15 years. However, history shows that old media often survive. In 1835, a New York journalist said books and theatre were finished, but theatre survived newspapers, cinema, and television. Radio has done well even with TV, and cinema has survived videos and DVDs. Even vinyl records are popular again, with sales up 745% since 2008. Newspapers were once new media too, but it took centuries for them to become the main source of news. This happened in the mid-19th century with the steam press, railway, and telegraph. People also started to believe that everything is always changing, and they needed regular news updates. In medieval times, people only noticed the changing seasons and big events like famine or disease. Journalism as a job you could live on didn't really exist before the 19th century. Even then, it wasn't clear why people needed news every day or week. Regular newspaper publication can be limiting. Online news lets readers choose what is important to them. Search engines and algorithms help us find news that matches our interests. Online news can update stories quickly and correct mistakes. There are no space limits, and we can often see full documents or events. This is very different from newspapers. However, online news often focuses on being first and getting reader comments, which can create confusion. In medieval times, news was shared in markets or taverns, mixed with rumors and misunderstandings. In some ways, we are going back to that. Newspapers have not always been good at explaining how the world works. They might disappear, or they might help us understand our complex world.'"
91,C1,"'If humans were truly comfortable under the light of the moon and stars, we would enjoy the darkness, seeing the midnight world as clearly as many nocturnal animals do. However, we are daytime creatures, with eyes made for sunlight. This is a basic part of our genetics, even if we don't often think of ourselves this way. This explains why we have changed the night by adding light to it. We have filled the night with artificial light, similar to how we control rivers with dams. This has benefits but also causes problems, known as light pollution, which scientists are just beginning to study. Light pollution mostly comes from poor lighting design, where artificial light shines into the sky instead of focusing downward. This bad lighting changes the natural darkness, affecting the light levels and rhythms that many living things, including humans, have adapted to. Wherever artificial light enters the natural world, it can affect life processes like migration, reproduction, and feeding. For most of human history, the idea of 'light pollution' would not have made sense. Imagine walking towards London on a moonlit night around 1800, when it was the world's largest city. Nearly a million people lived there, using candles, torches, and lanterns. Only a few houses had gas lighting, and there were no public gaslights in the streets yet. From a few miles away, you would have smelled London before seeing its faint glow. We have lit up the night as if it were empty, but it is not. Many mammals are nocturnal. Light is a strong biological force and attracts many species. It is so strong that scientists say songbirds and seabirds are 'captured' by searchlights or gas flares, circling until they fall. Birds migrating at night can crash into brightly lit buildings; young birds on their first journey are especially at risk. Some birds, like blackbirds and nightingales, sing at odd times because of artificial light. People once thought light pollution only affected astronomers, who need a clear night sky. Unlike astronomers, most of us don't need a perfect night sky for work, but we do need darkness. Ignoring darkness is pointless. It is as important for our health as light. Changing our natural rhythms can cause health problems. The regular pattern of waking and sleeping is a biological reflection of Earth's light cycle. These rhythms are so important that changing them is like altering our balance. In the end, humans are as affected by light pollution as frogs near a bright highway. Living in our own bright world, we have lost touch with our evolutionary and cultural heritage – the light of the stars and the natural day-night cycle. Light pollution makes us forget our true place in the universe, which is best understood under a deep night sky with the Milky Way above.'"
92,C1,"The founder of a large international company recently announced that his company will stop tracking employees' paid holiday time. This decision was inspired by an internet company with a similar policy. The founder got the idea from an email from his daughter, which seemed like it was written by his media team. Ignoring the way the announcement was made, we should ask: is this idea practical? The internet company and the multinational corporation are very different. The internet company has 2,000 employees and offers one service, while the multinational has 50,000 employees and many different services like finance, transport, and healthcare. The idea of ""take as much time off as you want if it doesn't harm the business"" might work better in a smaller company where employees know each other's work better. The founder of the multinational said employees can take as much leave as they want if they are sure their work and team are up to date and their absence won't harm the business or their careers. But can anyone be that sure? No matter how much you prepare, there is always work waiting when you return. This is the nature of taking leave; work piles up and is out of your control. Someone following these rules might not take leave at all or feel guilty about it. Guilt can lead to stress, and not taking enough leave can reduce productivity over time. There could also be pressure from colleagues and gossip about who is off and for how long. This pressure already affects when people start and end their workday. In the corporate world, there is a culture of working late, and this could lead to a ""no holiday"" culture in a company with unlimited leave, where workers compete for promotions. If the security of guaranteed leave is removed, people might feel they can't take the leave they need, fearing they look lazy. They would lose their legal right to fall back on. The policy might result in workers not taking their leave or using their legal rights as a guideline, making the policy pointless. Modern technology lets us get work messages anytime, blurring work and leisure time. The internet company started their unlimited leave policy when employees asked how this new way of working fit with the old time-off policy. If the company can't track work hours accurately, why use an old standard for time off? But if there are no set work hours, all hours could be work hours. Employees might not know if their hours are being watched, leading them to monitor themselves, which can be harmful. Employment laws exist for a reason. Workers have a right to a minimum amount of paid leave because rest is important for health. The benefits of the unlimited leave policy, like better morale and creativity, can exist without harming worker well-being. I doubt that ""taking as much holiday as they want"" is the real goal or likely result of this policy."
93,C1,"'Journal-based peer review is the process where experts in the same field examine a scientific research paper. It is seen as a way to ensure the quality of research. This process is supposed to stop flawed or nonsensical papers from being published. Scientists often mention it to reassure the media and the public. However, reviewing a paper can delay its publication by up to a year. Is this delay worth it to ensure trust in published research? The answer is both yes and no. Examining these issues shows that scientific publishing is changing. I am not ready to give up on journal-based peer review. I still think papers should be checked before they are formally published, but I feel changes happening. The use of preprints, which are drafts of papers posted online without peer review, is an important part of this change. Preprints allow new results to be shared quickly so they can be read, critiqued, and built upon. Publishing in journals has become more about gaining fame and advancing careers, which affects both authors and reviewers. Competition for spots in top journals pushes scientists to do their best work, but it also encourages cutting corners. Reviewers now often decide if a paper is good enough for a journal, not just if it is good. For top journals, this can depend on how newsworthy a paper is, not just its scientific quality. These problems are known, but few people want to change the current system. However, as biologist Ron Vale recently argued in a preprint, preprints might offer a solution because they don't require a big change from the norm. Preprint archives have existed for twenty years, but they are not widely used. This slow adoption is partly because scientists are conservative and because many believe journals won't accept papers that have been posted as preprints. There is also a fear that publishing without peer review will lead to 'junk science', but this hasn't happened yet. Preprints are not peer-reviewed, but authors know they will be critiqued by a global community. Tanya Elks, a psychology professor, shared her experience: ‘My paper critiqued a published paper, which is hard to do in traditional journals. With anonymous peer review, the original authors might block a critical paper, or they might not be chosen as reviewers and could complain about misrepresentation. By posting a preprint, the original authors could respond, and we could consider their points. All comments are public, so readers can judge the arguments. Rejection by journals is less of a problem because the preprint and comments are still available, so our work isn't wasted.’ Preprint archives allow global scientific discussions that used to happen only between individuals. They could also be a place for negative results, which are important but often ignored by journals focused on new discoveries. Being on preprint archives increases how often papers are read and cited, showing the effectiveness of sharing through preprints. By using the web's openness and the collaborative spirit in the scientific community, preprints can help focus on the work itself, not just where it is published.'"
94,C1,"'When I ask my literature students what a poem is, they often say ‘a painting in words’. These answers don't satisfy me or them. One day, I asked them to choose an object and write one paragraph describing it like a scientist, and another from the object's point of view, titled ‘Poem.’ One student wrote: Poem I may look strange or scary, but I help people breathe. I’m used only in emergencies and for a short time. Most people will never need me. The item? An oxygen mask. This unusual choice helped the class see how poetry works differently from anything else. The exercise was fun and led to good discussions. When I was in school, poetry confused me. I thought poems were puzzles that blocked true understanding and feeling. After school, many people lose interest in poetry. Sometimes you see a poem, and it stands out because it’s not continuous prose. It challenges you to read it, and often you feel let down because it seems dull or hard to understand. Still, you feel proud for trying. What do we want from poems? Aren’t they supposed to hold deep feelings, beautiful images, or sharp wit? The answer seems yes. But for tears, we watch movies; for information or sharp critique, we read online articles. Novels let us escape to other worlds, paintings please our eyes, and music – nothing beats the mix of lyrics, instruments, and melody. Yet, a poem can offer something unique: ambiguity. Life is full of it, unlike straightforward reading. But this doesn’t explain what a poem really is. If you search online for ‘poem’, it leads to ‘poetry’: ‘a form of literary art using language’s aesthetic and rhythmic qualities.’ This is academic language, but it hides the word’s roots. ‘Poem’ comes from the Greek poí?ma, meaning ‘a thing made,’ and a poet is ‘a maker of things.’ So, if a poem is a thing made, what kind of thing is it? Poets sometimes compare poems to wild animals – untameable, unpredictable – or to machines – precise and human-made – depending on their view. But these comparisons don’t hold up. The value in comparing a poem to something else is not in the comparison itself but in the discussion it creates. Whether you see a poem as a machine or a wild animal, this can change how you think about machines or wild animals. It helps us see familiar things in a new way. A poem as a mental object is easy to imagine, especially since song lyrics often stick in our minds. The mix of words and melody is powerful, like schoolyard rhymes. But aren’t words sometimes like sticks and stones? Think of a poem in a newspaper or magazine: A poem can impact us like nothing else, even though it’s just ink on paper like the prose around it. What about the empty space around the poem – space that could be used for a longer article or ad? A poem is written and rewritten like an article, story, or novel, but it doesn’t become a commodity like they do. Publishers send out press releases and review copies of poetry collections, but few expect them to make money. A poem is not for the market but for its own sake. Because it’s special – set apart in a magazine or book – a poem can still surprise us, even if just for a moment.'"
