text_id,text_level,text
1,C2,"Some time ago, a website talked about the dangers of public check-ins, which are online announcements of where you are. The website's message was clear: you might think you're just telling people, ""Hey, I'm here,"" but you're also letting everyone know you're not at home, including people you might not want to meet. This made people more aware that sharing everything online might have negative sides. The internet gives us many chances to share our lives with a global audience, offering possibilities like wealth and fame. So, we dive into the internet, sharing confessions, personal photos, and stories. But soon, we realize the online world is crowded and risky, and we can get lost. This might sound depressing, but don't lose hope. There is a guide for this future, created by a group of early internet users. In the early days of the web, they explored these challenges. They lost jobs, made and lost friends, and dealt with the dangers of fame, all before social media existed. These early users, the first bloggers, have already experienced what many of us are going through now. Before we forget their stories, it's worth learning from them. As the saying goes, those who don't learn from history are doomed to repeat it. 
In January 1994, Justin Hall, a 19-year-old student, started posting on the ""WWW,"" which was mostly used by students, scientists, and a few young people like him. The web was created at CERN, a physics lab in Switzerland, to help researchers share their work. Hall saw it as a chance to share his life. He created a detailed online autobiography with text, photos, and art. In January 1996, he started a daily blog, and many people were interested in his bold use of this new medium. Hall's rule was simple: if you crossed his path, you might end up on his site, and no topic was off-limits. While his work was very open, it also had a certain beauty that some might call art. However, one day, visitors to Hall's site found it replaced with a video called Dark Night. He shared that he had fallen in love, but when he wrote about it online, he was told, ""either the blog goes, or I do."" He realized that sharing his life online made people not trust him. He stopped the blog, but the problem remains. Sharing online is fun, but if you think it will make people like you more, you'll be disappointed.
In 2002, Heather Armstrong, a web worker in Los Angeles, had a blog called Dooce. She sometimes wrote about her job at a software company. One day, a colleague sent her blog's address to all the vice presidents at her company, including some she had made fun of, and she lost her job. Experts call this the ""online distribution effect,"" where people feel they can say things online they wouldn't say in person. But the internet is not a separate reality where we can say anything without consequences. Our online and real lives are connected. Ignoring this can lead to serious mistakes. Armstrong's story ended well. Although she was upset and stopped blogging for a while, she got married and started her blog again, focusing on her family. Now, she is a successful ""mommy blogger,"" and her writing supports her family. She learned that while the internet lets us say anything, it doesn't mean we should."
2,C2,"Some time ago, scientists started a campaign to warn people about a low-budget film called ""What the Bleep Do We Know?"" This film mixed documentary and drama to show that there is a lot about our universe that we don't understand. Most scientists agree that we don't know everything about the universe. However, some scientists felt the need to warn the public about the film, calling it ""atrocious"" and ""a very dangerous piece of work."" This made the film seem interesting to watch.
At first, the film seemed harmless. Scientists in the film talked about how new discoveries show that the universe is stranger than we thought. But then the film talked about discoveries like water molecules being affected by thoughts. I had heard before that a Japanese researcher claimed that the shape of a water molecule could change just by the thoughts of people around it. The film showed pictures of ice crystals looking nice after being talked to by someone happy and looking bad after being exposed to someone in a bad mood. Some people find this evidence convincing because it is simple and clear. However, scientists usually say, ""Give me a break."" They mean that while the idea is interesting, we need strong proof to believe it. Nice pictures of crystals are not enough.
The real issue is that the film's claims were not strange enough. Consider this: water molecules might have properties due to a form of energy that comes from nowhere and is linked to a force that is expanding the universe. This idea is supported by decades of research in labs and observatories worldwide. Scientists have discovered that the universe is made of unknown matter and is driven by a mysterious force called ""dark energy.""
Apart from the film, other amazing discoveries are being made. Neuroscientists found that our conscious perception of events is about half a second behind reality, but our brains edit this delay out. Anthropologists think they have found where modern humans first lived and why they spread across the world. Some theorists even suggest there are links between life on Earth and the universe's design.
Science is far from complete. We are further from knowing everything than ever before. Concepts like chaos and quantum uncertainty limit what we can know. Some of the world's top physicists are trying to create a ""Theory of Everything"" to explain all the forces and particles in the universe with one equation. Despite these theories, many people think the universe can be described in one word: incredible."
3,C2,"In simple terms, I find writing novels challenging and writing short stories enjoyable. Writing a novel is like planting a forest, while writing a short story is like planting a garden. Both activities complement each other, creating a complete landscape that I value. The trees provide shade, and the wind moves the leaves, which sometimes turn golden. In the garden, flowers bloom, and their colorful petals attract bees and butterflies, showing the change of seasons. Since I started my career as a fiction writer, I have alternated between writing novels and short stories. My routine is this: after finishing a novel, I want to write short stories; after completing short stories, I focus on a novel. I never mix the two; I don't write short stories while working on a novel and vice versa. Writing these two types of stories might use different parts of the brain, and it takes time to switch from one to the other. I began my career with two short novels in 1975, and from 1984 to 1985, I started writing short stories. I didn't know much about writing short stories then, so it was difficult, but I found it memorable. It felt like my fictional world expanded, and readers liked this new side of me. One of my first works, ‘Breaking Waves’, was in my first short-story collection, Tales from Abroad. This was my start as a short-story writer. One joy of writing short stories is that they don't take long to finish. It usually takes me about a week to shape a short story (though revising can take a long time). It's not like the full commitment needed for the year or two it takes to write a novel. You go into a room, finish your work, and leave. That's it. Writing a novel can feel like it goes on forever, and I sometimes wonder if I'll make it through. So, writing short stories is a necessary change of pace for me. Another nice thing about short stories is that you can create them from small things – an idea, a word, an image, anything. It's often like jazz improvisation, with the story leading me. Also, with short stories, you don't have to worry about failing. If the idea doesn't work out, you just accept it and move on. Even great writers like F. Scott Fitzgerald and Raymond Carver didn't make every short story a masterpiece. This is comforting to me. You can learn from your mistakes and use that in the next story. When I write novels, I try to learn from the successes and failures of my short stories. In this way, short stories are like an experimental lab for me as a novelist. It's hard to experiment in a novel, so without short stories, writing novels would be even harder. My short stories are like soft shadows I've left in the world, faint footprints I've made. I remember where I wrote each one and how I felt. Short stories are like guideposts to my heart, and it makes me happy to share these feelings with my readers."
4,C2,"Science can be very abstract, like philosophy, or very practical, like curing diseases. It has made our lives easier but also posed threats. Science tries to understand everything from tiny ants to the vast universe, but it often struggles. It influences poets, politicians, philosophers, and even tricksters. Its beauty is clear to experts, but its dangers are often misunderstood. People sometimes overestimate or underestimate its importance, and its mistakes are either ignored or exaggerated. 
Science is full of conflicts. Old theories are often changed or replaced by new ones, similar to how new music styles are first mocked but later accepted. Scientists can be competitive and emotional. This book will explore how scientific ideas have changed not just science but also human thought. We will focus on ideas, not practical inventions like non-stick pans. We will admire these ideas but also question them, recognizing both human creativity and limitations.
Science is always changing. Scientists often challenge each other's ideas. Usually, these changes don't affect society much, but sometimes they cause big shifts in our beliefs. For example, in the 17th century, science described the universe as a giant clock. Later, physics showed that our observations can affect the universe, and we don't fully understand basic concepts. Some people think this shows science can't explain everything, but scientific changes usually help us understand and predict nature better. Isaac Newton explained more than Aristotle, and Albert Einstein explained more than Newton. Science makes mistakes but keeps progressing.
At the end of the 19th century, many physicists thought there was nothing new to discover. Then came radioactivity, X-rays, electrons, quantum mechanics, relativity, and more. Biology has also made many discoveries. Some people now think we are close to a complete theory of the universe. Maybe. Science is not just a harmless hobby. In the last 200 years, we have started to control nature, sometimes upsetting its balance without understanding the consequences. Science needs to be monitored. Non-scientists must understand scientific advances because they affect the world their children will live in and even the children themselves. Science is now part of how we plan and shape our future, and these decisions are important for budgets, health, and life on Earth."
5,C2,"From around 2015, after years of growth, big publishers noticed that ebook sales stopped increasing or even went down. This raised new questions about the future of ebooks in the publishing world. One publishing executive, who didn't want to be named, said that the excitement about ebooks might have led to bad investments, as his company lost faith in the traditional printed book. Despite the clear idea that digital and print books can exist together, people still wonder if ebooks will replace printed books. Whether people are predicting or dismissing this idea, the thought of books disappearing continues to capture our imagination and spark debate. Why is this idea so strong? Why do we see the relationship between ebooks and print books as a battle, even when evidence suggests otherwise? The answers go beyond ebooks and reveal our mixed feelings of excitement and fear about new technology and change. 
In my research, I have talked about how the idea of one type of media replacing another often comes up when new technologies appear. Even before digital technology, people predicted the end of existing media. For example, when television was invented, many thought radio would die. But radio survived by finding new uses, like being listened to in cars or at work. The idea of books disappearing isn't new either. As early as 1894, people thought the phonograph would replace books with what we now call audiobooks. This pattern has repeated many times. Movies, radio, television, and smartphones have all been seen as threats to print books. Some people thought the end of books would lead to cultural decline, while others believed ebooks would bring great benefits. 
The idea of books disappearing often comes up during times of technological change. This story shows our hopes and fears about new technology. To understand why these feelings are common, we need to remember that we form emotional connections with media as they become part of our lives. Many studies show that we develop close ties with things like books, TVs, and computers, sometimes even naming our cars or getting angry at our laptops. So, when new technology like e-readers comes along, it doesn't just mean economic and social change. It also means we have to change our relationship with something that is part of our daily life. As technology advances, we often miss what we used to have. This is why industries grow around old products and technologies. 
For example, when the printing press spread in 15th-century Europe, people wanted original manuscripts. The change from silent to sound movies in the 1920s made people nostalgic for silent films. The same happened with the shift from analog to digital photography and from vinyl records to CDs. Not surprisingly, e-readers have made people appreciate the physical quality of old books, even their sometimes unpleasant smell. This should comfort those who worry about print books disappearing. However, the idea of a disappearing medium will continue to be a popular story about the power of technology and our dislike of change. One way we try to understand change is by using familiar stories, like those of tragedy and endings. These stories are easy to remember and share, and the story of the death of the book reflects both our excitement for the future and our fear of losing parts of our personal world – and, in the end, ourselves."
6,C2,"For a year and a half, almost every weekday morning, I woke up at 5:30, brushed my teeth, made coffee, and sat down to write. I focused on how some of the greatest minds from the past 400 years managed their time to be creative and productive. By looking at the everyday details of their lives—like when they slept, ate, and worked—I wanted to show a different side of their personalities and careers. I aimed to create interesting portraits of these artists as people with habits, just like us. 
The French gastronome Jean Anthelme Brillat-Savarin once said, ""Tell me what you eat, and I shall tell you what you are."" I say, ""Tell me what time you eat, and if you take a nap afterward."" This book is about the conditions for creativity, not the final product. It’s about how things are made, not what they mean. But it’s also personal. The novelist John Cheever believed that even writing a business letter reveals something about yourself. My book explores questions I face in my own life: How can you do meaningful creative work while earning a living? Is it better to focus entirely on a project or to work on it a little each day? When time is short, do you have to give up things like sleep or income, or can you learn to do more in less time?
I don’t claim to answer these questions in the book—some might not have clear answers. But I’ve tried to show how different successful people have dealt with these challenges. I wanted to illustrate how big creative ideas are built from small daily actions and how work habits affect the work itself. The book is called Daily Rituals, but it’s really about people’s routines. A routine might seem ordinary, but it can be a powerful tool for using limited resources like time, willpower, and optimism. A good routine can help focus your mental energy and keep you from being controlled by your moods. Psychologist William James believed that having good habits allows us to focus on more interesting things. Ironically, James himself often procrastinated and couldn’t stick to a schedule.
This book started from my own procrastination. One Sunday, I was alone at the architecture magazine where I worked, trying to write a story due the next day. Instead of working, I was tidying my desk and making coffee. I’m a morning person, able to focus well in the early hours but not so much after lunch. To feel better about this, I searched online for other writers’ work schedules. I found them easily and thought someone should collect these stories. That’s how I started the Daily Routines blog and eventually this book. The blog was simple; I posted descriptions of people’s routines from biographies and articles. For the book, I expanded and researched more, keeping the variety of voices that made the blog popular. I let my subjects speak for themselves through quotes from letters, diaries, and interviews. In other cases, I summarized their routines from other sources. This book wouldn’t be possible without the work of many biographers, journalists, and scholars. I’ve listed all my sources in the Notes section, which can also guide further reading."
7,C1,"Howard became a palaeontologist because of a change in interest rates when he was six years old. His father, who was careful with money and had a big mortgage, decided they couldn't afford a holiday to Spain. Instead, they rented a chalet on the English coast. One rainy August afternoon, Howard found an ammonite* on the beach. From then on, he knew he wanted to be a palaeontologist. By the end of university, he knew what kind of palaeontologist he wanted to be. He wasn't interested in the popular areas like the Jurassic period or dinosaurs. He was more interested in the very beginnings of life, studying ancient creatures in grey rocks. 
When he finished his doctoral thesis, he worried about finding a job, especially in the kind of place he wanted. He was confident in his abilities but knew that deserving something doesn't always mean you get it. The process of getting an academic job can be unfair. When a job at Tavistock College in London was available, he applied, but he wasn't very hopeful. On the day of his interview, the professor who was supposed to lead the panel had an argument with his wife, crashed his car, and ended up in the hospital. The interview went on without him, and the professor who replaced him didn't like the candidate the first professor supported. This led to Howard getting the job. Howard was surprised and grateful, but later learned the real reason he got the job. He was a little disappointed but happy to have the job he wanted.
Howard often thought about how his professional life was organized and planned, unlike the chaos of personal life. He realized how strangers could change your life, like when his briefcase with lecture notes was stolen at an Underground station. Angry, he went back to the college, postponed his lecture, and reported the theft. Then he had coffee with a colleague and a visiting curator from the Natural History Museum in Nairobi. He learned about a new collection of fossils there, which would be his biggest challenge and secure his career. If his briefcase hadn't been stolen, he wouldn't have known about this opportunity. He quickly changed his plans, deciding not to go to a conference in Stockholm or take students to Scotland. Instead, he focused on finding the money to visit the museum in Nairobi. 
*An ammonite is a type of fossil."
8,C1,"Charles Spence is willing to try almost any food. ""We have ice cream made from bee larvae at home,"" says the Professor of Experimental Psychology at Oxford University in the UK. Although they look like maggots, they taste ""slightly nutty and floral."" Making bug-eating acceptable is just one of the challenges Spence and his team are working on. Through his research on how our senses work together to create the perception of flavor, Spence is quietly influencing what we eat and drink. This includes working with big food companies and top restaurants. 
Spence and his colleagues study how we experience food and drink, a field informally known as gastrophysics. They look at details like who we eat with, how food is presented, the color and texture of plates and cutlery, and background noise—all of which affect taste. Spence's book, ""The Perfect Meal,"" co-written with Betina Piqueras-Fiszman, offers many interesting insights. For example, the first person to order in a restaurant usually enjoys their food the most. Also, we eat about 35% more when dining with one other person, and 75% more with three others.
Spence's lab in Oxford is simple and low-tech, with soundproof booths and old audio-visual equipment. By keeping costs low, he can work creatively with chefs who can't afford academic research. Much of his work is funded by a major food company. In the past, research funded by industry was seen as less serious in universities. However, now universities are encouraged to show that their work has an impact, making this research valuable. Spence is helping well-known brands reduce salt and sugar, often secretly, so customers don't notice the change. Research shows that if people know about these changes, they focus on the taste and may not like it as much.
Spence first met Heston Blumenthal, a famous experimental chef, while working on a project for a major food producer. At the time, people thought science and food didn't mix, but most food is scientific. Blumenthal and Spence worked together on the ""Sound of the Sea"" dish at Blumenthal's restaurant. Interestingly, the Italian futurists, an early 20th-century art movement, tried similar ideas with food and sound, but it didn't become popular.
Now, the food industry is using Spence's sensory science widely. For example, his research shows that high-pitched music makes food taste sweeter, while low-pitched sounds make it taste bitter. An airline will soon match music with the food served to passengers. Last year, a brand released a smartphone app that played music while ice cream softened, but they didn't match the music to the taste, which Spence says happens often.
At home, Spence's dinner parties are unique. Once, they ate rabbit with the fur wrapped around the cutlery. Another time, they used remote-controlled, multi-colored light bulbs. They've also had parties with a tone generator, headphones, and different drinks to see if they have different pitches. For Spence, home, shops, food conventions, and international gastronomy conferences are all extensions of his lab."
9,C1,"Our brains are busier than ever. We are bombarded with facts, fake facts, nonsense, and rumors, all pretending to be information. We have to sort through this to find what we need and what we can ignore. At the same time, we are doing more tasks ourselves. Thirty years ago, travel agents booked our flights, and salespeople helped us in stores. Now, we do most things on our own. We are doing the work of many people while trying to manage our lives, families, jobs, hobbies, and TV shows. Smartphones help us fit as much as possible into every spare moment. But there is a problem. We think we are multitasking, doing several things at once, but this is a dangerous illusion. Earl Miller, a neuroscientist at MIT, says our brains are not designed to multitask well. When we think we are multitasking, we are actually switching quickly from one task to another, which has a mental cost. We are not expert jugglers; we are more like amateur plate spinners, quickly moving from one task to another, worried that something will go wrong. Even though we think we are getting a lot done, multitasking makes us less efficient. It increases stress hormones like cortisol and adrenaline, which can make our thinking unclear. Multitasking creates a craving for new things, rewarding the brain for losing focus. The prefrontal cortex, a part of the brain, is easily distracted by new things. This is a problem because we need this part of the brain to stay focused. Just having the chance to multitask can hurt our thinking. Glenn Wilson, a former professor of psychology, calls it info-mania. His research found that trying to focus on a task while an email is unread in your inbox can lower your IQ by almost 10 points. Wilson showed that the mental losses from multitasking are worse than those from being tired. Russ Poldrack, a neuroscientist at Stanford University, found that learning new information while multitasking sends the information to the wrong part of the brain. If students do homework and watch TV at the same time, the information goes to the striatum, which stores new skills, not facts and ideas. Without TV, the information goes to the hippocampus, where it is organized and easier to remember. Multitasking also requires decision-making, like whether to answer a text message or not. Decision-making is hard on our brains, and small decisions use the same mental resources as big ones. After making many small decisions, we can make bad decisions about important things. When talking about information overload, email is often mentioned as a problem. It is not about email itself, but the huge amount of communication. A colleague's 10-year-old son said his father ""answers emails"" for a living, which is almost true. We feel we must reply to emails, but it seems impossible to do so and get other things done."
10,C1,"In a Swedish zoo, a chimpanzee named Santino would break concrete into pieces at night to throw at visitors during the day. Was he being mean? In the US, female bats help other fruit bat mothers if they can't find the right position to give birth. Are they being kind? Fifty years ago, these questions were not considered important. Scientists focused on animal behaviors and their outcomes, not on whether animals have feelings or moral systems. But recently, this has started to change. Research on animals like bats, chimps, rats, dolphins, and chickens has begun to explore animal emotions. This change has led to popular science books like Mark Bekoff’s ""Wild Justice"" and Victoria Braithwaite’s ""Do Fish Feel Pain?"". This has started a debate: do animals have consciousness? This leads to another question: do animals have a conscience, a sense of right and wrong? 
In a recent experiment, cows had to open a locked gate to get food. Those that opened the gate themselves showed more pleasure by jumping and kicking than those that had the gate opened for them. If cows enjoy solving problems, what does this mean for how we produce and eat beef? The observations are clear, but their meaning is debated. Dr. Jonathan Balcombe, author of ""Second Nature,"" believes the logical response is to stop eating meat. He thinks humanity is on the verge of a major ethical change, like the end of slavery. Aubrey Manning, a professor at Edinburgh University, says we should rethink how we see animal intelligence. He believes animals have a simpler version of a ""theory of mind"" than humans. Professor Euan MacPhail thinks we should stop giving animals human-like qualities. The disagreement is not just scientific or moral, but philosophical. Since defining consciousness is difficult, can we ever know what it is like to be a bat?
Balcombe describes an experiment that suggests starlings, a type of bird, can feel depressed. At Newcastle University, starlings were split into two groups. One group lived in nice cages with lots of space and water, while the other group lived in small, empty cages. Both groups learned to eat tasty worms from one box and avoid unpleasant worms from another. Later, when only unpleasant worms were offered, only the birds in nice cages would eat. Balcombe concluded that the birds in bad cages felt pessimistic about life. Balcombe, who works with animal rights groups, has a clear bias. He says, ""We look back with horror at times of racism. One day, we will feel the same about how we treat animals. We can't support animal rights while eating a cheeseburger."" If Balcombe were the only one with this view, it might be easy to dismiss him. But Professor Aubrey Manning shares his views. Manning, who wrote a textbook on animal behavior, says, ""We are seeing a change. In the early 20th century, people thought animals thought like us, and there was a reaction against that. Now we are swinging back. But it is a controversial topic, and you want to avoid the noise of academics with personal opinions."""
11,C1,"Critical thinking, also known as analytical thinking, is a way to understand what we read or hear more deeply. Adrian West, from the Edward de Bono Foundation U.K., says that people often think arguments help find the truth. While technology helps us store and process information, it might also change how we solve complex problems, making it harder to think deeply. West points out that we are surrounded by a lot of poor but attractive ideas and opinions, which can overwhelm our ability to reason. Surprisingly, having more data doesn't always lead to better knowledge or decisions. 
The National Endowment for the Arts reports that reading literature has dropped by 10%, and this decline is speeding up. Patricia Greenfield, a psychology professor, believes that focusing more on visual media, like TV and video games, might reduce critical thinking. She says that less reading might be linked to this decline because people now focus more on real-time media and multitasking instead of concentrating on one thing. However, we still don't have a clear answer on how technology affects critical thinking. 
Technology has changed how we think, says Greenfield, who studied over 50 research papers on learning and technology. She notes that reading helps develop imagination, reflection, and critical thinking, unlike visual media. But visual media can improve some types of information processing. Unfortunately, most visual media don't allow time for reflection or analysis. As a result, many people, especially younger ones, might not reach their full potential.
How society views technology affects how it sees critical thinking. This is especially true with video games. James Paul Gee, an educational psychology professor, says that video games are not just entertainment; they can also be good learning tools. Research shows that games like Sim City and Civilization help develop decision-making and analytical skills. These games let players explore ideas and concepts in ways that might not be possible otherwise. In today's digital age, as reading and math scores drop, it's important to understand how technology affects thinking and analysis."
12,C1,"Matthew Crawford is a writer and motorcycle mechanic. He left his office job because he was unhappy with it. His first book talked about the benefits of manual work. His latest book is about dealing with modern life. He got the idea for this book when he noticed ads on a credit card machine while shopping. Crawford realized that these ads are hard to avoid and take our attention away from what we want to think about. This makes it difficult to remember conversations or think clearly. Because of constant interruptions, people avoid talking to strangers and close themselves off.
Crawford says we often see the world through things like video games and phone apps, which can manipulate us. These things reflect our desires and can take over our lives. Many people are worried about this, like office workers who complain about emails but spend their free time on them. Studies show that just seeing a phone can distract us. There is no proof yet that our attention spans are shorter, but we are more aware of other things we could be doing.
Crawford believes technology has made it easy for us to focus on ourselves. With so many choices, it's hard to control ourselves, which affects society. We prefer texting to talking because it's easier. By only interacting with screens, we might lose important social skills. Crawford gives an example of his gym, where people used to share music. Now, everyone listens to their own music with earbuds, making the gym less social. He says real connections happen when people work through differences together.
Crawford suggests two solutions. First, we need rules to reduce noise and distractions in public. More importantly, he thinks we should engage in skilled activities, like cooking or playing sports, to connect with the real world. These activities require good judgment and dealing with others. They show that real experiences are better than virtual ones. Crawford doesn't mean everyone should become a chef, but that using your judgment is important. This helps you focus better and resist distractions."
13,C1,"""‘What do you do for a living?’ is a common question we ask each other. We often define ourselves by our jobs. Usually, we answer this question in a few words. But when you are a philosopher, like me, it can be a bit tricky. Calling yourself a philosopher might sound a bit arrogant. Saying you study or teach philosophy is okay, but saying you are a philosopher might make people think you believe you have special knowledge or wisdom. This is not true; philosophers are just like everyone else. But this idea makes me pause. Why do people think this way about philosophers? One reason is that philosophers are seen as people who judge others' actions and value intellectual life. The Greek philosopher Aristotle (384 – 322 BCE) said that a life of thinking, or a philosophical life, was the best kind of life. Few modern philosophers would agree completely, but philosophy is still linked with deep thinking. Another Greek philosopher, Socrates, said, ‘the unexamined life is not worth living’. He meant that just accepting what society says is not satisfying. Our ability to think about the world helps us control our lives and make our own choices. But living an examined life doesn’t mean you have to read a lot of philosophy books or spend all your time thinking. It means looking closely at your daily life to see if it deserves its importance. You don’t have to be a wise person living away from society to do this. In fact, to help guide people’s lives, the examined life should be practical and involve sharing knowledge, as it’s important for a good life. Another reason people misunderstand philosophers is that academic philosophy has become more separate from everyday life, especially for people who haven’t studied it formally. This isn’t entirely philosophers' fault: universities focus on expensive and hard-to-access academic journals. So, philosophers often only talk to other experts in their field. For most people, philosophy can seem far from reality. If philosophers used to challenge this idea, many don’t anymore. The university system created this isolated environment, and academics have supported it. As some areas of philosophy have become more focused on specific, technical debates, explaining them to other philosophers is hard enough, let alone to people who haven’t studied philosophy. In some cases, philosophy has moved away from society. This needs to change. I sometimes call myself an ‘ethicist’ because I work in ethics, a part of philosophy that looks at human actions. But recently, I feel this title doesn’t fully describe my work because ethics is often linked to rules, values, and laws. This is a new and not well-explored change in philosophy: ethics now often means applied ethics, which looks at whether certain social practices are fair. The job of an ethicist today is to decide if an action is ‘ethical’ or acceptable. These are important questions, and I often think about them. But philosophy is more than this. A typical discussion might start by asking if illegally downloading films is wrong (it is) and then move to questions about responsibility, our views on art, and the impact of consumerism. In this way, philosophy can help people look more closely at the actions and behaviors that shape their lives. Sometimes this shows us something we already know; other times, we find out our beliefs are hard to defend. Either way, by examining these ideas, we help everyone."""
14,C1,"Food lovers, chefs, and others who value food might think that thinking deeply about what and how we eat can make eating more enjoyable. However, in history, philosophers have often used food to talk about other topics, like learning. Sometimes, discussions that seem to be about food are actually about other, loosely related ideas. For example, the ancient Greek philosopher Epicurus talked about seeking pleasure and avoiding pain in many areas of life. Yet, his name is now often linked with a love for eating and drinking. We see his name used in restaurants, food shops, and recipe websites, likely because businesses think it will help them sell more if people associate their products with famous philosophers.
Food is naturally social and cultural. It doesn't just appear; it comes from history and is shared with people in our communities. These communities give us people to eat with and the systems to grow and distribute food. We interact with food more often and in more basic ways than any other product, making it an interesting topic for philosophy. Once food is made, critics start talking and writing about it. But why do they have such a special role? One philosopher says that tasting food is not a special skill; food critics are just better at describing tastes. Philosophically, the way we taste hasn't been studied much, as most research on perception focuses on sight, which should change.
Another part of food is its beauty. We often say paintings or music are beautiful, but not food. Some philosophers argue that with modern cooking, food should be part of discussions about beauty, like art or poetry. But food is eaten and disappears, unlike art or music, which lasts over time. So, some think food can't be considered beautiful in the same way.
There are many ethical questions about food. We can ask what we should eat: organic, free-range, local, vegetarian, or non-genetically modified foods? Our choices often show our ethical beliefs, and philosophers explore why we make these choices, which is important for food discussions.
Cooking at home and in restaurants is different. Home cooks have a special duty to their guests because of personal relationships. At home, everyone shares food and friendship. Professional cooks have responsibilities to their employers and the food, so their kitchens are only for qualified people, and their relationships are professional.
A recent essay called ‘Diplomacy of the Dish’ looks at how food can help bridge cultural gaps. This happens in two ways: by enjoying food from other cultures and by eating together, which has a long history in international diplomacy. The essay includes many interesting examples and stories, making this part of food philosophy engaging for readers."
15,C1,"Rob Daviau, from the US, creates 'legacy' board games. He felt that the board games he played as a child were not fun or challenging anymore, so he thought about how they could change. Could they have a story? Could choices made in one game affect the next one? He changed a classic game called Risk to make a new version: Risk Legacy. In this game, decisions made during play have lasting effects. Players might have to tear up cards, write on the board, or open packets with new rules at important moments. The game is played over a set number of sessions, and long-term rivalries become part of the game. Daviau said: ‘You could point to the board and say: “Right here! You did this!”’
Daviau was then asked to help create a legacy version of Pandemic, a popular game where players work together to cure diseases. His next project was a game called SeaFall. While Pandemic Legacy was very popular, SeaFall was seen as a real test of the legacy format because it was the first game not based on an earlier version. Set in the age of sail (16th – mid 19th century), players become sailors exploring a new world. Legacy game designers must think about all possible player choices to keep the story together. To do this, they use testers to see how the game will play out. Jaime Barriga was a tester for SeaFall. ‘It takes a lot of time,’ he said. ‘Sometimes it starts great, but after a few games, it gets shaky, and by game six it’s broken. Then you have to fix everything.’
Legacy games were not expected to become popular. Even Daviau thought it would be a small interest. ‘When I was working on it, I thought: “This is really different and I think it’s pretty good,”’ he said. ‘But it’s strange, unexpected, and breaks many rules. I thought it would be popular with only a few people. I thought I would be known as the guy who did this strange project.’ However, most players, like Russell Chapman, loved the idea. He sees it as a big step forward in game design. ‘It’s a new level of commitment, intensity, excitement,’ he said. ‘There’s nothing more exciting for a board gamer than learning a new game or way to play, and you get that all the time with legacy games.’
Another fan, Ben Hogg, liked the adventure of the game more than worrying about its lasting value. ‘At first, I was worried about changing and basically ruining your board as you played,’ he said. ‘But Pandemic Legacy changed that. Most people don’t watch the same movie twice, do they? You’re buying an experience. It’s like the story you get from video games.’ The legacy format is inspired by video games and the demand for episodic entertainment from popular TV series. While in college, Daviau wanted to be a TV writer but moved to advertising and then game design. Still, he loved telling stories. Pandemic creator Matt Leacock compares designing a legacy game to writing a novel. ‘You need to know how you want it to end and have a good idea of where to start,’ he said. ‘But it’s not enough to just have a basic plan.’
While Daviau feels proud of his work, he is interested in seeing how others use the idea. But he also thinks people might soon ask: ‘What’s next?’ Colby Dauch, studio manager for the publisher of SeaFall, is not so sure. For him, the legacy format has been a big discovery. ‘It’s the kind of idea that changes how you think about what a board game can be.’"
16,C1,"One night, an octopus named Inky escaped from his tank at New Zealand’s National Aquarium. He moved across the floor and squeezed into a drain that led to the Pacific Ocean. This story, which sounds like a children's movie, was shared widely online. Stories like this are fun because they make us think animals are like us. This is especially true for octopuses, which are very smart but look very different from humans. They can open jars, recognize faces, use coconut shells as armor, and even play in complex ways.
People often think that giving animals human traits is unscientific. However, Dr. Frans de Waal, who studies primates like gorillas and chimpanzees, believes the opposite is true. He says that not recognizing human-like traits in animals, which he calls ""anthropodenial,"" is more common. By studying animal behavior, he shows that animals can do many things we thought only humans could do, like thinking about the past and future, showing empathy, being self-aware, and understanding others' motives. Animals are smarter than we often think.
In the past, people believed animals had complex minds. In medieval Europe, animals could even be put on trial for crimes. In the 19th century, naturalists looked for connections between human and animal intelligence. Charles Darwin, who developed the theory of evolution, said the difference between humans and higher animals is one of degree, not kind.
In the 20th century, behaviorism changed how people viewed animal intelligence. Animals were seen as machines that responded to stimuli or as robots with instincts. People who thought animals had inner lives were considered unscientific. This view came at a time when humans were destroying animal habitats and ignoring animal welfare.
Dr. de Waal believes we are now starting to see animal intelligence as similar to human intelligence, though not the same. He notes that recent decades have brought a lot of new knowledge about animals, shared widely on the internet. The best tests of animal intelligence consider the specific traits and skills of each species. For example, squirrels might not do well on human memory tests, but they can remember where they hid nuts. In her book, ""The Soul of an Octopus,"" naturalist Sy Montgomery suggests that if an octopus tested human intelligence, it might judge us on how many color patterns we can make on our skin. We would fail that test, and the octopus might think we are not very smart.
Dr. de Waal is not sure if Inky really found his way to the ocean. While octopuses have escaped before, it might be too hopeful to think Inky knew where he was going. However, he understands that viral stories can help people appreciate animal intelligence. He once did an experiment to see if capuchin monkeys feel envy. When some monkeys got cucumbers and others got grapes, the monkeys with cucumbers got upset because grapes are better. The study was published in a scientific journal, but a short video of the experiment convinced more people of the findings. This shows how our minds work in interesting ways."
17,C1,"Robotics, once only seen in science fiction, is now becoming a major change in technology, similar to the impact of industrialization. Robots have been used in car manufacturing for many years, but experts say that soon, robots will be used in many more areas, and many countries are not ready for this big change. Most people agree that robots will take over many jobs in the next 50 years, but they still believe their own jobs will remain the same. This is not true, as every industry will be affected by robots soon. For example, an Australian company, Fastbrick Robotics, has created a robot called Hadrian X that can lay 1,000 bricks in one hour, a job that would take two human workers almost a day. In San Francisco, a company called Simbe Robotics has made a robot named Tally that moves around supermarkets to check if products are stocked and priced correctly.
Supporters of robots say that robots cannot yet take care of or program themselves, which means new jobs will be created for skilled workers like technicians and programmers. However, critics warn that we should not ignore the importance of human interaction at work. Dr. Jing Bing Zhang, an expert in robotics, studies how robots are changing the workforce. His research shows that in two years, many robots will be smarter and able to work with humans. In three years, many top companies will have a chief robotics officer, and some governments will have laws about robots. In five years, salaries in the robotics field will rise by 60%, but many jobs will still be unfilled due to a lack of skilled workers.
Dr. Zhang believes that people with lower skills will be most affected by automation and robotics. He suggests that they should not rely on the government to protect their jobs but should instead look for ways to learn new skills. As technology advances, new types of robots will be created for consumers, such as robots that live with us at home and interact with us in new ways. This presents a great opportunity for companies but also requires new rules to protect our safety and privacy. With many jobs at risk, education is key to preparing for the future workforce. Developed countries need more graduates in science, technology, engineering, and math (STEM) to stay competitive."
18,C1,"George Mallory, a famous mountaineer, once answered a reporter's question about why he wanted to climb Mount Everest by saying, ""Because it’s there."" The reporter's question reflects a common curiosity: why would anyone risk their life for something that seems pointless? Mallory's answer might inspire some people to follow their dreams and face the unknown. It suggests that climbing might be about adventure and fun.
In 1967, Bolivian writer Tejada-Flores wrote an important essay called ‘Games that Climbers Play’. He described seven different types of climbing activities, or ‘games’, each with its own rules. He said that the style of climbing, like using more or less equipment, should match the rules of the game. Over the years, this idea has become very popular in Western climbing culture, influencing everything from magazines to discussions around campfires.
Many climbers love the feeling of freedom that climbing gives them. However, climbing can also be very limiting, like when climbers are stuck in a tent during a storm. This shows a paradox: climbing can feel both freeing and restricting. But some argue that the simplicity of having limited choices is a form of freedom.
US rock climber Joe Fitschen offers a different perspective in his essay. He suggests asking, ""Why do people climb?"" instead of ""Why climb?"" Fitschen believes climbing is in our genes; it's natural for humans to take on challenges and test limits, even if it's risky. So, the joy of climbing might be more about biology than logic.
US academic Brian Treanor adds another view. He thinks climbing helps develop important virtues like courage, humility, and respect for nature. While not all climbers show these virtues, Treanor believes they are important in our modern, risk-averse world. Climbing can help people develop traits that are useful in everyday life.
Another idea is that climbers who don't rely on others or technology must be fully committed to succeed. Expert climbers Ebert and Robinson argued that climbing achievements are more impressive when done independently, without help like bottled oxygen. This view caused some controversy, as it might encourage climbers to take unnecessary risks.
Finally, climbing can be seen from a non-Western perspective. Many climbers talk about being ""in the moment"" or ""in the zone"" while climbing. The physical effort, the meditative focus, and intuitive problem-solving are key parts of climbing. Some say these aspects are similar to Zen philosophy, which aims for a state of perfect peace. This offers another reason why people are drawn to climbing."
79,C2,"In recent years, both in the UK and around the world, there has been a trend in higher education to get new students involved in research early in their studies. This change shows that research is not just for famous scholars at old universities or scientists making big discoveries. Instead, research is a natural and important way to learn and develop skills. Research skills are useful not only in your studies but also in your job because they help you think about the world and how you work. As a student, you contribute to knowledge. You don't just learn and repeat it; you create it. Creating knowledge involves asking questions like Why? How? When? What does this mean? How might that be done? What if this were different? How does it work in that context? Why does it matter? These questions are at the heart of what we call research.
Research can be seen as a range of activities. On one end, there is the complex, groundbreaking research done by highly trained experts, which leads to big changes and new knowledge. On the other end, research can be everyday inquiries with a strong design, careful work, and thoughtful questions about issues, practices, and events. Most students have been researchers in some way. You have done research for school projects and answered questions since your early days at school or work. You have asked questions that led to investigations and research since you first became interested in studying. You have also developed research skills when planning a holiday, growing plants, fixing things at home, training a pet, choosing a music system, or shopping online.
In college and higher education, having a curious mind, identifying problems and questions, critically exploring and evaluating information and ideas, and creating your own responses and knowledge are expected learning activities. Some students might find this challenging because, in some cultures, knowledge is seen as already established, and you learn by listening to teachers and texts. It might seem disrespectful to question established knowledge and authorities, and you might feel you need to be told what is important to learn. However, in the UK, US, much of Europe, and Australasia, questioning established knowledge and authorities is encouraged. This process of inquiry and knowledge creation can seem daunting. Critical thinking is very important in research. The research of others is useful to students, academics, and in the workplace, but we need to do more than just repeat what we read. We need to engage with it, think about it, test it, and see if it is logical, reasoned, and supported by evidence. We should not blindly accept facts and information given to us by others."
80,C2,"Cities have always been places where people share ideas. In the 18th century, people in London met in coffee houses to talk about science and politics. In modern Paris, artists like Pablo Picasso discussed art in cafés. However, living in a city is not always easy. Those same London coffee houses also spread diseases like cholera, and Picasso eventually moved to the countryside. Cities are full of creativity, but they can also be stressful and unnatural.
Scientists are now studying how city life affects our brains, and the findings are concerning. We have known that city life is tiring, but new research shows it can also make us think less clearly. One reason is the lack of nature, which is surprisingly good for our brains. Studies show that hospital patients heal faster when they can see trees. Even a quick look at nature can help our brains because it gives us a break from city life.
This research is important because, for the first time, most people live in cities. Instead of open spaces, we live in crowded areas with many strangers. These unnatural surroundings affect our mental and physical health and change how we think. Walking down a busy street, our brains have to keep track of many things: distracted people, traffic, and the confusing city layout. This constant attention is tiring because our brains have to decide what to focus on and what to ignore.
Natural settings, on the other hand, don't require as much mental effort. This idea is called attention restoration theory, developed by psychologist Stephen Kaplan. He suggested that being in nature helps restore our attention. Nature captures our attention without causing stress, unlike city noises like police sirens. This allows our minds to relax and recharge.
Before scientists studied this, philosophers and landscape architects warned about the effects of city life and tried to bring nature into cities. Parks like Central Park in New York offer a break from urban life. A well-designed park can improve brain function quickly. While people try many things to boost brain performance, like energy drinks or changing office layouts, simply walking in nature seems to be more effective.
Despite the mental challenges of city life, cities continue to grow. Even in the digital age, they remain centers of intellectual life. Research from the Santa Fe Institute shows that the same city features that tire our brains, like crowded streets, also lead to innovation. The many social interactions in cities drive creativity. Just as 18th-century London was a hub of new ideas, modern cities like Cambridge, Massachusetts, are centers of technology and creativity. Less crowded cities might produce less innovation over time.
The challenge is to reduce the mental stress of city life while keeping its benefits. As the saying goes, sometimes people get tired of nature and want to return to the city."
81,C2,"Where should we look for the mind? It might seem obvious to say that thinking happens inside our heads. Today, we have advanced brain-scanning technology to show this. However, I believe that the study of the mind should not stop at the brain. There is a lot of evidence, from ancient times to now, showing that objects, as well as brain cells, are part of human thinking. Archaeology shows that things like stone tools, jewelry, carvings, clay tokens, and writing systems have played a role in human evolution and the development of the mind. So, I suggest that what is outside the head might also be part of the mind.
It is easy to see why people think the mind and brain are the same. Most of what we know about the mind comes from studying people without the objects they usually have around them. This makes sense for neuroscientists because of the limits of brain-scanning machines. But this often makes us forget that much of our thinking happens outside our heads. I am not saying that the brain is not important for thinking, but that the mind is more than just the brain. It might be helpful to think that human intelligence extends beyond the body into culture and the material world.
This is where my new theory, Material Engagement Theory (MET), comes in. MET looks at how objects become part of our thinking, like when we make numbers and symbols out of clay or use a stone to make a tool. It also studies how these ways have changed over time and what that means for how we think. This approach gives us new ideas about what minds are and what they are made of by changing what we know about how objects help the mind.
Think of a blind person with a stick. Where does this person’s self begin? The connection between the blind person and the stick shows how minds and objects can be continuous. It also shows how flexible the human mind is: using a stick, the blind person turns touch into sight, and the stick plays an active role. The brain treats the stick as part of the body. This reminds us that human intelligence can change a lot by using new technology.
I see the human mind as always evolving. It is important to remember that, throughout history, the 'stick' has been a way to explore the world, not a limit. Through the 'stick', humans feel, discover, and understand the environment, and also find new ways forward. This is different from a monkey using a stick to get food. For humans, 'sticks' are used to satisfy our curiosity. This unique human tendency to engage with objects explains why we make things and how those things shape our minds. I call this metaplasticity – our minds are flexible and change as they interact with the material world.
I want to include material objects in our understanding of the mind. MET offers a new way to understand how different objects, from stone tools to smartphones, have helped define and change what we are and how we think. Mind-changing technology might sound futuristic, but humans have used it since they first evolved."
82,C1,"Photography is one of the few inventions that has greatly changed how we see the world and our culture. This is especially true in the United States, where photography quickly became a part of everyday life. It was used in many areas, such as science, industry, art, and entertainment. Historians say that photography might be the greatest contribution of the US to the visual arts. No other art form from the US has been as influential. To understand its impact, we need to look at its beginnings in the mid-19th century.
Why was photography so popular? First, it was a mechanical process, which matched the growing interest in technology in the US. Just like steam power, railroads, and electricity made the world feel smaller by improving communication and travel, photography brought the wonders of the world into people's homes. Second, the camera was a tool that helped people show their identity. In the US, it was common to create and recreate personal and national identities. Third, the camera was important for creating family memories, even if they were idealized. Lastly, the realistic nature of photographs matched the US artists' focus on realism and everyday life.
A photograph draws attention to something the photographer wants us to see. It is a record of events, people, or things, showing what was in front of the camera. This gives photographs a sense of objectivity. However, since a person takes the photograph, it also has a personal point of view, or subjectivity. We might think we understand a photograph because we recognize the subject, but its meaning can be complex. No image is shown without some context, like a caption in a newspaper or its placement in a gallery, which affects how we understand it.
To understand a photograph historically, we need to consider its purpose and how it was first seen. The same photograph can be seen in different places and times, and its meaning can change. The camera's role in art was once a secret, but now it is widely used by contemporary artists. Since the time of US artist Andy Warhol, who used photographs in his art, artists have been incorporating photographs in various ways. In short, photography has become an important part of art, working alongside painting to shape our ideas of representation before the camera was invented."
83,C1,"In 1890, William James, an American philosopher and one of the founders of modern psychology, described psychology as the ""science of mental life."" This definition is still useful today. We all have a mental life, so we have some idea of what this means. However, understanding mental life can be tricky, even though it can be studied in animals like rats and monkeys, as well as humans.
James was mainly interested in human psychology. He believed it included basic elements: thoughts and feelings, the physical world around us, and how we know about these things. Our knowledge is personal and comes from our own thoughts, feelings, and experiences. It might be influenced by scientific facts, but often we use our own experiences to make judgments about psychological issues. We act like amateur psychologists when we give opinions on complex topics, like whether brainwashing works, or why people behave in certain ways, such as feeling insulted or unhappy.
Problems occur when people see things differently. Formal psychology tries to find methods to decide which explanations are most likely correct in a situation. Psychologists help us tell the difference between subjective thoughts, which can be biased, and scientific facts. Psychology, as James defined it, is about the mind or brain. Although psychologists study the brain, they don't fully understand how it affects our hopes, fears, and behaviors. It's hard to study the brain directly, so psychologists learn more by observing behavior and making guesses about what's happening inside us.
A challenge in psychology is that scientific facts should be objective and verifiable, but the mind's workings aren't directly observable like an engine. We can only see them indirectly through behavior. Psychology is like solving a crossword puzzle, using clues from careful observation and analysis. These clues must be measured accurately and interpreted logically.
Psychology aims to describe, understand, predict, and control or change the processes it studies. Once these goals are met, psychology can help us understand our experiences and apply findings to real life. Psychological research has helped in areas like teaching children to read, designing safer machines, and helping people express their feelings better.
Psychological questions have been discussed for centuries, but scientific investigation began only about 150 years ago. Early psychologists used introspection, or looking into one's own mind, to answer questions. They wanted to identify mental structures, but introspection has limits. As Sir Francis Galton noted, it only shows a small part of brain activity. William James compared it to trying to see darkness by quickly turning on a light. Today, psychologists prefer to base their theories on careful observations of behavior rather than personal reflections."
84,C1,"In a warehouse in a business park in Michigan, USA, there is a place called the Museum of Failed Products. This museum shows the other side of consumer capitalism, where not all products succeed. Here, you can find unusual items like A Touch of Yogurt shampoo and Breakfast Cola, which were taken off the market because almost no one wanted to buy them. The museum is owned by a company called GfK, and its owner, Carol Sherry, believes each product tells a sad story about the people who designed, marketed, and sold it. 
What is surprising about the museum is that it is a successful business. You might think that companies would keep their own collections of failed products, but they often don't. Many business executives visit the museum because they didn't keep samples of their own failed products. The museum started by accident. Robert McMath, a former marketing professional, wanted to create a 'reference library' of consumer products, not just failures. Since the 1960s, he collected samples of every new product he could find. He discovered that most products fail, so his collection mostly consists of unsuccessful ones.
Today's culture of optimism might explain why these products ended up in the museum. Each product went through meetings where no one realized it would fail. Even if they did, marketers might have spent more money on a failing product to try to make some sales and save face. People involved often don't talk about what went wrong, focusing instead on being positive.
This focus on optimism is common in the growing 'self-help' industry. One popular method is 'positive visualization,' which suggests that imagining success makes it more likely to happen. Neuroscientist Tali Sharot found that people tend to believe things will go better than they actually might. Her research shows that well-balanced people are often overly optimistic about their ability to influence events, unlike those with depression.
Psychologist Gabriele Oettingen studied whether 'positive fantasies about the future' are effective. Her research found that thinking too much about success can actually reduce motivation to achieve goals. For example, people who imagined having a very successful week at work often achieved less.
Psychologist Carol Dweck says our beliefs about ability affect how we handle failure. People with a 'fixed theory' mindset think ability is natural and unchangeable, so they see failure as proof they aren't good enough. For example, a sports star who believes he is a 'natural' might not practice enough and fail to reach his potential. On the other hand, people with an 'incremental theory' mindset believe ability can grow with effort and challenges. They see failure as a sign they are pushing their limits. Dweck compares this to weight training, where muscles grow stronger after being pushed to their limits. Having an incremental mindset can lead to a happier life, whether or not it leads to success."
85,C1,"Sports teams often have an advantage when they play at home, but why is that? Many people think they know the reason, but professional sports are changing quickly, and what we used to believe is now being questioned. Two main factors are challenging the idea of home advantage: science and money. Sports scientists are studying what helps players perform their best, and they have many theories about home advantage. On the other hand, people who invest money in sports wonder if home advantage should matter at all. If players are paid well, shouldn't they perform well anywhere?
What about the fans? Would it matter if a team like Manchester United played some home games in the Far East to reach fans there? It would matter to British fans who believe their support is crucial for the team. Fans often think that their cheering leads to goals, but they forget the times when their support didn't result in a goal.
However, there is one thing that fans seem to get right. Home fans often try to influence the referee. In an experiment, referees watched a match with and without crowd noise. Those who heard the noise were less likely to call fouls against the home team. This suggests that referees try to avoid making decisions that would upset the home crowd.
Studies show that home advantage has decreased in all major sports, but not as much as expected. For example, in the 1890s, home teams in English football won about 70% of the points, compared to 60% today. Travel used to be difficult, but now players travel in comfort, and stadiums are more similar to each other. Despite these changes, home advantage still varies by sport. Basketball has the most home advantage, followed by football, while baseball has the least. This might be because basketball relies more on teamwork, which is boosted by playing at home.
Another reason for home advantage could be related to players' testosterone levels, which are higher before home games. This might be a natural urge to defend their home ground. In recent rugby matches, underdog teams playing at home won against stronger opponents, showing the power of home advantage. As one referee said, ""It's the law of the jungle out there."""
86,C1,"""‘The more I practise, the luckier I get,’ said golfer Gary Player about 50 years ago. This saying is famous in sports and is important to athletes and their coaches. The saying is interesting for two reasons. First, many people argue about who actually said it first. Second, its meaning is not clear. Player didn’t really mean 'lucky'; he was speaking ironically. What he meant was: ‘The more I practise, the better I get.’ But how much better can you get with practice? This question is part of the nature-nurture debate, which asks if talent is something you are born with or something you learn. However, calling it a debate is not quite right because it suggests both sides are equal. In reality, the idea that practice is more important than natural talent has been more popular. This idea comes from psychologist Anders Ericsson, who said that you can become an expert in anything with 10,000 hours of practice. This theory became popular through books like Malcolm Gladwell’s ""Outliers"" and others like ""Talent Is Overrated"" and ""Bounce: The Myth of Talent and the Power of Practice."" These books say that practice is the most important thing and that 10,000 hours is enough to make anyone an expert. But, as David Epstein argues in his book, this is not always true. He shows many examples to prove his point. Epstein starts by pointing out a problem with studies on what makes people excellent: they often only look at successful people. He asks if there are other ways to understand why some people have certain skills and others don’t. He explores how much of this is due to natural ability and how much is due to environment, support, and determination. Epstein travels around the world, from Kenya to Sweden, and even looks at animals. In Alaska, he learns that the key to winning a tough sled dog race might be the dogs’ natural drive, suggesting a mix of genes and learned traits. This challenges the idea that commitment and determination are just choices. What if, like the huskies, a person’s drive is also partly genetic? These examples show how complex the topic is. Epstein talks about 'hardware' (nature) and 'software' (nurture) and says that for top athletes, you need both. He doesn’t ignore the importance of training or environment. For example, he suggests that if Usain Bolt had grown up in the US, he might have become a good basketball player instead of the fastest runner. But Epstein also looks at cases where natural ability is so important that you can’t ignore genes. He also discusses race and gender. He asks: ‘If only practice matters, why do we have separate competitions for men and women in sports?’ Sometimes the simplest questions are the most important."""
87,C1,"A recent international report shows that many children in rich countries feel lonely and unhappy. Jay Griffiths asks why this is happening. In her book, ""Kith,"" she suggests that children are unhappy because they spend too much time indoors, in front of screens like TVs and computers, and have lost touch with nature. She believes this is the main problem. This idea is similar to other discussions about childhood today. A follow-up study talked to children and found that many would be happier if they could play outside more. Many adults over 40 remember their own childhoods fondly, when they spent time exploring outside, swimming, or building forts. They think this was healthier than the way children live now. However, they often forget that they are the ones who have made life more protective for their children, avoiding risks like playing in old sheds.
Griffiths' book has strong arguments. She talks about how parents' fear of danger keeps kids indoors, which benefits the toy and gadget industry. She also criticizes trends like giving medication to restless children or making them wear goggles for playground games. It's unclear how common these rules are, but Griffiths expresses her frustration clearly. She also talks about childhood freedoms and rules, from fairy tales to school regulations. Her arguments are interesting, but sometimes she goes too far and ignores important counter-arguments. For example, she compares modern treatment of children to racism, which seems exaggerated.
Griffiths is very romantic about children. She mostly sees them in an idealized way, which might not match reality. She believes children should have freedom to explore and take risks. She argues that children need small accidents to learn how to avoid bigger ones later. However, she doesn't explain how to ensure these accidents are ""the right size."" Also, not all children might want the adventurous life she describes. Some children are naturally shy or cautious. The real issue might be forcing all children into the same lifestyle, whether keeping them inside or pushing them outside."
88,C1,"I am a research bio-psychologist with a PhD, so I have spent a lot of time in school. I am good at solving problems in my work and life, but this is not because of my schooling. Most life problems cannot be solved with complex formulas or memorized answers from school. They need judgment, wisdom, and creativity, which come from life experiences. For children, these experiences come from play. My recent research focuses on the importance of play for children's development. All young mammals, including humans, play, and those with more to learn play the most. Carnivores play more than herbivores because hunting is harder to learn than grazing. Primates play more than other mammals because their lives depend more on learning than on instincts. Children, who have the most to learn, play more than any other young primates when they can. Play is how adults and mammals have always learned. The most important skills for children to live happy, productive, and moral lives cannot be taught in school. These skills are learned and practiced through play. They include creativity, getting along with others, cooperating, and controlling impulses and emotions. Creativity is important for economic success. We no longer need people to follow instructions like robots or do routine calculations. We need people who can ask new questions and solve new problems. If we can develop thinkers who anticipate problems, we will have a strong workforce. This requires creative thinking. A creative mind is a playful mind. Geniuses are adults who keep and build on their childlike creativity. Albert Einstein said school almost destroyed his interest in math and physics, but he regained it after leaving school. He called his innovative work 'combinatorial play.' He developed his theory of relativity by imagining himself chasing a sunbeam and thinking about the consequences. We cannot teach creativity, but we can suppress it with schooling that focuses on imposed questions with one right answer. More important than creativity is the ability to get along with others, care about them, and cooperate. Children are born wanting to play with others, and through play, they learn social skills, fairness, and morality. Play is voluntary, meaning players can quit if they want. This makes play democratic because players must keep others happy to continue the game. School has become more demanding: breaks are shorter, homework has increased, and pressure for high grades has grown. Outside school, adult-directed sports have replaced spontaneous games. 'Play dates' with adults present have replaced unsupervised neighborhood play, and adults now intervene instead of letting children solve their own problems. These changes have been gradual but significant over time. They are due to social factors like parents' fears, experts warning about dangers, less cohesive neighborhoods, and the belief that children learn more from adults than from each other. Our children do not need more school; they need more play. If we care about our children and future generations, we must reverse the trend of the past fifty years. We must give childhood back to children. They must be allowed to follow their natural desire to play and explore, so they can grow into strong adults ready for an unpredictable future."
89,C1,"In many countries, more people in their twenties are using social media to find jobs. Platforms like Twitter and LinkedIn allow them to directly contact potential employers, which used to be possible only by standing outside an office with a ""hire me"" sign. However, with this access, there is also a higher chance of making mistakes. For example, a young jobseeker in the US contacted a senior marketing executive on LinkedIn. This executive had many important contacts, and the jobseeker thought they could help him get a job. But the executive was upset by his request. She refused his contact request and sent a harsh rejection note, which became very popular online. People who saw the note were shocked, and the executive might now regret how she wrote it, even if she still agrees with the message. If this incident makes young people think more carefully about using social media professionally, it might actually help them. It shows that social media can be risky for job seekers who don’t know how to use it properly, and many are making mistakes. 
There is an irony here because, in many countries, social media sites like Facebook and Twitter have been a big part of young people's social lives for years. When my generation was young, social media was a way to escape from parents and teachers. It was like an online playground where we could impress others and try new things. It was mostly about fantasy. You could chat with someone online for hours and then ignore them at school. By choosing the right pictures or songs for your Facebook page, you could become a different and more interesting person overnight. And if you didn’t want to talk, you could just ""poke"" people on Facebook. 
However, using social media for professional networking is very different. For some young people, the difference is not clear. We first became popular online by being bold and confident, which might be why some of us still think this is a good idea. Just because many people liked your posts on Facebook doesn’t mean you can use LinkedIn to show employers you’re worth hiring. We need to understand that what we learned about social networking as teenagers doesn’t work anymore, and we must meet employers’ standards to succeed in the job market. 
One common complaint from employers about young job seekers on professional networking sites is that they are too familiar and seem arrogant. This makes older generations think of us as an ""entitled generation."" In reality, we are far from this; in many countries, we are very worried about finding jobs, which is why we turn to social media. This impression of arrogance can hurt young people's chances of getting a job, even if they have the skills and motivation to be valuable workers. 
So, what is the right way to contact someone on a professional networking site? First, clearly explain who you are and what you can offer them – maybe you could do some research for them or help in another way. This approach gives you a better chance of getting a positive response. Avoid sending impersonal, mass emails, and keep your tone humble to avoid upsetting the recipient. Remember, social media can be a great way to make useful contacts, but it needs careful handling if you don’t want to be rejected."
90,C1,"Anyone who claims they can predict the future of newspapers is either lying or mistaken. If you look at the numbers, newspapers seem to be in trouble. Since 2000, the circulation of most UK national newspapers has dropped by a third to half. In the USA, the Pew Research Centre reports that newspapers are now the main news source for only 26% of people, compared to 45% in 2001. Many people predict that printed newspapers will disappear within 15 years. However, history shows that old media often survive. In 1835, a New York journalist said books and theatre were outdated, but theatre survived newspapers, cinema, and television. Radio has thrived alongside TV, and cinema has competed with videos and DVDs. Even vinyl records have become popular again, with online sales increasing by 745% since 2008.
Newspapers were once new media too, but it took centuries for them to become the main source of news. This happened in the mid-19th century with the steam press, railway, and telegraph. It was also because people started to believe that everything is always changing and they needed regular updates, a concept that was strange in medieval times. Now, we expect change. In medieval times, people only noticed the changing seasons and unexpected disasters like famine or disease. Life was seen as cyclical, with important truths repeating.
Journalism as a full-time job only began in the 19th century. Even then, there was no clear reason why people needed news regularly. Regular newspaper publication can be limiting. Online news allows readers to check updates based on their interests. Search engines and algorithms help personalize news. Online news can provide minute-by-minute updates and correct errors quickly. There are no space limits, and full documents can be accessed. This is very different from traditional newspapers.
However, online news often focuses on being first and creating excitement, rather than spreading understanding. In medieval times, news was shared in markets or taverns, mixed with rumors and misunderstandings. In some ways, we are returning to that. Newspapers have not always been good at explaining how the world works. They might face extinction, or they might find a way to help us understand our complex world."
91,C1,"If humans were comfortable under the light of the moon and stars, we would enjoy walking in the dark, just like many animals that are active at night. However, we are creatures that are active during the day, with eyes suited for sunlight. This is a basic part of our nature, even if we don't often think about it. This is why we have changed the night by adding artificial light. We have filled the night with light, similar to how we control rivers with dams. This has benefits but also causes problems, known as light pollution, which scientists are just starting to study. 
Light pollution happens mostly because of poor lighting design. This means artificial light shines into the sky instead of focusing downwards where it is needed. This extra light changes the natural darkness of night, affecting the light levels and rhythms that many living things, including humans, have adapted to. When artificial light enters the natural world, it can affect important activities like migration, reproduction, and feeding. 
In the past, the idea of 'light pollution' would not have made sense. For example, if you walked towards London on a moonlit night around 1800, you would not see much light from the city, even though it was the most populated city on Earth. People used candles, torches, and lanterns, and only a few houses had gas lights. There were no public gaslights in the streets until seven years later. 
We have lit up the night as if it were empty, but it is not. Many animals, especially mammals, are active at night. Light is a strong force in nature and attracts many species. For example, songbirds and seabirds can be drawn to bright lights on land or from oil platforms at sea, flying around them until they are exhausted. Birds migrating at night can crash into tall, brightly lit buildings, and young birds on their first journey are especially at risk. Some birds, like blackbirds and nightingales, sing at strange times because of artificial light.
People used to think light pollution only bothered astronomers, who need a clear night sky for their work. While most of us don't need a perfect view of the night sky, we do need darkness. Darkness is important for our health, just like light. Changing our natural sleep and wake cycles can cause health problems. These cycles are a natural part of life on Earth, and disturbing them is like changing our balance. 
In the end, humans are affected by light pollution just like animals are. By creating so much artificial light, we have lost touch with our natural and cultural heritage, which includes the light of the stars and the natural rhythms of day and night. Light pollution makes us forget our true place in the universe, which we can best understand by looking at a dark night sky filled with stars."
92,C1,"The founder of a large international company recently said that his company will stop keeping track of how much paid holiday time employees take. This idea was inspired by an internet company that has a similar policy. The founder got the idea from an email from his daughter, which was shared in many newspapers. The email seemed like it was written by someone from his media team. 
Ignoring the way the announcement was made, which seemed like an attempt to make the policy look friendly, we should ask: is this idea practical? The internet company and the multinational corporation are very different. The internet company has 2,000 employees and offers one service, while the multinational has 50,000 employees and many different services like finance, transport, and healthcare. The idea of ""take as much time off as you want as long as it doesn’t hurt the business"" might work better in a smaller company where employees know each other’s work better. 
The founder of the multinational company said employees can take as much leave as they want if they are sure their work and team are up to date and their absence won’t harm the business or their careers. But can anyone be that sure? No matter how much you prepare before a holiday, there is always a lot of work waiting when you return. This is just how taking leave works; work piles up and you can’t control it. Someone following these rules might not take any leave or feel very guilty about it. This guilt can lead to stress, and if workers don’t take enough leave, productivity might decrease over time. 
There could also be pressure from colleagues and office gossip about who is off and for how long. This pressure already affects decisions like when to start and end the workday. In the corporate world, there is a culture of working late, and this could lead to a ""no holiday"" culture in a company with unlimited leave, where workers compete for promotions. If the feeling of safety that comes with guaranteed leave is removed, people might feel they can’t take the leave they need because they don’t want to seem lazy. They wouldn’t have their legal right to leave to rely on. 
The policy might make workers feel stuck, unable to take their leave, or they might just stick to their legal rights, making the policy pointless. Modern technology lets us get work messages anytime, anywhere, which has blurred the line between work and free time. The internet company started their unlimited leave policy because employees asked how this new way of working fit with the old time-off policy. If the company couldn’t track work hours accurately, why should it use an old standard for time off? 
However, if there are no set work hours, all hours could become work hours. Employees might not know if their work hours are being watched, making them monitor themselves, which could be harmful. Employment laws exist for a reason. Workers have a right to a minimum amount of paid leave because rest is important for their health. The benefits like increased morale, creativity, and productivity that are expected from the unlimited leave policy can happen without affecting worker well-being. I am doubtful that letting employees ""take as much holiday as they want"" is the real goal or likely result of this policy."
93,C1,"Journal-based peer review is a process where experts in a field review a scientific paper before it is published. This is seen as a way to ensure the quality of research, preventing flawed or nonsensical papers from being published. Scientists often mention this process to assure the public and media about the reliability of research. However, reviewing a paper can delay its publication by up to a year. Is this delay worth it to ensure trust in published research? The answer is both yes and no. 
Examining these issues shows that scientific publishing is changing. While I still believe in some form of review before publication, I notice changes happening. One major change is the use of preprints. Preprints are drafts of papers shared online before peer review. They allow researchers to quickly share new results for others to read, critique, and build upon. 
Publishing in journals has become more about gaining fame and advancing careers, which affects both authors and reviewers. Scientists compete for spots in top journals, which can lead to cutting corners, like leaving out inconvenient data. Reviewers now often decide if a paper is suitable for a journal based on its newsworthiness, not just its scientific quality. These issues are known, but few are willing to change the current system.
Biologist Ron Vale suggests that preprints might help solve these problems because they don't require a big change from the norm. Although preprints have been around for twenty years, they are not widely used. This is partly because scientists are conservative and because many believe journals won't accept papers already posted as preprints. There is also a fear that publishing without peer review could lead to 'junk science,' but this hasn't happened yet. 
Preprints are not peer-reviewed, but authors know they will be critiqued by a global community. Tanya Elks, a psychology professor, shared her experience with preprints. Her paper critiqued another paper, which is difficult in traditional journals. With preprints, the original authors could respond, and all comments were public, allowing readers to judge the arguments' quality. Even if a journal rejects the paper, the preprint and comments remain available, so the work isn't wasted.
Preprint archives allow for global scientific discussions that used to happen only between individuals. They can also share negative results, which are important but often ignored by journals focused on new discoveries. Preprints increase how often papers are read and cited, showing their effectiveness in spreading information. By using the internet's openness and the collaborative spirit in science, preprints can help focus on the research itself, not just where it is published."
94,C1,"When I ask my literature students what a poem is, they often say things like ""a painting in words."" These answers usually don't satisfy me or them. So, one day, I asked a group to pick an object and write one paragraph describing it like a scientist would, and another paragraph from the object's point of view, calling it ""Poem."" One student wrote: 
Poem: I may look strange or scary, but I'm a device that helps people breathe. I'm only used in emergencies and even then, only for a short time. Most people will never need me. 
The object? An oxygen mask. This unusual choice helped the class see how poetry works in a unique way. The exercise was fun and led to a good discussion. 
When I was in school, poetry was confusing to me. I thought every poem was a silly puzzle that blocked real understanding and feeling. After school, most people find poetry less interesting. Sometimes you see a poem, and it stands out because it's not like regular writing. It almost challenges you to read it, and often you feel let down because it's hard to understand or seems boring. Still, you feel good for trying. 
What do we want from poems? Aren't they supposed to be full of deep feelings, beautiful images, gentle thoughts, or sharp humor? The answer might seem to be yes. But if we want to cry, we watch movies; for information or strong opinions, we read online articles. Novels let us escape to other worlds, paintings please our eyes, and music – well, nothing beats the mix of lyrics, instruments, and melody. 
However, one thing a poem can offer is ambiguity. Everyday life is full of it, unlike reading sentence by sentence. But these ideas still don't explain what a poem really is. If you search online for ""poem,"" it takes you to ""poetry"": ""a form of literary art that uses the beauty and rhythm of language."" This is academic talk, but it hides the word's roots. ""Poem"" comes from the Greek word ""poí?ma,"" meaning ""a thing made,"" and a poet is ""a maker of things."" So, if a poem is a thing made, what kind of thing is it? 
Poets sometimes say poems are like wild animals – untameable and unpredictable – or like machines – carefully made and precise – depending on their view. But these comparisons don't hold up when you look closely. The best part of trying to define a poem through comparison is not the comparison itself but the discussion it creates. Whether you see a poem as a machine or a wild animal, this process can change how you think about machines or wild animals. It helps the mind try new ways of thinking and see ordinary things differently. 
Thinking of a poem as a mental object is not hard, especially if we think about how song lyrics can get stuck in our heads. The mix of words and melody is powerful, like schoolyard rhymes such as ""Sticks and stones may break my bones, but words can never hurt me."" But aren't words sometimes like sticks and stones? 
Think about a poem on a newspaper or magazine page, looking right at you: A poem can make a strong impact like nothing else, even though, as ink on paper, it does nothing more than the prose around it. What about all that empty space around the poem – space that could have been used for a longer article or ad? A poem is written and rewritten like an article, story, or novel, but it never really becomes what they usually are – something made to be sold. Publishers write press releases and send out review copies of poetry books, but few expect a book to pay for its printing costs. A poem shows itself not as something for the market, but as something for its own sake. Because of its special place in a magazine or book, a poem can still surprise us, even if only for a moment."
