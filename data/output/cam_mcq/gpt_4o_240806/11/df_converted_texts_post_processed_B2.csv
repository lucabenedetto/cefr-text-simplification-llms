text_id,text_level,text
1,C2,"Some time ago, a website talked about the dangers of public check-ins, which are online announcements of where you are. The website's message was clear: you might think you are just telling people, ""Hey, I'm here,"" but you are also letting everyone know you are not at home. This made people more aware that sharing too much online can be risky. The internet gives us many chances to share our lives with a global audience, offering possibilities like wealth and fame. So, we dive into the internet, sharing personal stories and photos. But soon, we realize that the online world can be dangerous and confusing.
However, there is hope. Years ago, a group of online pioneers explored these risks. In the early days of the internet, they faced challenges like losing jobs and friends and dealing with fame, even before social media existed. These pioneers, the first bloggers, have experienced what many of us are going through now. We should learn from their experiences to avoid repeating their mistakes.
In January 1994, Justin Hall, a 19-year-old student, started posting on the ""WWW,"" which was mostly used by students and scientists. The web was created at Cern, a physics lab in Switzerland, to help researchers share their work. Hall saw it as a chance to share his life. He created a detailed online autobiography with text, photos, and art. In 1996, he started a daily blog, and many people followed his bold use of this new medium. Hall shared everything, but one day, his site disappeared, replaced by a video called Dark Night. He had fallen in love, but when he wrote about it online, he was told, ""either the blog goes, or I do."" He realized that sharing too much online made people not trust him. He stopped blogging, but the problem of oversharing remains. Sharing online is fun, but if you expect it to make people like you, you might be disappointed.
In 2002, Heather Armstrong, a web worker in Los Angeles, had a blog called Dooce. She sometimes wrote about her job. One day, a colleague sent her blog to the company's vice presidents, including some she had criticized, and she lost her job. Experts call this the ""online distribution effect,"" where people feel they can say things online they wouldn't say in person. But the internet is not a separate reality where we can say anything without consequences. Our online and real lives are connected, and ignoring this can lead to big mistakes.
Armstrong's story ended well. Although she was upset and stopped blogging for a while, she got married and started blogging again about her family. Now, she is a successful ""mommy blogger,"" and her writing supports her family. She learned that while the internet lets us say anything, it doesn't mean we should."
2,C2,"Some time ago, scientists started a campaign to warn people about a low-budget film called ""What the Bleep Do We Know?"" This film used a mix of documentary and drama to show that there is a lot about our universe we don't understand. Most scientists agree that we don't know everything about the universe. However, some scientists felt the need to warn the public about the film, calling it ""atrocious"" and ""a very dangerous piece of work."" This made the film seem interesting to watch. At first, the film seemed harmless, with scientists saying that new discoveries show the universe is stranger than we thought. But then the film talked about discoveries like water molecules being affected by thoughts. It mentioned a Japanese researcher who claimed that the shape of a water molecule could change just by the thoughts of people around it. The film showed pictures of ice crystals looking nice after being talked to by someone happy, and looking bad after being exposed to someone in a bad mood. Some people find this evidence convincing, but many scientists say, ""Give me a break."" They believe that while the idea is amazing, there needs to be solid proof. Pretty pictures of crystals are not enough. The real issue is that the film's claims were not strange enough. For example, water molecules might have properties due to a form of energy that comes from nowhere and is linked to a force expanding the universe. This idea is supported by decades of research. Scientists have discovered that the universe is made of unknown matter and is driven by a mysterious force called ""dark energy."" Besides the film, other discoveries are also amazing. Neuroscientists found that our perception of events is delayed by about half a second, but our brains edit this out. Anthropologists think they have found where modern humans first lived and why they spread across the world. Some theorists suggest there are links between life on Earth and the universe's design. Science is far from complete. We are further from knowing everything than ever before. Concepts like chaos and quantum uncertainty limit what we can know. Some top physicists are trying to create a ""Theory of Everything"" to explain all forces and particles in the universe. Many people think the universe can be described in one word: incredible."
3,C2,"'In simple terms, I find writing novels challenging and writing short stories enjoyable. Writing novels is like planting a forest, while writing short stories is like planting a garden. These two activities complement each other, creating a complete landscape that I value. The trees provide shade, and the wind moves the leaves, which sometimes turn a bright gold. In the garden, flowers bloom, and their colorful petals attract bees and butterflies, showing the change of seasons. Since I started my career as a fiction writer, I have alternated between writing novels and short stories. My routine is this: after finishing a novel, I want to write short stories; after completing short stories, I focus on a novel. I never write short stories while working on a novel, and vice versa. These two types of writing might use different parts of the brain, and it takes time to switch from one to the other. After starting my career with two short novels in 1975, I began writing short stories from 1984 to 1985. I knew little about writing short stories then, so it was difficult, but the experience was memorable. I felt my fictional world expand. Readers seemed to like this other side of me as a writer. One of my early works, ‘Breaking Waves’, was in my first short-story collection, Tales from Abroad. This was my start as a short-story writer. One joy of writing short stories is that they don’t take long to finish. It usually takes me about a week to shape a short story (though revisions can be endless). It’s not like the long commitment needed for a novel, which can take a year or two. You go into a room, finish your work, and leave. That’s it. Writing a novel can feel like it goes on forever, and I sometimes wonder if I will finish. So, writing short stories is a necessary change of pace. Another nice thing about short stories is that you can create them from small things – an idea, a word, an image. It’s often like jazz improvisation, with the story leading me. Also, with short stories, you don’t have to worry about failing. If the idea doesn’t work, you just move on. Even great writers like F. Scott Fitzgerald and Raymond Carver didn’t always write masterpieces. I find this comforting. You can learn from your mistakes and use that in the next story. When I write novels, I try to learn from the successes and failures of my short stories. In this way, short stories are like an experimental lab for me as a novelist. It’s hard to experiment in a novel, so without short stories, writing novels would be more difficult. My short stories are like soft shadows I have set out in the world, faint footprints I have left behind. I remember where I wrote each one and how I felt. Short stories are like guideposts to my heart, and it makes me happy to share these feelings with my readers.'"
4,C2,"Science can be very abstract, like philosophy, or very practical, like curing diseases. It has made our lives easier but also posed threats. Science tries to understand everything from tiny ants to the vast universe, but it often struggles. It influences poets, politicians, philosophers, and even tricksters. Its beauty is clear to experts, but its dangers are often misunderstood. People sometimes overestimate or underestimate its importance, and its mistakes are either ignored or exaggerated. 
Science is full of conflicts. Old theories are often changed or replaced by new ones, similar to how new music styles are first mocked but later accepted. Scientists can be competitive and emotional. This book will explore how scientific ideas have changed not just science but also human thought. We will focus on ideas, not practical inventions like non-stick pans. We will admire these ideas but also question them, recognizing both human creativity and limitations. 
Science is always changing. Scientists often challenge each other's ideas. Usually, these changes don't affect society much, but sometimes they cause big shifts in our beliefs. For example, in the 17th century, science described the universe as a giant clock. Three centuries later, physics showed that our observations can affect the universe. Some people think this shows science can't fully explain the universe, but scientific changes usually help us understand and predict nature better. Isaac Newton explained more than Aristotle, and Albert Einstein explained more than Newton. Science makes mistakes but keeps progressing. 
At the end of the 19th century, many physicists thought there was nothing new to discover. Then came radioactivity, X-rays, electrons, quantum mechanics, and more. Biology has also made many discoveries. Some people now claim we are close to a complete theory of everything. Maybe. 
Science is not just a harmless hobby. In the last two centuries, we have started to control nature, sometimes upsetting its balance without understanding the consequences. Science needs to be monitored. Non-scientists should not ignore scientific advances because they affect the future world and future generations. Science is now part of how we plan and shape our future. This is not just a philosophical question; it affects national budgets, health, and even the future of life on Earth."
5,C2,"From around 2015, after years of growth, major publishers noticed that ebook sales had stopped increasing or even decreased. This raised new doubts about the future of ebooks in the publishing industry. One publishing executive admitted that the excitement about ebooks might have led to poor investments, with his company losing faith in the value of printed books. Despite the clear idea that digital and print books can exist together, people still wonder if ebooks will replace print books. Whether predicting or dismissing this idea, the thought of books disappearing continues to capture our imagination and spark debate. Why is this idea so strong? Why do we see ebooks and print books as being in conflict, even when evidence suggests otherwise? The answers go beyond ebooks and reveal our mixed feelings of excitement and fear about innovation and change. In my research, I have shown that the idea of one medium replacing another often comes with new technologies. Even before digital technology, people predicted the end of existing media. For example, when television was invented, many thought radio would die, but radio survived by finding new uses, like being listened to in cars. The idea of books disappearing isn’t new either. As early as 1894, people thought the phonograph would replace books with what we now call audiobooks. This pattern has repeated many times. Movies, radio, television, and smartphones have all been seen as threats to print books. Some thought the end of books would lead to cultural decline, while others exaggerated the benefits of ebooks. The idea of the death of the book often appears during times of technological change. This story reflects our hopes and fears about technology. To understand why these reactions are common, we must consider that we form emotional connections with media as they become part of our lives. Studies show we develop close ties with objects like books, TVs, and computers, even naming our cars or getting frustrated with our laptops. So, when new technology like e-readers appears, it doesn’t just mean economic and social change. It also means adjusting our bond with something important in our daily lives. As technology advances, we miss what we used to have. This is why industries develop around retro products and older technologies. For example, when the printing press spread in 15th-century Europe, people sought original manuscripts. The move from silent to sound movies in the 1920s made people nostalgic for silent films. The same happened with the shift from analog to digital photography and from vinyl records to CDs. Not surprisingly, e-readers have made people appreciate the physical quality of old books, even their smell. This should reassure those worried about print books disappearing. Yet, the idea of a disappearing medium will continue to be a popular story about technology’s power and our resistance to change. We use familiar stories, like tragedy and endings, to understand change. The story of the death of the book reflects our excitement for the future and our fear of losing parts of our world and ourselves."
6,C2,"'Almost every weekday morning for a year and a half, I woke up at 5:30, brushed my teeth, made coffee, and sat down to write about how some of the greatest minds of the past four hundred years managed their daily work. I wanted to see how they organized their schedules to be creative and productive. By writing about the simple details of their daily lives – when they slept, ate, worked, and worried – I hoped to show a new side of their personalities and careers. I wanted to create interesting portraits of these artists as creatures of habit, just like us. The French gastronome Jean Anthelme Brillat-Savarin once said, ‘Tell me what you eat, and I shall tell you what you are.’ I say, ‘Tell me what time you eat, and if you take a nap afterward.’ In this way, this book is about the circumstances of creative activity, not the product itself. It’s about how things are made, not what they mean. But it’s also personal. The novelist John Cheever believed that even writing a business letter reveals something about your inner self. My main concerns in the book are issues I face in my own life: How do you do meaningful creative work while also earning a living? Is it better to focus completely on a project or to work on it a little each day? And when there isn’t enough time for everything, do you have to give things up (like sleep, money, or a clean house), or can you learn to do more in less time, to ‘work smarter, not harder’, as my dad always says? I don’t claim to answer these questions in the book – some of them might not have answers, or can only be solved individually. But I have tried to show examples of how different successful people have faced these challenges. I wanted to show how big creative ideas are built from small daily actions; how working habits affect the work itself, and vice versa. The book’s title is Daily Rituals, but I focused on people’s routines. A routine can seem ordinary and automatic, but it can also be a well-planned way to use limited resources: time, willpower, and optimism. A good routine helps keep your mind focused and prevents you from being controlled by your moods. The psychologist William James thought you should put part of your life on autopilot; by forming good habits, he said, we can ‘free our minds to advance to really interesting fields of action.’ Ironically, James himself often procrastinated and couldn’t stick to a regular schedule. Interestingly, it was a moment of procrastination that led to this book. One Sunday afternoon, I was alone in the office of the small architecture magazine where I worked, trying to write a story due the next day. Instead of working, I was tidying my desk, making coffee, and wasting time. I’m a ‘morning person’, able to focus well in the early hours but not so much after lunch. That afternoon, to feel better about this habit, I started looking online for information about other writers’ work schedules. These were easy to find and very interesting. I thought someone should collect these stories in one place – so I started the Daily Routines blog that afternoon, and now, this book. The blog was casual; I just posted descriptions of people’s routines as I found them in biographies, magazine profiles, newspaper obituaries, and similar sources. For the book, I’ve gathered a much larger and better-researched collection, while keeping the short and varied voices that made the blog appealing. As much as possible, I’ve let my subjects speak for themselves, using quotes from letters, diaries, and interviews. In other cases, I have summarized their routines from other sources. I should mention that this book wouldn’t have been possible without the research of many biographers, journalists, and scholars whose work I used. I have listed all my sources in the Notes section, which I hope will also guide further reading.'"
7,C1,"Howard became a palaeontologist because of a change in interest rates when he was six years old. His father, who was careful with money and had a big mortgage, decided they couldn't afford a holiday to Spain. Instead, they rented a chalet on the English coast. There, on a rainy August day, Howard found an ammonite* on the beach. From that moment, he knew he wanted to be a palaeontologist. By the end of his university studies, he knew what kind of palaeontologist he wanted to be. He was interested in the very early history of life, not in dinosaurs or the Jurassic period. He studied ancient, delicate creatures found in grey rocks. 
When he finished his doctoral thesis, he worried about finding a job, especially the kind of job he wanted. He was confident in his abilities but knew that deserving something doesn't always mean you get it. When a job at Tavistock College in London was advertised, he applied immediately, though he wasn't very hopeful. On the day of his interview, the professor who was supposed to lead the panel had an argument with his wife, crashed his car, and ended up in the hospital. The interview went on without him, and the professor who replaced him disliked the original professor's favorite candidate. This led to Howard getting the job. Howard was surprised by the support from someone he didn't know, but later learned the real reason he was chosen. He was a little disappointed but mostly happy to have the job he wanted.
Howard often thought about how his professional life was organized and planned, unlike the chaos of personal life. He realized how strangers could change your life, like when his briefcase with lecture notes was stolen at an Underground station. Angry, he returned to the college, postponed his lecture, and reported the theft. Then he had coffee with a colleague and a visiting curator from the Natural History Museum in Nairobi. This meeting led him to learn about a new collection of fossils that would be his biggest challenge and secure his career. If not for the theft, he wouldn't have known about this opportunity. He quickly changed his plans, deciding not to go to a conference in Stockholm or take students on a field trip to Scotland. Instead, he focused on finding the money to visit the museum in Nairobi."
8,C1,"Charles Spence is willing to try almost any food. ""We have ice cream made from bee larvae at home,"" says the Professor of Experimental Psychology at Oxford University in the UK. Although they look like maggots, they have a ""slightly nutty, floral"" taste. Making bug-eating acceptable is just one of the challenges Spence and his team are working on. Through his research on how our senses combine to create our perception of flavor, Spence is quietly influencing what we eat and drink. He works with both large food companies and top restaurants. Spence and his colleagues study how we experience food and drink, a field informally known as gastrophysics. They look at details like who we eat with, how food is presented, the color and texture of plates, and background noise—all of which affect taste. Spence's book, ""The Perfect Meal,"" co-written with Betina Piqueras-Fiszman, offers many interesting insights. Did you know that the first person to order in a restaurant usually enjoys their meal the most? Or that we eat about 35% more when dining with one other person, and 75% more with three others?
Spence's lab in Oxford is simple and low-tech. It has soundproof booths and old audio-visual equipment. By keeping costs low, he can work creatively with chefs who can't afford academic research. Much of his work is funded by a major food company. In the past, research funded by industry was seen as less serious in universities. But now, since the government wants universities to show their work has an impact, this type of research is more valued. Spence is helping well-known brands reduce salt and sugar, often because of government rules. Companies do this gradually so customers don't notice the change. ""Research shows that if you tell people about the changes, they focus on the taste and don't like it as much,"" he says.
Spence first met Heston Blumenthal, a famous experimental chef, while working on a project for a big food company. People used to think science and food didn't mix, but most food is scientific. Blumenthal and Spence worked together on the ""Sound of the Sea"" dish at Blumenthal's restaurant. Interestingly, Spence notes that the Italian futurists were experimenting with sound and food a century ago, but it didn't become popular. Now, the food industry is using Spence's sensory science widely. For example, his research shows that high-pitched music makes food taste sweeter, while low-pitched sounds make it taste bitter. An airline will soon match music with the food served to passengers. Last year, a brand released a smartphone app that played music while your ice cream softened, but they didn't match the music to the taste, which Spence says happens often.
At home, Spence's dinner parties are unique. Once, they ate rabbit with the fur wrapped around the cutlery. Another time, they used remote-controlled, multi-colored light bulbs. They've also had parties with a tone generator, headphones, and different drinks to see if they have different pitches. For Spence, home, shops, food conventions, and international gastronomy conferences are all extensions of his lab."
9,C1,"Our brains are busier than ever. We are bombarded with facts, fake facts, chatter, and rumors, all pretending to be information. We have to sort through this to find what we need and what we can ignore. At the same time, we are doing more tasks ourselves. Thirty years ago, travel agents booked our flights, and salespeople helped us in stores. Now, we do most things on our own. We are doing the work of many people while trying to manage our lives, families, careers, hobbies, and TV shows. Smartphones help us fit as much as possible into every spare moment. But there is a problem. We think we are multitasking, doing several things at once, but this is a dangerous illusion. Earl Miller, a neuroscientist at MIT, says our brains are not designed to multitask well. When we think we are multitasking, we are actually switching quickly from one task to another, which has a mental cost. We are not expert jugglers; we are more like amateur plate spinners, quickly switching tasks and worrying about dropping one. Multitasking makes us less efficient. It increases stress hormones like cortisol and adrenaline, which can cause mental fog. Multitasking also creates a craving for new stimulation. The prefrontal cortex, a part of the brain, is easily distracted by new things. This is a problem because we need this part of the brain to focus. Just having the chance to multitask can hurt our thinking. Glenn Wilson, a former professor of psychology, calls it info-mania. His research found that trying to focus on a task while an email is unread in your inbox can lower your IQ by almost 10 points. This loss is greater than the loss from being tired. Russ Poldrack, a neuroscientist at Stanford, found that learning while multitasking sends information to the wrong part of the brain. If students do homework and watch TV at the same time, the information goes to the striatum, which stores new skills, not facts. Without TV, the information goes to the hippocampus, where it is organized and easier to remember. Multitasking also involves decision-making, which is hard on our brains. Small decisions use the same brain resources as big ones, leading to poor decisions. Business leaders, scientists, and writers often mention email as a problem. It is not the email itself, but the overwhelming amount of communication. A colleague's 10-year-old son said his father ""answers emails"" for a living, which is almost true. We feel we must reply to emails, but it seems impossible to do so and get other things done."
10,C1,"In a Swedish zoo, a chimpanzee named Santino spent his nights breaking concrete into pieces to throw at visitors during the day. Was he being mean? In caves in the US, female bats help other fruit bat mothers if they can’t find the right position to give birth. Are they being kind? Fifty years ago, these questions would not have been important. Animals had behaviors, and science recorded the results of these behaviors. The idea that animals have feelings and moral systems was seen as sentimental. But recently, this has started to change. Research into the behavior of bats, chimps, rats, dolphins, and chickens has begun to explore animal emotions. This change has reached popular science books, like Mark Bekoff’s ""Wild Justice"" and Victoria Braithwaite’s ""Do Fish Feel Pain?"". This has started a debate: can animals be said to have consciousness? This leads to another question: do animals have a conscience, a sense of right and wrong? In a recent experiment with cows, those that opened a locked gate to get food showed more pleasure than those that had the gate opened for them. If cows enjoy problem-solving, what does this mean for how we produce and consume beef? The observations are clear, but their interpretation is debated. Dr. Jonathan Balcombe, author of ""Second Nature"", believes the logical response to this research is to stop eating meat. He thinks humanity is on the verge of a major ethical change. Aubrey Manning, Professor Emeritus at Edinburgh University, believes we should rethink our view of animal thinking. He says, ‘the only tenable hypothesis is that animals do have a theory of mind, but it’s simpler than ours.’ Professor Euan MacPhail thinks we should stop giving animals human traits. The issue is not just scientific or moral, but philosophical. Since defining consciousness is difficult, can we ever know what it is like to be a bat? Balcombe describes an experiment that suggests starlings, a type of bird, can get depressed. At Newcastle University, starlings were split into two groups. Half were in large, comfortable cages, and the other half in small, empty cages. Both groups learned to eat tasty worms from one box and avoid unpleasant worms from another. Later, when only unpleasant worms were offered, only the birds in comfortable cages ate them. Balcombe concluded that being in a bad cage made the starlings pessimistic. Balcombe, who has worked with animal rights groups, has a clear bias. He says, ‘We look back with horror on times of racism. Our view about animals will someday be the same. We can’t support animal rights while eating a cheeseburger.’ If he were the only one with this view, it might be easy to dismiss him. But Professor Aubrey Manning shares his view. Manning has written a textbook, ""An Introduction to Animal Behaviour"". He says, ‘What we are seeing is a pendulum swing. At the start of the 20th century, people thought animals were like us, and there was a reaction against that. Now we are going the other way. But it is a highly debated subject, and you want to avoid the sound of academics with personal grievances and strong opinions.’"
11,C1,"Critical thinking is a way of understanding what we read or hear. Adrian West, from the Edward de Bono Foundation U.K., says that argument can help find the truth. Technology helps us store and process information, but it might also change how we solve problems and think deeply. West warns that we face a lot of poor thinking and opinions, which can overwhelm our ability to reason. More data doesn't always mean better knowledge or decisions. 
The National Endowment for the Arts reports that reading has dropped by 10%, and this decline is speeding up. Patricia Greenfield, a psychology professor, believes that focusing more on visual media can reduce critical thinking. She says that less reading might lead to less critical thinking because people focus more on real-time media and multitasking. However, we still don't have a clear answer on how technology affects critical thinking. Technology's impact is complex, and it can be both good and bad. For example, a computer game might help or hurt critical thinking. Reading online can improve analysis, but too many links can stop deeper thought.
Greenfield, who studied over 50 reports on learning and technology, says that technology changes how we think. Reading helps imagination and critical thinking, while visual media can improve some information processing. But most visual media don't allow time for reflection or imagination. This can lead to people, especially young ones, not reaching their full potential.
How we view technology affects our thoughts on critical thinking. Video games and cognition show this conflict. James Paul Gee, an educational psychology professor, says video games can be good for learning. Games like Sim City and Civilization teach decision-making and analytical skills. They let players explore ideas that might be hard to access otherwise. In the digital age, as reading and math scores drop, we need to understand how technology affects thinking and analysis."
12,C1,"Matthew Crawford is a writer and motorcycle mechanic. He left office work because he was unhappy with it. His first book talked about the benefits of manual work. His latest book is about dealing with modern life. He got the idea for this book while shopping. He noticed ads on the credit card machine while entering his pin. Crawford realized that these ads are hard to avoid and take our attention. He believes that what we focus on is personal, but we are losing control over it. It is becoming harder to think or remember past conversations. To avoid constant interruptions, people are closing themselves off and avoiding simple interactions like talking to strangers. Crawford says we experience the world through things like video games and phone apps, which can be manipulative. These things reflect our desires and can take over our attention. Many people are worried about this issue. For example, office workers complain about emails but spend their free time on them. Studies show that just seeing a phone on the table can distract us. There is no scientific proof yet that our attention spans are shorter, but we are more aware of other things we could be doing. Crawford thinks technology has made it easy for us to focus on ourselves. This constant choice affects our self-control and social life. We prefer texting to talking because it feels safer. By only interacting with representations of people, we might lose something important in society. Screens are part of the problem. Crawford gives an example from his gym. There used to be one music player for everyone, which could cause tension. Now, people listen to their own music with ear buds, making the gym less social. Crawford suggests two solutions. First, we need rules to reduce noise and distraction in public spaces. More importantly, he suggests engaging in skilled activities like cooking, playing ice hockey, or racing motorcycles. These activities require dealing with real things and using good judgment. They help us connect with the world in a satisfying way. This doesn't mean everyone should become a chef, but it's important to use your judgment. There are many benefits, including professional satisfaction. Constantly fighting distractions is tiring and makes it hard to focus. Paying attention to one thing helps you focus better on other things."
13,C1,"""‘What do you do for a living?’ is a common question. We often define ourselves by our jobs. Usually, this question is easy to answer. However, when you are a philosopher, like me, it can be a bit tricky. Calling yourself a philosopher might sound pretentious. Saying you study or teach philosophy is fine, but saying you are a philosopher can seem like you claim to know some special truth. This is not true; philosophers are just like everyone else. But this idea makes me pause. Why does this stereotype exist? One reason is that philosophers are seen as people who judge others and value intellectual life. The Greek philosopher Aristotle (384 – 322 BCE) said that a life of thinking, or a philosophical life, was the best life. Few modern philosophers agree, but philosophy is still linked with thinking deeply. Another Greek philosopher, Socrates, said, ‘the unexamined life is not worth living’. He meant that just accepting what society says is not satisfying. Our ability to think about the world helps us control our lives and make decisions. But living an examined life doesn’t mean reading lots of philosophy books or spending all your time thinking. It means looking closely at everyday experiences to see if they are important. It doesn’t mean living like a wise person away from society. In fact, to help people, the examined life should be practical and shared with others. This leads to another reason for the misunderstanding about philosophers: academic philosophy has become distant from real life, especially for people without formal training. This isn’t entirely philosophers’ fault: university funding and evaluations are linked to expensive and hard-to-access journals. So, philosophers often only talk to other experts in their field. For most people, philosophy can seem far from reality. If philosophers once tried to change this view, many have stopped. The university system created this isolated environment, and academics have supported it. As some areas of philosophy focus on specific, technical debates, explaining them to non-experts is very hard. Philosophy, in some cases, has moved away from society. This needs to change. I sometimes call myself an ‘ethicist’ because I work in ethics, the part of philosophy that looks at human actions. But recently, I feel this title doesn’t fully describe my work, because ethics is often linked to rules and laws. This is a new development in philosophy: ethics now often means applied ethics, which looks at the fairness of social practices. The job of an ethicist today is to decide if an action is ‘ethical’ or acceptable. These are important questions, and I often think about them. However, philosophy is more than this. A typical discussion might start by asking if illegally downloading films is wrong (it is) and then explore ideas about responsibility, art, and consumerism. Philosophy helps people look more closely at their actions and beliefs. Sometimes we find out what we already know; other times, we see our beliefs are hard to justify. By examining these ideas, we help everyone."""
14,C1,"'People who love food, like foodies, chefs, and gourmands, might think that thinking deeply about what and how we eat can make eating more enjoyable. However, in history, philosophers have often talked about food to discuss other topics, like learning. Sometimes, discussions that seem to be about food are actually about other, loosely related ideas. For example, the ancient Greek philosopher Epicurus talked about seeking pleasure and avoiding pain in many areas of life. Yet, his name is often linked with a love for eating and drinking. We see his name used in restaurants and food shops, likely because businesses think it will attract customers. Food is social and cultural. It comes from history and is shared with others in our communities. These communities give us people to eat with and the systems to grow and share food. We interact with food more than any other product, making it an interesting topic for philosophy. Once food is made, critics talk and write about it. But why do they have such an important role? One philosopher says that tasting food is not a special skill, but that food critics are just better at describing tastes. This area has not been studied much because most research on perception focuses on sight. Another part of food is its beauty. We say paintings or music are beautiful, but not usually food. Some philosophers think that with modern cooking, food should be part of discussions about beauty, like art or poetry. But food is eaten and disappears, unlike art, which lasts over time. So, some think food cannot be considered in the same way as art. There are many ethical questions about food. We can ask what we should eat: organic, free-range, local, vegetarian, or non-GMO foods? Our choices often show our ethical beliefs, and philosophers explore why we make these choices. Cooking at home and in restaurants is different. Home cooks have a duty to their guests because of personal relationships. At home, everyone shares food and friendship. Professional cooks have responsibilities to their employers and the food, so their kitchens are not open to everyone, only to qualified people. Their relationships are professional, not personal. A recent essay called ‘Diplomacy of the Dish’ looks at how food can help bridge cultural gaps. This happens in two ways. First, we can appreciate other cultures by enjoying their food. Second, we build personal connections with people from other cultures by eating together, a practice with a long history in diplomacy. The essay includes many examples and stories that make this part of food philosophy interesting for readers.'"
15,C1,"Rob Daviau, from the US, creates 'legacy' board games. He felt that the board games he played as a child were not fun or challenging anymore. He wondered if board games could have a storyline or if decisions made in one game could affect the next. He changed a classic board game called Risk to make a new version: Risk Legacy. In this game, decisions made during play have a lasting effect. Players might have to tear up cards, write on the board, or open packets with new rules. The game is played over a set number of sessions, and past actions become part of the game. Daviau said: ‘You could point to the board and say: “Right here! You did this!”’
Daviau was then asked to help create a legacy version of Pandemic, a popular game where players work together to cure diseases. His next project was a game called SeaFall. While Pandemic Legacy was very popular, SeaFall was seen as a real test of the legacy format because it was not based on an earlier game. Set in the age of sail (16th – mid 19th century), players explore and conquer new lands. Legacy game designers must think about all possible player choices to keep the story together. They use testers to see how the game will play out. Jaime Barriga, a tester for SeaFall, said: ‘It takes a lot of time. Sometimes the first few games are great, but then it starts to fall apart, and you have to fix everything.’
Legacy board games were not expected to become popular. Even Daviau thought it would be a small interest. ‘When I was working on it, I thought: “This is different and good,”’ he said. ‘But it’s strange and breaks many rules. I thought only a few people would like it.’ However, many players, like Russell Chapman, loved the idea. He thinks it is a big step forward in game design. ‘It’s a new level of commitment, intensity, excitement,’ he said. ‘There’s nothing more exciting for a board gamer than learning a new game or way to play.’
Another fan, Ben Hogg, said the adventure was more important than worries about the game’s lifespan. ‘At first, I was worried about changing and ruining the board,’ he said. ‘But Pandemic Legacy changed that. Most people don’t watch the same movie twice, do they? You’re buying an experience. It’s like the stories in video games.’ The legacy format is influenced by video games and popular TV series. Daviau once wanted to be a TV writer but moved to advertising and then game design. The love of storytelling stayed with him. Pandemic creator Matt Leacock compares designing a legacy game to writing a novel. ‘You need to know how it starts and ends,’ he said. ‘But you also need more than just an outline.’
While Daviau feels proud of his work, he is interested in seeing what others do with the idea. But he also thinks people will soon want something new. Colby Dauch, from the publisher of SeaFall, believes the legacy format has changed how people think about board games. ‘It’s the kind of idea that changes what a board game can be.’"
16,C1,"'One night, an octopus named Inky escaped from his tank at New Zealand’s National Aquarium. He moved across the floor and squeezed into a drain leading to the Pacific Ocean. This story was shared widely online and seemed like something from a children's movie. Stories like this, about animals escaping, are fun because they make us think animals are like us. This is especially true for octopuses, which are very smart but look so different from us. How can these sea creatures open jars, recognize faces, use coconut shells as armor, and even play? 
People often think it's unscientific to say animals are like humans, but Dr. Frans de Waal, who studies primates like gorillas and chimpanzees, disagrees. He says the real problem is not recognizing the human-like traits in animals, which he calls anthropodenial. By looking at animal-cognition research, he shows that animals can do many things we thought only humans could do, like thinking about the past and future, showing empathy, and understanding others' motives. Animals are smarter than we thought.
In the past, people believed animals had complex minds. In medieval Europe, animals could even be put on trial for crimes. In the nineteenth century, naturalists looked for connections between human and animal intelligence. Charles Darwin, who developed the theory of evolution, said the difference between humans and animals is one of degree, not kind.
In the twentieth century, behaviorism changed how people viewed animal intelligence. Animals were seen as machines that responded to rewards and punishments. People who thought animals had inner lives were considered unscientific. This view came at a time when humans were destroying animal habitats and ignoring animal welfare.
Dr. de Waal believes we are now starting to see animal intelligence as similar to human intelligence, though not the same. He says the best tests of animal intelligence consider the unique traits of each species. For example, squirrels might not do well on human memory tests, but they can remember where they hid nuts. In her book, The Soul of an Octopus, naturalist Sy Montgomery suggests that if octopuses tested human intelligence, they might think we are not very smart because we can't change our skin color like they can.
Dr. de Waal is not sure if Inky really found his way to the ocean, but he knows that viral stories can help people appreciate animal intelligence. He once did an experiment to see if capuchin monkeys feel envy. When some monkeys got cucumbers and others got grapes, the ones with cucumbers got upset. The study was published in a scientific journal, but a video clip of the experiment convinced more people. This shows how our minds work in interesting ways.'"
17,C1,"'Robotics, once only seen in science fiction, is now becoming a major technological change, similar to the impact of industrialisation. Robots have been used in car and manufacturing industries for many years, but experts now say that robots will soon be used in many more areas. However, many countries are not ready for this big change. Most people agree that robots will take over many jobs in the next 50 years, but they still believe their own jobs will remain the same. This is not true: robotic automation will affect every industry soon. For example, an Australian company, Fastbrick Robotics, has created a robot called Hadrian X that can lay 1,000 bricks in one hour, a job that would take two human workers almost a day. In San Francisco, a startup called Simbe Robotics has made Tally, a robot that moves through supermarket aisles to check that products are stocked and priced correctly. Supporters of robotic automation say that robots cannot yet service or program themselves, which means new jobs will be created for technicians and programmers. Critics warn that we should not ignore the importance of human skills at work. Dr. Jing Bing Zhang, an expert in robotics, studies how robots are changing the workforce. His research shows that soon, many robots will be smarter and able to work with humans. In a few years, many top companies will have a chief robotics officer, and some governments will have laws about robots. Salaries in the robotics field will rise, but many jobs will remain unfilled due to a lack of skilled workers. Zhang says that people with lower skills will be affected by automation and should retrain to adapt. New developments in technology will lead to new types of robots for consumers, like robots that live in our homes and interact with us. This change offers great opportunities for companies but also requires new rules to protect our safety and privacy. With many jobs at risk, education is key to preparing for the future workforce. Developed countries need more graduates in science, technology, engineering, and maths (STEM) to stay competitive.'"
18,C1,"George Mallory, a famous mountaineer, is known for his simple answer to why he wanted to climb Mount Everest: ""Because it’s there."" This response reflects a common curiosity about why people engage in such dangerous activities. Mallory's answer might inspire some to follow their dreams, challenging them to face the unknown. The real value of his answer is its simplicity, suggesting that climbing might be just for adventure and fun.
In 1967, Bolivian writer Tejada-Flores wrote an important essay called ‘Games that Climbers Play’. He described seven different types of climbing activities, each with its own rules. He believed that the style of climbing, like using more or less equipment, should match the rules of the specific climbing activity. Over the years, this idea has become very popular in Western climbing discussions, from magazines to debates.
Many climbers love the feeling of freedom that climbing gives them. However, climbing can also be very limiting, like being stuck in a tent during a storm. This creates a paradox: climbing can feel both freeing and restricting. But some argue that the simplicity of climbing, with limited choices, is a form of freedom.
US rock climber Joe Fitschen offers a different perspective. He suggests asking ""Why do people climb?"" to explore why humans are drawn to climbing. Fitschen believes climbing is in our genes, a natural part of us that pushes us to take on challenges and test limits, even with risks.
US academic Brian Treanor adds that climbing can help develop important virtues like courage, humility, and respect for nature. He believes these virtues are important in our modern, risk-averse world. Climbing, therefore, has practical value in helping people develop traits that are useful in everyday life.
Another idea is that climbers who climb without help or technology must be fully committed to succeed. Experts Ebert and Robinson argue that independent climbs deserve more recognition because they are harder and riskier. However, this view has caused some controversy, as it might encourage climbers to take unnecessary risks.
Finally, climbing can be seen from a non-Western perspective. Many climbers talk about being ""in the moment"" or ""in the zone"" during a climb. The physical effort, active meditation, and intuitive problem-solving are key parts of climbing. Some say these aspects are similar to Zen philosophy, which aims for a state of perfect peace. This offers another reason why people are drawn to climbing."
79,C2,"'In recent years, a key change in higher education in the UK and around the world has been to encourage new students to get involved with research early in their studies. This change shows that research is not just for famous scholars at old universities or scientists making new discoveries. Instead, research is a basic way to build knowledge and skills. Research skills are useful not only in your studies but also in jobs, because they help you think about the world and do your work. As a student, you add to knowledge. You do not just learn and repeat it; you create it. Creating knowledge means asking questions like Why? How? When? What does this mean? What if this were different? These questions are part of what we call research. Research can be seen as a range of activities. On one end, there is complex, groundbreaking research done by highly trained people, leading to big changes. On the other end, research can be everyday questions and careful work that asks thoughtful questions about issues and makes practical suggestions. Most students have always been researchers in some way. You have done research for school projects or at work. You have asked questions that led to research since you first became interested in studying. You have also used research skills when planning a holiday, growing plants, fixing things, training a pet, or shopping online. In college and higher education, having an enquiring mind, finding problems and questions, and thinking critically about information are expected. Some students may find this hard because, in some cultures, knowledge is seen as already established, and you learn by listening to teachers and books. It might seem disrespectful to question known knowledge and authorities. But in the UK, US, much of Europe, and Australia, questioning established knowledge and authorities is important. This might seem challenging. Critical thinking is very important in research. The research of others is useful to students, academics, and in jobs, but we need to do more than just repeat what we read. We need to think about it, test it, and see if it is logical and supported by evidence. We should not just accept facts and information without questioning them.'"
80,C2,"Cities have always been places of intellectual activity. In the 18th century, people in London met in coffee houses to talk about science and politics. In modern Paris, artist Pablo Picasso discussed modern art in cafés. However, living in a city is not always easy. The same London coffee houses that encouraged discussion also spread diseases like cholera. Picasso eventually moved to the countryside. While cities are creative places, they can also be overwhelming and unnatural. Scientists are now studying how city life affects our brains, and the findings are concerning. Although we know city life is tiring, new research shows that it can also make our thinking less sharp. One reason is the lack of nature, which is surprisingly good for the brain. Studies show that hospital patients recover faster when they can see trees from their windows. Even brief views of nature can improve brain function because they give us a break from city life. This research is important because, for the first time in history, most people live in cities. Instead of open spaces, we live in crowded areas with many strangers. These unnatural surroundings affect our mental and physical health and change how we think. 
When you walk down a busy street, your brain has to manage many things: distracted people, dangerous crossings, and the complex city layout. These tasks tire us out because they use a lot of brain energy. A city is full of stimuli, so we must constantly focus our attention. This controlled perception requires effort. The brain is like a powerful computer, but paying attention uses a lot of its power. In nature, we don't need as much mental effort. This idea is called attention restoration theory, developed by psychologist Stephen Kaplan. He suggested that being in nature can restore our attention. Natural settings capture our attention without causing negative emotions, unlike a police siren. This allows our brain to relax and recharge. 
Long before scientists studied this, philosophers and landscape architects warned about the effects of city life and tried to include nature in cities. Urban parks, like Central Park in New York, offer an escape from city life. A well-designed park can improve brain function quickly. While people look for ways to boost brain performance, like energy drinks or office redesigns, few are as effective as a walk in nature. So, with all the mental challenges of city life, why do cities keep growing? And why do they remain centers of intellectual life even in the digital age? Research from the Santa Fe Institute shows that the same city features that cause attention and memory problems, like crowded streets, also lead to innovation. The ""concentration of social interactions"" is a big reason for urban creativity. Just as crowded 18th-century London led to new ideas, crowded 21st-century Cambridge, Massachusetts, is a creative tech hub. Less crowded cities might produce less innovation over time. The key is to reduce the mental stress of city life while keeping its benefits. As the song goes, sometimes people say, ""I'm sick of the trees, take me to the city!"""
81,C2,"'Where should we look for the mind? This might seem like a strange question: we usually think that thinking happens inside our heads. Today, we have advanced brain-scanning techniques to show this. However, I believe that the study of the mind should not stop at the skin or the skull. There is a lot of evidence, from ancient times to now, showing that objects, as well as neurons, are part of human thinking. From an archaeological perspective, it is clear that stone tools, jewelry, engravings, clay tokens, and writing systems have played a role in human evolution and the development of the human mind. So, I suggest that what is outside the head might also be part of the mind. It is easy to see why people think the mind and the brain are the same. Most of what we know about the mind comes from studying people without the objects they usually have around them. This makes sense for neuroscientists because of the limits of brain-scanning machines. But this often hides the fact that much of our thinking happens outside our heads. I do not want to question the neural basis of thinking, but to show that the mind is more than just the brain. It would be better to explore the idea that human intelligence extends beyond the skin into culture and the material world. This is where a new theory I’ve developed – Material Engagement Theory (MET) – comes in. MET explores how objects become part of our thinking, like when we make numbers and symbols out of clay or use a stone to make a tool. It also looks at how these ways have changed since ancient times and what these changes mean for how we think. This approach gives new insight into what minds are by changing what we know about what objects do for the mind. Think of a blind person with a stick. Where does this person’s self begin? The unity of the blind person and the stick helps us see minds and objects as connected. It also shows the flexibility of the human mind: using a stick, the blind person turns touch into sight, and the stick plays an active role. The brain treats the stick as part of the body. The blind person’s stick reminds us that human intelligence can change a lot by using new technology. My approach sees the human mind as always evolving. It is important to remember that, whatever form the ‘stick’ has taken in our history – from early stone tools to the internet – its main function was as a pathway, not a boundary. Through the ‘stick’, humans feel, discover, and understand the environment, and also move forward. That is why a stick used by a monkey to get food is different. For humans, ‘sticks’ are used to satisfy our curiosity. This unique human tendency to engage with material culture explains why we, more than any other species, make things, and how those things, in return, shape our minds. I call this metaplasticity – our minds are flexible and change as they interact with the material world. I want to include materiality in the study of the mind. MET offers a new way to understand how different forms of material culture, from stone tools to smartphones, have helped define and transform what we are and how we think. Mind-changing technology sounds futuristic, but humans have used it since they first evolved.'"
82,C1,"Photography is one of the few inventions that have greatly changed how we see the world. This is especially true in the United States, where photography quickly became a part of American culture at all levels. It was used for practical and scientific purposes, in industry, and for art. It also opened up new ways for amusement and entertainment with the arrival of the camera. Historians say that photography might be the greatest contribution of the US to the visual arts. No other art form has been as influenced by US work. This is a big claim, but it shows how photography has become central to US intellectual and artistic discussions. To fully understand its impact, we need to look at its beginnings in the mid-19th century. 
Why was photography so attractive? First, because it was a mechanical process, it matched the growing interest in technology in the US, where change was seen as a normal part of life. Just like steam power, railroads, and electricity made the world smaller by improving communication and travel, photography brought the wonders of the world into people's homes with amazing speed. Second, the camera was the best tool for showing one's self-image in a country where personal and national identities were always being created and recreated. Third, the camera was important for creating and keeping a family record, even if it was a bit idealized. Finally, the realistic nature of photographs matched the US artists' tendency towards realism and showing everyday life. 
A photograph draws attention to something, showing what the photographer wants us to see. Every picture says, ""This, you should see!"" Because a photograph is made by a machine, it has a unique quality: it is a record of events, people, or things. It shows what was in front of the camera, giving it a sense of objectivity. However, since it is taken by a person, it also has a personal point of view, or subjectivity. We might think we understand a photograph because we recognize the subject, which is why photography has been called a ""universal language."" But the meaning of a photograph is more complex. No image is shown to us without some context that affects how we understand it, like a caption in a newspaper or its placement in a gallery. To understand a photograph historically, we need to consider its purpose and role in its cultural setting: Why did the photographer take it? How did people first see it? The same image might be seen in different places and times, and its meaning can change each time.
For a long time, the camera's importance to artists was not widely known, but now it is very clear. In recent decades, starting with US artist Andy Warhol, who used photographs in his paintings, silk screens, and other works, artists have been using photographs in many ways. In short, photography has become an essential part of art, not against painting, but as an important addition to it, shaping our ideas of representation before the camera was invented."
83,C1,"In 1890, William James, an American philosopher and one of the founders of modern psychology, defined psychology as the ""science of mental life."" This definition is still useful today. We all have a mental life, so we have some idea of what this means. However, even though we can study mental life in animals like rats and monkeys, it is still a complex idea. James was mainly interested in human psychology, which he believed included thoughts and feelings, the physical world, and how we know about these things. This knowledge is personal and comes from our own thoughts, feelings, and experiences. It may or may not be influenced by scientific facts. Because of this, we often make judgments about psychological matters based on our own experiences. We act like amateur psychologists when we give opinions on complex topics, like whether brainwashing works, or why people behave in certain ways, such as feeling unhappy or quitting their jobs. Problems occur when people understand things differently. Formal psychology tries to find methods to decide which explanations are most likely correct in a situation. Psychologists help us tell the difference between subjective thoughts, which can be biased, and scientific facts. Psychology, as defined by James, is about the mind or brain. Although psychologists study the brain, they do not fully understand how it works in our experiences and behaviors, like giving birth or watching a football match. It is hard to study the brain directly, so psychologists learn more by studying behavior and making guesses about what happens inside us. A challenge in psychology is that scientific facts should be objective and verifiable, but the mind's workings are not directly observable like an engine. We can only see them indirectly through behavior. Psychology is like solving a crossword puzzle, using clues from careful observation and logical analysis. Psychology aims to describe, understand, predict, and learn how to control or change the processes it studies. Once these goals are achieved, psychology can help us understand our experiences and apply findings to real life. Psychological research has been useful in many areas, like teaching children to read, designing safer machines, and helping people communicate their feelings better. Although people have discussed psychological questions for centuries, they have only been studied scientifically in the last 150 years. Early psychologists used introspection, or looking into one's own mind, to answer questions. They wanted to identify mental structures, but introspection has limitations. As Sir Francis Galton said, it only shows a small part of brain activity. William James compared it to trying to see darkness by quickly turning on a light. Today, psychologists prefer to base their theories on careful observations of behavior rather than personal reflections."
84,C1,"In a warehouse in a business park in Michigan, USA, there is a place called the Museum of Failed Products. This museum shows the other side of modern marketing, where not all products succeed. Here, you can find unusual items like A Touch of Yogurt shampoo and Breakfast Cola, which were taken off the market because almost no one wanted to buy them. The museum is owned by a company called GfK, and its owner, Carol Sherry, believes each product tells a sad story of failure for the designers, marketers, and salespeople involved.
The surprising thing about the museum is that it is a successful business. You might think that companies would keep their own collections of failed products, but many do not. Executives visit the museum regularly, showing how rare it is for companies to keep their own records of failures. The museum started by accident. Robert McMath, a former marketing professional, began collecting new products in the 1960s to create a 'reference library.' He didn't plan to focus on failed products, but since most products do not succeed, his collection naturally became one of failures.
Today's culture of optimism might explain why these products ended up in the museum. Each product likely went through many meetings where no one realized it would fail. Even if someone did, marketers might have continued to invest money to try to make some sales and save face. Little effort is made to understand why these products failed, and people involved often avoid discussing it.
This focus on optimism is also common in the growing 'self-help' industry. One popular method is 'positive visualization,' which suggests that imagining success makes it more likely to happen. Neuroscientist Tali Sharot has found that people tend to be overly optimistic about their chances of success. Her research shows that well-balanced people often have a less accurate view of their ability to influence events compared to those with depression.
Psychologist Gabriele Oettingen has studied whether 'positive fantasies about the future' are effective. Her research shows that thinking too much about success can actually reduce motivation to achieve goals. For example, people who imagined having a successful week at work often achieved less.
Psychologist Carol Dweck suggests that our beliefs about ability affect how we handle failure. People with a 'fixed theory' mindset think ability is natural and unchangeable, so they see failure as proof they are not good enough. In contrast, those with an 'incremental theory' believe ability can grow with effort and see failure as a chance to improve. Dweck compares this to weight training, where muscles grow stronger after being pushed to their limits. Having an incremental mindset can lead to a happier life, whether or not it leads to success."
85,C1,"Everyone knows that sports teams have an advantage when they play at home. But why is that? Many people think they know the answer, but professional sports are changing quickly, and what we used to believe is now being questioned. Two main reasons for this are science and money. Sports scientists want to understand what helps players perform their best, and they have many theories about home advantage. On the other hand, people who manage money in sports wonder why home advantage matters if players are paid well. Shouldn't players perform well anywhere if they are paid enough?
What about the fans? Would it matter if a team like Manchester United played some home games in the Far East to reach their fans there? It would matter to British fans, who believe their team needs their support. Fans often think that when they cheer, their team scores, but they forget the times when cheering didn't lead to a goal.
However, there is one thing that fans believe which is supported by science. Home fans often try to influence the referee. In an experiment, referees watched a match with and without crowd noise. Those who heard the noise were less likely to call fouls against the home team. This matches what happens in real games. Referees try to avoid making decisions that would upset the home crowd.
Studies show that home advantage has decreased in all major sports, but not as much as expected. For example, in the 1890s, home teams in England's Football League won about 70% of the points, compared to 60% today. Travel used to be difficult, but now players travel in comfort, and stadiums are more similar to each other. Despite these changes, home advantage still varies by sport. Basketball has the most home advantage, followed by football, while baseball has the least. This might be because baseball focuses more on individual actions, while basketball requires teamwork, which is boosted by playing at home.
Another reason for home advantage might be related to players' testosterone levels, which are higher before home games. This could be a natural urge to defend their home ground. In the recent Rugby World Cup, some underdog teams won at home against stronger opponents, showing the power of home advantage. As one referee said, ""It's the law of the jungle out there."""
86,C1,"""‘The more I practise, the luckier I get,’ said the golfer Gary Player about 50 years ago. This saying is famous in sports and is valued by athletes and coaches. The quote's origin is debated, with many claiming it for other golfers of that time. Its meaning is also interesting. Player didn’t mean he was literally lucky; he meant, ‘The more I practise, the better I get.’ This idea is part of the nature-nurture debate in sports: is talent something you are born with, or can it be developed? However, this is not really a balanced debate. The idea that practice is more important than natural talent has been very popular. Psychologist Anders Ericsson is known for the theory that becoming an expert in any field requires 10,000 hours of practice. This theory has inspired books like Malcolm Gladwell’s ""Outliers"" and others that say practice is the key to success. They suggest that 10,000 hours is enough to make anyone an expert. But, as Epstein argues in his book, this is not always true. He points out a problem with studies on excellence: they often only look at successful people. Epstein explores why some people have certain abilities and others don’t, considering factors like environment, support, and determination. His research takes him around the world, from Kenya to Sweden, and even to the Arctic. In Alaska, he learns that in a tough sled dog race, the dogs’ drive and desire might be influenced by their genes. This suggests that determination might also have a genetic component. Epstein talks about the complexity of the topic, distinguishing between nature (genes) and nurture (environment and practice). He agrees that both are important for elite athletes. He does not dismiss the role of training or environment, suggesting that if Jamaican sprinter Usain Bolt had grown up in the US, he might have become a basketball player instead of the fastest man in history. Epstein also looks at how genes can be crucial in some cases and examines issues of race and gender. He asks: ‘If only practice matters, why do we separate men and women in sports competitions?’ Sometimes the best questions are the simplest."""
87,C1,"A recent international report shows that many children in rich countries feel lonely and unhappy. Jay Griffiths asks a simple question: why are children today so unhappy? In her book, ""Kith,"" Griffiths explains that children spend too much time indoors, in front of screens like TVs and computers, and have lost touch with nature. She believes this is the main problem. This idea is similar to other discussions about childhood today. A follow-up study interviewed children to understand their unhappiness better. It found that many children would be happier if they could play outside more. Many people over 40 remember their own childhoods fondly, talking about the freedom they had to explore and play outside. They think this was healthier than the way children live now. However, they often forget that they are the ones who have made life more protective for their children, worrying about risks. Griffiths' book has strong arguments. She talks about how parents' fear of danger keeps children indoors, which benefits the toy and gadget industry. She also discusses trends like giving medication to restless children or making them wear goggles for playground games. It's unclear how common these rules are, but Griffiths expresses her frustration clearly. She also talks about childhood freedoms and rules, from fairy tales to school regulations. While her arguments are interesting, Griffiths sometimes goes too far. She ignores important counter-arguments and at one point compares modern treatment of children to racism, which seems extreme. Griffiths is a romantic when it comes to children. She has an idealized view of them, which might not match reality. She believes children should have freedom to explore and take risks. She says children need small accidents to learn how to avoid bigger ones later. However, she doesn't explain how to ensure these accidents are ""the right size."" Also, not all children may want the adventurous life she describes. What about the quiet and cautious children? Maybe the real mistake is forcing all children into the same lifestyle, whether keeping them inside or pushing them outside."
88,C1,"I am a research bio-psychologist with a PhD, so I have spent a lot of time in school. I am good at solving problems in my work and life, but this is not because of my schooling. Most life problems cannot be solved with complex formulas or memorized answers from school. They need judgment, wisdom, and creativity, which come from life experiences. For children, these experiences come from play. My recent research focuses on the value of play for children's development. All mammals, including humans, play when they are young. Those with more to learn play more. Carnivores play more than herbivores because hunting is harder to learn than grazing. Primates play more than other mammals because their lives depend more on learning than on instincts. Children, who have the most to learn, play more than any other young primates when they can. Play is how adults and mammals have always educated themselves. The most important skills for children to live happy, productive, moral lives cannot be taught in school. These skills are learned and practiced in play. They include creativity, getting along with others, cooperating, and controlling impulses and emotions. Creativity is important for economic success. We no longer need people to follow instructions like robots or do routine calculations. We need people who can ask new questions and solve new problems. If we can develop thinkers who anticipate obstacles, we will have a strong workforce. This requires creative thinking. A creative mind is a playful mind. Geniuses are adults who keep and build on their childlike creativity. Albert Einstein said school almost destroyed his interest in math and physics, but he regained it after leaving school. He called his work 'combinatorial play'. He developed his theory of relativity by imagining himself chasing a sunbeam. We cannot teach creativity, but we can suppress it with schooling that focuses on imposed questions with one right answer. More important than creativity is getting along with others, caring for them, and cooperating. Children are born wanting to play with others, and this play helps them learn social skills, fairness, and morality. Play is voluntary, meaning players can quit. If you cannot quit, it is not play. Players know they must keep others happy to continue the game. This makes play very democratic. School has become more demanding: breaks are shorter, homework has increased, and there is more pressure for high grades. Outside school, adult-directed sports have replaced spontaneous games. 'Play dates' with adults have replaced unsupervised neighborhood play, and adults now feel they must intervene instead of letting children solve their own problems. These changes have been gradual but significant. They are due to social factors like parents' fears, experts warning about dangers, less cohesive neighborhoods, and the belief that children learn more from adults than from each other. Our children do not need more school. They need more play. If we care about our children and future generations, we must reverse the trend of the past fifty years. We must give childhood back to children. They must be allowed to play and explore so they can grow into strong adults ready for an unpredictable future."
89,C1,"In many countries, more people in their twenties are using social media to find jobs. Platforms like Twitter and LinkedIn allow them to contact potential employers directly, which used to be possible only by standing outside an office with a ""hire me"" sign. However, with more access, there is also a higher chance of making mistakes. For example, a young jobseeker in the US contacted a senior marketing executive on LinkedIn. This executive had many important contacts, and the jobseeker hoped they could help him get a job. But the executive was not happy with this request. She refused his contact request and sent a harsh rejection note, which went viral online. People who saw the note were shocked, and the executive might regret her tone, even if not the message. However, this incident might help young people think more carefully about using social media professionally. It shows that social media can be risky for job seekers who do not know how to use it properly, and many are making mistakes. 
There is an irony here because, in many countries, social media sites like Facebook and Twitter have been a big part of young people's social lives for years. When my generation was young, social media was a way to escape from parents and teachers. It was a place to impress and experiment, often based on fantasy. You could talk to someone online for hours and then ignore them at school. With the right pictures or songs on your Facebook page, you could become a different person overnight. But when it comes to professional networking, our experience with sites like Facebook might actually be a problem. Using social media for work is very different, but some young people do not see the difference. We first became popular online by being bold and confident, which might be why some of us still think this is a good idea. Just because many people liked your posts on Facebook, it does not mean you can use LinkedIn to show employers you are worth hiring. We need to understand that what we learned about social networking as teenagers does not apply now, and we must meet employers' standards to succeed in the job market.
One common complaint from employers about young job seekers on professional networking sites is that they are too familiar in their communication and seem arrogant. This reinforces older generations' views of us as an ""entitled generation."" In reality, we are far from this; in many countries, we are desperate to find jobs, which is why we turn to social media. This impression of arrogance can hurt young people's chances of getting a job, even if they have the skills and motivation to be valuable employees.
So, what is the right way to contact someone on a professional networking site? First, clearly explain who you are and what you can offer the person you are contacting. Maybe you could do some research for them or help in another way. This approach gives you a better chance of getting a positive response. Avoid sending impersonal, mass emails, and keep your tone humble to avoid offending the recipient. Remember, social media can be a great way to make useful contacts, but it needs careful handling if you do not want to be rejected."
90,C1,"'Anyone who says they can predict the future of newspapers is either lying or foolish. If you look at the numbers, newspapers seem to be in trouble. Since 2000, the circulation of most UK national newspapers has dropped by a third to half. The Pew Research Centre in the USA says that newspapers are now the main news source for only 26% of US citizens, compared to 45% in 2001. Many people predict that the last printed newspaper will disappear within 15 years. However, history shows that old media often survive. In 1835, a New York journalist said that books and theatre were finished and newspapers would become the main form of social life. But theatre survived newspapers, cinema, and television. Radio has done well in the TV age, and cinema has stayed popular despite videos and DVDs. Even vinyl records have become popular again, with online sales up 745% since 2008. Newspapers were once new media too, but it took centuries for them to become the main source of news. This happened in the mid-19th century with the steam press, railway, and telegraph. It was also important that people started to believe that everything is always changing and we need regular updates, a new idea in those times. Now, we expect change. In medieval times, people only noticed the changing seasons and big events like famine or disease, which they couldn't predict. Life was seen as cyclical, with important truths repeating. Journalism as a full-time job only started in the 19th century. Even then, there was no clear reason why people needed news regularly. Regular newspaper publication can be a burden. Online news readers can check news whenever they want. Search engines and algorithms let us personalize news to our interests. When big stories happen, internet news can update minute-by-minute. Mistakes can be corrected quickly, and there are no space limits for stories. This is very different from newspapers. However, online news often focuses on being first and creating excitement, rather than spreading understanding. In medieval times, news was shared in markets or taverns, mixed with rumors and misunderstandings. In some ways, we are returning to that. Newspapers have not always been good at explaining how the world works. They might face extinction. Or, as the internet adds to our feeling of living in a chaotic world, newspapers might find that they can guide us to wisdom and understanding.'"
91,C1,"'If humans were truly comfortable under the light of the moon and stars, we would enjoy the darkness, seeing the midnight world as clearly as many nocturnal animals do. However, we are daytime creatures, with eyes made for sunlight. This fact is part of our genetics, even if we don't often think of ourselves as daytime beings, just like we don't always think of ourselves as primates or mammals. This explains why we have changed the night by filling it with light. We have controlled the night like we control rivers with dams. This control has benefits but also causes problems, known as light pollution, which scientists are just starting to study. Light pollution mostly comes from poor lighting design, where artificial light shines into the sky instead of focusing downwards. This bad lighting changes the darkness of night, affecting the natural light levels and rhythms that many living things, including humans, have adapted to. Wherever artificial light enters the natural world, it affects life processes like migration, reproduction, and feeding. For most of human history, the idea of 'light pollution' would not have made sense. Imagine walking towards London on a moonlit night around 1800, when it was the world's most populated city. Nearly a million people lived there, using candles, torches, and lanterns. Only a few houses had gas lights, and there were no public gaslights in the streets yet. From a few miles away, you would have smelled London before seeing its faint glow. We have lit up the night as if it were empty, but it is not. Many mammals are nocturnal. Light is a strong biological force and attracts many species. It is so strong that scientists say songbirds and seabirds are 'captured' by searchlights on land or by light from gas flares on marine oil platforms, flying around them until they fall. Birds migrating at night can crash into brightly lit tall buildings; young birds on their first journey are especially at risk. Some birds, like blackbirds and nightingales, sing at strange times because of artificial light. People once thought light pollution only affected astronomers, who need a clear night sky. Unlike astronomers, most of us don't need a perfect view of the night sky for work, but we do need darkness. Ignoring darkness is pointless. It is as important for our health as light. Changing our internal clock can cause health problems. The regular pattern of waking and sleeping is a biological expression of the Earth's light cycle. These rhythms are so important that changing them is like changing our balance. In the end, humans are as affected by light pollution as frogs near a bright highway. Living in our own bright world, we have lost touch with our evolutionary and cultural heritage – the light of the stars and the natural rhythms of day and night. Light pollution makes us forget our true place in the universe, which is best understood under a deep night sky with the Milky Way above.'"
92,C1,"The founder of a large international company recently announced that his company will stop tracking employees' paid holiday time. This decision was inspired by an internet company with a similar policy. The founder got the idea from an email from his daughter, which seemed like it was written by his media team. Ignoring the way the announcement was made, we should ask: is this idea practical? The internet company and the multinational corporation are very different. The internet company has 2,000 employees and offers one service, while the multinational has 50,000 employees and many different services like finance, transport, and healthcare. The idea of ""take as much time off as you want as long as it doesn’t harm the business"" might work better in a smaller company where employees know each other's work better. The founder of the multinational said employees can take as much leave as they want if they are sure their work and team are up to date and their absence won’t harm the business or their careers. But can anyone be that sure? Even if you prepare before a holiday, there is always work waiting when you return. This is just how leave works; work piles up while you are away. Someone following these rules might not take leave at all or feel guilty about it. Guilt can lead to stress, and not taking enough leave can reduce productivity over time. There might also be pressure from colleagues and office gossip about who is off and for how long. This pressure already affects when people start and end their workday. In the corporate world, there is a culture of working late, and this could lead to a ""no holiday"" culture in a company with unlimited leave, where workers compete for promotions. If the security of guaranteed leave is removed, people might feel they can’t take the leave they need, fearing they look lazy. They would lose their legal right to rely on. The policy might make workers feel unable to take leave, or they might stick to their legal rights, making the policy pointless. Modern technology lets us get work messages anytime, blurring the line between work and free time. The internet company started their unlimited leave policy when employees asked how this new way of working fit with the old time-off policy. If the company can’t track work hours accurately, why use an old standard for time off? But if there are no set work hours, all hours could be work hours. Employees might not know if their work hours are being watched, making them self-monitor, which can be harmful. Employment laws exist for a reason. Workers have a right to a minimum amount of paid leave because rest is important for their health. The benefits like better morale, creativity, and productivity from unlimited leave can exist without affecting worker well-being. I doubt that ""taking as much holiday as they want"" is the real goal or likely result of this policy."
93,C1,"
Journal-based peer review is the process where experts in the same field check a scientific research paper. It is seen as a way to ensure the quality of research. This process is said to stop flawed or nonsensical papers from being published. Scientists often mention it to reassure the media and the public. However, reviewing a paper can delay its publication by up to a year. Is this delay worth it to ensure trust in published research? The answer is both yes and no. 
Examining these issues shows that scientific publishing is changing. I am not ready to give up on journal-based peer review. I believe all papers should be checked before formal publication, but things are changing. The use of preprints is a big part of this change. Preprints are drafts of papers posted online without peer review. They allow new results to be shared quickly so others can read, critique, and build on them. 
Publishing in journals has become more about gaining fame and advancing careers. This has changed the motivations of authors and reviewers. Scientists compete for spots in top journals, which publish excellent research. But the high rewards for publishing in these journals can lead to cutting corners. Stories that leave out inconvenient data are more likely to be published. Reviewers now often decide if a paper is good enough for a journal, not just if it is good. For top journals, this can depend on how newsworthy a paper is, not just its scientific quality. 
These problems are known, but few people want to change the system. However, preprints might offer a solution. They do not require a big change from the current system. Preprint archives have existed for twenty years, but they are not widely used. This is partly because scientists are conservative and because many think journals will not accept papers posted as preprints. There is also a fear that publishing without peer review will lead to 'junk science,' but this has not happened yet. 
Preprints are not peer-reviewed, but authors know they will be critiqued by a global community. Tanya Elks, a psychology professor, shared her experience: ‘My paper critiqued a published paper, which is hard to do in traditional journals. With anonymous peer review, the original authors might block a critical paper. By posting a preprint, the original authors could respond, and all comments were public. This way, readers can judge the quality of the arguments. Even if journals reject our paper, the preprint and comments are still available, so our work is not wasted.’
Preprint archives allow global scientific discussions that used to be private. They can also share negative results, which journals often ignore. Preprints increase how often papers are read and cited, showing their effectiveness. By using the web’s openness, preprints focus on the work itself, not where it is published."
94,C1,"'When I ask my literature students what a poem is, they often say things like ‘a painting in words’. These answers usually don't satisfy me or them. One day, I asked a group to choose an object and write one paragraph describing it like a scientist would, and another from the object's point of view, calling it ‘Poem.’ Here’s what one student wrote: Poem I may look strange or scary, but I’m a device that helps people breathe. I’m only used in emergencies and even then only for a short time. Most people will never have to use me. The item? An oxygen mask – a surprising choice that helped the class see how poetry works in a unique way. The exercise led to fun and useful discussion. When I was in school, poetry often made no sense to me. I saw every poem as a confusing puzzle that blocked true understanding and feeling. After school, poetry becomes something most people care about less and less. Sometimes you see a poem, and it stands out because it is not like regular writing. It almost challenges you to read it, which you do, and often feel let down by its simplicity or because it is hard to understand at first. Still, you feel good for trying. What do we hope to find in poems? Aren’t they supposed to hold deep feelings, beautiful images, gentle thoughts, or sharp humor? The answer might seem to be yes. But if we want tears, we watch movies; for information or sharp critique, we read online articles. Novels offer us worlds to escape to, paintings give us visual pleasure, and music – well, nothing beats the mix of lyrics, instruments, and melody. However, one thing a poem can offer is ambiguity. Everyday life – unlike reading sentence by sentence – is full of it. But these ideas still don’t tell us what a poem really is. If you search online for ‘poem’, it redirects to ‘poetry': ‘a form of literary art which uses the beauty and rhythm of language like sound symbolism, etc.’ This is fine for English professors, but it hides the word’s roots. ‘Poem’ comes from the Greek poí?ma, meaning a ‘thing made,’ and a poet is ‘a maker of things.’ So if a poem is a thing made, what kind of thing is it? Poets sometimes compare poems to wild animals – untameable, unpredictable – or to machines – carefully made and precise – depending on their view. But these comparisons don’t hold up when looked at closely. The best part of trying to define a poem through comparison is not the comparison itself but the discussion it creates. Whether you see a poem as a machine or a wild animal, this process can change how you think about machines or wild animals. It can help the mind try new ways of thinking and make us see everyday things differently. The poem as a mental object is not hard to imagine, especially if we think about how song lyrics can get stuck in our heads. The mix of words and melody is powerful, going back to childhood rhymes like ‘Sticks and stones may break my bones, but words can never hurt me.’ But aren’t words sometimes like sticks and stones? Think about a poem on a page of a newspaper or magazine, looking right at you: A poem can hit home like nothing else, even though, in terms of ink on paper, it does nothing more than the prose around it. What about all that empty space around the poem – space that could have been used for a longer article or ad? A poem is made by writing and rewriting just like an article, story, or novel, yet it never really becomes what they usually are – a thing made to be sold. Publishers write press releases and send out review copies of poetry collections, but few expect a collection to pay for its printing costs. A poem presents itself not as something for the market, but as something for its own sake. Because of its special status – set apart in a magazine or book – a poem still has the power to surprise, even if only for a moment.'"
