text_id,text_level,text
1,C2,"Some time ago, a website warned about the dangers of public check-ins, which are online posts about where you are. The website's message was clear: while you might think you are just sharing your location, you are also letting many people know where you are, including those you might not want to meet. This reflects a growing understanding that sharing everything online can have negative effects. The internet offers many exciting opportunities to share our lives with a global audience, which can lead to wealth and fame. However, we often realize too late that this sharing can be risky and overwhelming.
But don’t lose hope. There is guidance from early internet users who explored these challenges before us. In the beginning days of the web, they faced many difficulties, losing jobs and friends while dealing with the temptations of fame, long before social media existed. These early bloggers experienced what many of us are now going through. It’s important to learn from their experiences.
In January 1994, Justin Hall, a 19-year-old student, started posting on the web, which was mostly used by graduate students and scientists. The web was created at CERN, a physics lab in Switzerland, to help researchers share their work. Hall saw a chance to share his own life. He created a personal website filled with stories, photos, and art. In January 1996, he began a daily blog, attracting many readers who were fascinated by his boldness. Hall was open about everything; no topic was off-limits. While some might see this as attention-seeking, there was also a creative and artistic side to his work.
However, one day, visitors to Hall’s site found it replaced by a video called Dark Night. He shared that he had fallen in love, but when he wrote about it online, he was told to choose between his blog and his relationship. He felt that sharing his life online made people distrust him. The blog ended, but the issue remains: sharing online can be fun, but if you think it will make people want to be around you, you might be disappointed.
In 2002, Heather Armstrong, a young woman in Los Angeles, had a blog called Dooce. She sometimes wrote about her job at a software company. One day, a colleague sent her blog link to the company’s vice presidents, including some she had criticized, which led to her losing her job. Experts call this the “online distribution effect,” which means people often say things online that they wouldn’t say in person. However, the internet is not a separate world; our online actions can have real-life consequences. Ignoring this can lead to serious mistakes.
Armstrong’s story had a happy ending. Although she was upset and stopped blogging for a while, she eventually got married and started a new blog about her family. Today, she is successful as a “mommy blogger,” and her writing supports her family. Once known for her online mistakes, she has learned to share her life more carefully. What Armstrong teaches us is important: the internet allows us to say anything, but that doesn’t mean we should."
2,C2,"Some scientists recently started a campaign to warn people about a low-budget film called *What the Bleep Do We Know?* This film mixes documentary and drama to suggest that there is much we do not understand about our universe. While scientists can sometimes be arrogant, even the most confident physicist would not claim to know everything about the cosmos. However, some scientists felt it was important to warn the public about the film, calling it everything from ""atrocious"" to ""very dangerous."" This made me curious to see the movie.
At first, I didn’t understand why there was so much concern. Various scientists made simple statements about how new discoveries show that the universe is stranger than we thought. But when the film started discussing some of these discoveries, I began to see the reason for the fuss. One claim was that water molecules can be influenced by thoughts. I had heard about a Japanese researcher who suggested that the shape of a water molecule could change based on the feelings of people nearby. However, the film only showed pictures of ice crystals that looked beautiful when someone spoke kindly to them and ugly when exposed to negative emotions. Many people find this kind of evidence convincing, but scientists often respond with skepticism.
I understand their point. The idea that thoughts can affect water is fascinating and could suggest new forces in the universe. But before we get too excited, we need solid proof that this effect is real. Beautiful pictures of crystals are not enough. The real issue is that the movie's claims were not strange enough. For example, water molecules might have properties linked to a mysterious energy that is causing the universe to expand. This idea is supported by decades of research from scientists around the world.
In reality, discoveries are being made that show the universe is indeed stranger than we ever imagined. Astronomers have found that the universe is made of an unknown type of matter and is driven by a mysterious force called ""dark energy."" On a more practical level, neuroscientists have discovered that our awareness of events happens about half a second after they occur, a delay we don’t notice because our brains edit it out. Anthropologists believe they have found where modern humans first appeared and how they spread across the world. Some theorists even suggest there are connections between life on Earth and the fundamental design of the universe.
Despite what some may think, science is not close to being complete. In fact, we seem to be further from knowing everything than ever before. Many natural phenomena may never be fully understood. The ideas of chaos and quantum uncertainty show that there are limits to what we can know. Some of the world’s top physicists are trying to create a ""Theory of Everything"" that would explain all the forces and particles in the universe with one equation. Overall, many of us believe that the universe can be described in one word: incredible."
3,C2,"To put it simply, I find writing novels challenging, but writing short stories is a joy. If writing novels is like planting a forest, then writing short stories is more like planting a garden. Both activities work together to create a beautiful landscape that I love. The trees provide shade, and the wind makes the leaves rustle, sometimes turning them a bright gold. In the garden, flowers bloom, and their colorful petals attract bees and butterflies, showing the change of seasons.
Since I started my career as a fiction writer, I have often switched between writing novels and short stories. My routine is this: after finishing a novel, I want to write short stories; after completing a group of short stories, I feel ready to focus on a novel. I never write short stories while working on a novel, and I don’t write a novel while working on short stories. The two types of writing may use different parts of my brain, and it takes time to switch from one to the other.
I began my career with two short novels in 1975, and from 1984 to 1985, I started writing short stories. I didn’t know much about writing short stories back then, so it was difficult, but I found the experience very memorable. I felt my fictional world expand, and readers seemed to enjoy this different side of me as a writer. One of my early works, “Breaking Waves,” was included in my first short-story collection, Tales from Abroad. This marked my beginning as a short-story writer.
One of the joys of writing short stories is that they don’t take long to finish. Usually, it takes me about a week to shape a short story (though I can revise it many times). This is different from the long commitment needed to write a novel, which can take a year or two. For me, writing a novel can feel endless, and I sometimes wonder if I will make it through. So, writing short stories gives me a needed change of pace.
Another nice thing about short stories is that you can create a story from the smallest ideas—like a thought, a word, or an image. Often, it feels like jazz improvisation, where the story takes me in unexpected directions. Plus, with short stories, you don’t have to worry too much about failing. If an idea doesn’t work out, you can just accept it and move on. Even great writers like F. Scott Fitzgerald and Raymond Carver didn’t write a masterpiece every time. This thought comforts me. You can learn from your mistakes and use that knowledge in your next story.
When I write novels, I try to learn from both my successes and failures in writing short stories. In this way, short stories serve as a kind of experimental space for me as a novelist. It’s hard to experiment the way I want to within a novel, so without short stories, writing novels would be even more challenging.
My short stories are like soft shadows I have left in the world, faint traces of my thoughts and feelings. I remember exactly where I placed each one and how I felt at that moment. Short stories are like guideposts to my heart, and it makes me happy as a writer to share these personal feelings with my readers."
4,C2,"At its most basic level, science is related to philosophy, and at its most practical, it helps to cure diseases. Science has made our lives easier but has also posed threats to our existence. It tries to understand everything from tiny ants to the vast universe, but it often struggles to do so. Science has influenced poets, politicians, philosophers, and even frauds. Its beauty is often seen only by those who study it deeply, while its dangers are often misunderstood. People have both overestimated and underestimated its importance, and the mistakes made by scientists are sometimes ignored or exaggerated.
The history of science is full of conflict. Established ideas are often changed or completely rejected, similar to how new music styles are sometimes mocked before becoming popular. The battle between old and new ideas in science is rarely respectful. Scientists can be jealous and angry, and conflict is a big part of scientific history. This book will show science as a collection of ideas that have changed not just science itself but also many areas of human thought. While science has practical benefits, this book will focus more on the ideas behind it, appreciating their beauty and being ready to question them. We must recognize both the creativity and the limitations of human understanding.
Science is always changing. There is always a scientist somewhere challenging the ideas of another. Most of the time, these changes do not disturb society too much. However, sometimes, they can shake our established beliefs. For example, in the 17th century, science described the universe as a giant clock. Three hundred years later, physics began to question this idea, leading us to a complex understanding where our observations can change what we see, and we still do not fully understand our basic concepts.
Some people think that the changing nature of scientific theories shows that science cannot explain the universe. However, these changes usually help us understand and predict nature better. For instance, Isaac Newton explained much more than the ancient Greek thinker Aristotle, and Albert Einstein explained even more than Newton. Science may make mistakes, but it continues to progress. 
At the end of the 19th century, many physicists believed there was not much left to discover in physics. Then came major breakthroughs like radioactivity, X-rays, the electron, and many new particles, as well as concepts like quantum mechanics and relativity. Biology has also made significant discoveries. Today, some people claim that a complete theory explaining everything about the universe is close to being found. 
Science is not just a harmless activity. In the last 200 years, we have moved from simply observing nature to controlling it in some ways. This has sometimes upset the balance of nature in ways we did not fully understand. It is important for everyone, not just scientists, to understand scientific advances because they will shape the world our children will live in. Science is now a key part of how humanity thinks about and shapes its future. The decisions made in science can impact budgets, health, and even the future of life on Earth."
5,C2,"From around 2015, after many years of growth, major publishers began to notice that ebook sales had stopped increasing or even decreased in some cases. This raised doubts about the future of ebooks in the publishing industry. One publishing executive anonymously admitted that the excitement around ebooks may have led to poor investments, causing his company to lose faith in the value of printed books. Despite the clear idea that digital and print books can exist together, the question of whether ebooks will replace print books still comes up. Whether people are trying to predict or dismiss this idea, it is interesting that the possible disappearance of books continues to spark our imagination and create strong discussions.
Why is this idea so strong? Why do we see the relationship between ebooks and print books as a battle, even when evidence shows they can coexist? The answers to these questions go beyond ebooks and reveal much about our mixed feelings of excitement and fear regarding change and innovation. In my research, I have explored how the idea of one medium replacing another often appears with new technologies. For example, when television was invented, many believed that radio would disappear. However, radio survived by finding new ways to be used, like listening in cars or during commutes.
The idea that books might disappear is not new either. As early as 1894, people thought that the phonograph would replace books with what we now call audiobooks. This pattern has repeated many times. Movies, radio, television, hyperlinks, and smartphones have all been said to threaten print books as sources of culture and entertainment. Some people worried that the end of books would lead to a decline in culture, while others exaggerated the benefits of ebooks in a digital future. 
The idea of the death of books often appears during times of technological change. This narrative reflects our hopes and fears about new technology. To understand why we react this way, we need to recognize that we form emotional connections with different media as they become important in our lives. Many studies show that we develop strong feelings for objects like books, televisions, and computers, even naming our cars or getting frustrated with our laptops. Therefore, when new technology, like e-readers, appears, it not only brings economic and social change but also forces us to rethink our relationship with something that has been a big part of our daily lives. As technology evolves, we often miss what we used to know and no longer have. This is why industries often grow around older products and technologies.
For example, when the printing press spread in 15th-century Europe, people began to seek out original manuscripts. The change from silent to sound movies in the 1920s made people nostalgic for the older style. The same happened when photography changed from analog to digital and from vinyl records to CDs. Similarly, e-readers have led to a new appreciation for the physical qualities of traditional books, even their unique smell. This should comfort those who worry about the future of print books. However, the idea of disappearing media will continue to be an interesting story about the power of technology and our fear of change. 
One way we cope with change is by using familiar stories, like those of tragedy and endings. These stories are easy to remember and share, and the narrative of the death of the book reflects both our excitement for the future and our fear of losing parts of our familiar world—and ultimately, ourselves."
6,C2,"For a year and a half, I woke up every weekday at 5:30 AM. I brushed my teeth, made coffee, and wrote about how some of the greatest thinkers of the last four hundred years managed their time to do their best work. I focused on the everyday details of their lives, like when they slept, ate, worked, and worried. My goal was to show a different side of their personalities and careers, presenting them as people with habits like everyone else. 
The French writer Jean Anthelme Brillat-Savarin once said, ""Tell me what you eat, and I shall tell you what you are."" I believe, ""Tell me what time you eat, and if you take a nap afterward."" This book is somewhat simple; it looks at the conditions for creativity rather than the creative work itself. However, it is also personal. The novelist John Cheever believed that even a business letter reveals something about the writer. 
I wrote this book to explore questions I face in my own life: How can you do meaningful creative work while making a living? Is it better to focus entirely on one project or to work on it a little each day? When time is limited, do you have to give up things like sleep or a clean house, or can you learn to do more in less time? I don’t claim to answer these questions, as some may only be resolved through personal compromises. Instead, I provide examples of how many successful people have dealt with similar challenges. 
The book is called Daily Rituals, but it really focuses on people’s routines. Routines can seem ordinary and thoughtless, but they can also be powerful tools for managing limited resources like time, willpower, and optimism. A good routine helps organize our mental energy and can protect us from being influenced by our moods. The psychologist William James believed that having good habits allows us to focus on more interesting tasks. Ironically, he struggled with procrastination and sticking to a schedule himself.
This book began during a moment of procrastination. One Sunday, while trying to write an article for a small architecture magazine, I found myself cleaning my workspace and making coffee instead of working. As a ""morning person,"" I am very focused in the early hours but not as productive later in the day. To feel better about this, I started looking online for information about other writers’ schedules. I found many interesting stories and thought it would be great to collect them, which led to the Daily Routines blog I started that day and now this book.
The blog was informal; I shared descriptions of people’s routines from biographies and articles. For the book, I gathered a much larger and better-researched collection while keeping the variety of voices that made the blog enjoyable. I let my subjects speak for themselves through quotes from their letters, diaries, and interviews. In some cases, I summarized their routines from other sources. I want to acknowledge that this book would not have been possible without the research of many biographers, journalists, and scholars. I have listed all my sources in the Notes section for those interested in further reading."
7,C1,"Howard became a palaeontologist because of a change in interest rates when he was six years old. His father, who was careful with money and had a big mortgage, said that their planned holiday to Spain was no longer possible. Instead, they rented a chalet on the English coast. One rainy August afternoon, Howard found an ammonite on the beach. He had always wanted to be a palaeontologist, and by the end of university, he knew what kind he wanted to be. He was not interested in dinosaurs or the Jurassic period; instead, he was fascinated by the very beginnings of life and the ancient creatures found in grey rocks.
As he finished his doctoral thesis, he worried about finding a job. He believed in his abilities but knew that deserving a job does not always mean getting one. When a position for an Assistant Lecturer at Tavistock College in London opened up, he applied, but did not expect much. On the day of his interview, the professor who was supposed to lead the panel had a fight with his wife, drove his car into a gatepost, and ended up in the hospital. The interview went on without him, and a colleague who did not like Howard’s former professor took his place. This colleague helped Howard get the job, even though Howard was surprised by this unexpected support.
At first, Howard was grateful but later learned the truth about how he got the job. He felt a bit disappointed because he wanted to believe he was chosen for his skills. However, what mattered most was that he had the job and could do the work he loved. He often thought about how organized his professional life was, where he could plan and achieve goals, compared to the chaos of personal life, where strangers could unexpectedly change everything.
One day, Howard's briefcase, which contained notes for a lecture, was stolen at an Underground station. Angry, he returned to college, called to postpone the lecture, and reported the theft. After that, he went for a coffee and met a colleague who was with a curator from the Natural History Museum in Nairobi. During their conversation, Howard learned about a new collection of fossils that needed to be studied, which would be a great opportunity for him. Because of the theft, he changed his plans. He decided not to go to a conference in Stockholm or take students on a field trip to Scotland. Instead, he would find a way to visit the museum in Nairobi."
8,C1,"Charles Spence is a professor at Oxford University in the UK, and he is known for trying unusual foods. For example, he has ice cream made from bee larvae at home, which he says tastes a bit like nuts and flowers. Spence and his team are working on making eating bugs more acceptable. His research focuses on how our senses work together to create the flavors we experience. This research influences what we eat and drink, from large food companies to high-end restaurants.
Spence studies many factors that affect our taste, such as who we eat with, how food is presented, and even the color and weight of plates and cutlery. In his book, ""The Perfect Meal,"" co-written with Betina Piqueras-Fiszman, he shares interesting facts about eating. For instance, the first person to order in a group usually enjoys their meal more, and we tend to eat 35% more food when dining with one other person, and 75% more with three others.
Spence's lab is simple and not very modern. He uses basic equipment and soundproof booths for his experiments. By keeping costs low, he can work with chefs who cannot afford research on their own. Much of his funding comes from a large food company. In the past, university research funded by companies was often looked down upon, but now it is seen as valuable because it shows that research can have a real impact.
Spence is helping food brands reduce salt and sugar in their products, which is important for keeping customers healthy. Surprisingly, many companies make these changes slowly so that customers do not notice. Spence explains that when people know about these changes, they focus more on the taste and often do not like it as much.
Spence first met famous chef Heston Blumenthal while working on a project for a food company. At that time, people thought combining science and food was strange, but Spence believed it could change how people think about food. Their collaboration led to the creation of a dish called ""Sound of the Sea,"" which uses sound to enhance the dining experience.
Spence has found that different sounds can change how we taste food. For example, high-pitched music can make food taste sweeter, while lower sounds can make it taste bitter. Soon, airlines will match music with the food they serve. Last year, a brand even released an app that played music while ice cream melted, but they did not match the music to the taste, which is often a missed opportunity.
At home, Spence's dinner parties are quite unique. One time, they ate rabbit with the fur still on the cutlery, and another time, they used remote-controlled lights. They have even experimented with sounds and different drinks to see how they affect taste. For Spence, home, shops, food events, and conferences are all part of his research into food and flavor."
9,C1,"Our brains are working harder than ever. We are bombarded with facts, false information, chatter, and rumors, all pretending to be useful information. We have to sort through this to find what we really need to know and what we can ignore. At the same time, we are doing more tasks ourselves. Thirty years ago, travel agents booked our flights and salespeople helped us in stores. Now, we do most of these things on our own. We are trying to manage many responsibilities, including our lives, families, jobs, hobbies, and favorite TV shows, often with the help of our smartphones. These devices have become part of our busy lives, filling every free moment with tasks.
However, there is a problem. Although we believe we are multitasking—doing several things at once—this is a misleading idea. Earl Miller, a neuroscientist at MIT, explains that our brains are not designed to multitask effectively. When we think we are multitasking, we are actually switching quickly between tasks. Each time we switch, it costs us mentally. Instead of being expert jugglers, we are more like amateur plate spinners, moving from one task to another and worrying about what we might drop.
Even though we feel productive, multitasking actually makes us less efficient. It increases stress hormones like cortisol and adrenaline, which can lead to confusion and unclear thinking. Multitasking creates a desire for constant stimulation, rewarding our brains for losing focus. The prefrontal cortex, the part of the brain we need to stay focused, is easily distracted by new things, like shiny objects that attract babies.
Just the chance to multitask can harm our ability to think clearly. Glenn Wilson, a former psychology professor, calls this problem ""info-mania."" His research shows that trying to focus on a task while seeing an unread email can lower our IQ by almost 10 points. The cognitive losses from multitasking are even worse than those caused by being tired. Russ Poldrack, a neuroscientist at Stanford, found that learning while multitasking sends information to the wrong part of the brain. For example, if students do homework while watching TV, the information goes to a part of the brain for skills instead of the part for facts, making it harder to remember later.
Additionally, multitasking often involves making decisions, like whether to reply to a text or ignore it. Decision-making is tough on our brains, and small decisions use the same mental energy as big ones. After making many small choices, we can end up making poor decisions about important matters.
In discussions about information overload, email is often mentioned as a major issue. It’s not that email itself is the problem, but the overwhelming amount of messages we receive. When asked what his father does for a living, the son of a neuroscientist said, ""He answers emails."" This reflects a common truth: we feel we must respond to emails, but it seems impossible to do that and accomplish anything else."
10,C1,"In a zoo in Sweden, a chimpanzee named Santino spent his nights breaking concrete into pieces to throw at visitors during the day. Was he being mean? In caves in the US, female bats help other fruit bat mothers if they can’t find the right position to give birth. Are they being kind? Fifty years ago, these questions were not considered important. Animals had behaviors that led to measurable results, and scientists focused on those results. The idea that animals have thoughts, feelings, and moral values was seen as sentimental. However, this view has started to change. Research on the behavior of bats, chimps, rats, dolphins, and chickens has begun to explore animal emotions, which were once a taboo subject. This change has influenced popular science books like Mark Bekoff’s ""Wild Justice"" and Victoria Braithwaite’s ""Do Fish Feel Pain?"". 
This shift has sparked a debate that may never be fully answered: do animals have consciousness? This leads to another important question about conscience, which is a person’s sense of right and wrong. In a recent experiment with cows, those that opened a locked gate to get food showed more happiness—by jumping and kicking—than those that had the gate opened for them. If cows enjoy solving problems, what does this mean for how we produce and eat beef? While the observations are clear, their meaning is debated. Dr. Jonathan Balcombe, author of ""Second Nature,"" believes the logical response to this research is to stop eating meat. He thinks humanity is close to a major change in ethics, similar to the end of slavery. Aubrey Manning, a professor at Edinburgh University, believes we should reconsider how we view animal intelligence, suggesting that animals have a simpler understanding of the world than humans. Professor Euan MacPhail thinks we should avoid attributing human feelings to animals. 
These three views may never agree because the main issue is not just scientific or moral, but philosophical. Since defining consciousness is very difficult, can we ever truly understand what it is like to be a bat, as philosopher Thomas Nagel asked? Balcombe describes an important experiment showing that starlings, a type of bird, can become depressed. In a study at Newcastle University, starlings were divided into two groups. One group lived in spacious, comfortable cages, while the other lived in small, empty cages. Both groups learned to eat tasty worms from one box and avoid unpleasant worms from another. However, when only unpleasant worms were offered, only the starlings in the comfortable cages would eat. Balcombe concluded that living in a bad environment made the starlings pessimistic about life. 
Balcombe, who has worked with animal rights groups, has a strong opinion. He believes that just as we now look back at racism with disgust, we will one day feel the same about how we treat animals. He argues that we cannot support animal rights while eating meat. If he were the only one with this view, it might be easy to ignore him. However, Professor Aubrey Manning shares similar beliefs. Manning has written a textbook on animal behavior and says we are seeing a shift in how we think about animals. In the past, some believed animals thought like humans, but now we are moving in the opposite direction. This is a complex topic, and it is important to listen carefully to different academic opinions."
11,C1,"Critical thinking is a way to engage with what we read or hear to understand it better. Adrian West, a research director at the Edward de Bono Foundation in the U.K., notes that people often believe that arguing helps us find the truth. While technology helps us store and process information, there are worries that it also changes how we solve complex problems and makes it harder to think deeply. West points out that we are exposed to a lot of poor-quality thinking and popular opinions, which can overwhelm our ability to reason. Surprisingly, having more information does not always lead to better knowledge or decision-making. 
According to the National Endowment for the Arts, fewer people are reading literature, and this decline is speeding up. Patricia Greenfield, a psychology professor, believes that focusing more on visual media may be hurting our critical thinking skills. She says that people are more focused on real-time media and multitasking instead of concentrating on one thing. However, there is still no clear answer on how technology affects critical thinking. 
Technology has complicated the way we think, and researchers can no longer rely on old beliefs. It is easy to see technology as either good or bad, but the truth is that it can be both, depending on how it is used. For example, a computer game might help or hurt critical thinking. Reading online can improve analysis skills, but constantly clicking on links can prevent deeper thinking. Greenfield, who studied over 50 research papers on learning and technology, says that technology changes how we think. She explains that reading helps develop imagination and critical thinking, while visual media like video games and TV do not engage the imagination in the same way. 
However, she also found that visual media can improve some types of information processing. Unfortunately, most visual media do not give us time to reflect or think deeply. As a result, many young people may not reach their full potential. How society views technology affects how we think about critical thinking. This is especially clear when looking at video games and learning. James Paul Gee, a professor of educational psychology, argues that video games are not always bad for kids. In fact, they can be a great learning tool. Evidence shows that playing video games can help children develop better reasoning skills. Games like Sim City and Civilization teach decision-making and analytical skills in ways that feel real. These games also allow players to explore ideas that they might not be able to in real life. 
In today’s digital world, as reading and math scores drop, it is important to examine how technology affects our thinking and analysis."
12,C1,"When Matthew Crawford is not writing about how we should live, he works as a motorcycle mechanic. He chose this job after feeling unhappy with office work and his role in social policy. His first book praised the value of manual jobs, while his latest book discusses how to deal with modern life. He was inspired to write it when he noticed advertisements appearing on a credit card machine while he was shopping. Crawford realized that these distractions are hard to avoid. What we want to think about at any moment is personal, but we often cannot choose this because of things we do not even notice. It is becoming harder to think clearly or remember conversations. To avoid constant interruptions, people tend to isolate themselves and stop doing simple things like talking to strangers. Crawford points out that we increasingly experience the world through things like video games and apps, which often try to manipulate us. These distractions reflect our desires but can take over our lives.
Crawford is not alone in his concerns. Many office workers complain about emails but still spend their free time checking them. Studies show that just seeing a phone can distract us, which many of us have experienced during boring dinners. While there is no scientific proof yet that our attention spans have changed, it is clear that we are more aware of other things we could be doing. Some people think this problem is caused by technology, but Crawford believes that technology has made it easier for us to focus on ourselves. With so many choices, it is hard to control our attention, which affects our social lives. For example, we might prefer to text a friend instead of talking to them in person. By only interacting with digital representations of people, Crawford warns that we might lose something important in our society.
He also gives an example from his gym. In the past, there was one music player for everyone, which sometimes caused disagreements. Now, people listen to their own music with earbuds, and the gym has lost its social atmosphere. Real connections often happen through disagreements, like discussing different music tastes.
Crawford suggests two solutions. First, we need to reduce noise and distractions in public spaces. More importantly, he believes we should engage in skilled activities to connect with the world in a more meaningful way. He mentions cooks, ice hockey players, and motorcycle racers as examples of people who deal with real materials and situations. No digital experience can replace the feeling of a hockey puck on ice or the gravel under a motorcycle tire. These activities require good judgment and the ability to interact with others.
Crawford argues that when we engage with the real world, we see that manufactured experiences are not as fulfilling. This does not mean everyone should become a chef, but it is important to use our judgment in daily life. There are many benefits to this approach, including professional satisfaction. Constantly fighting distractions can be tiring and make it harder to focus on what is important. In contrast, paying attention to one thing can help us pay attention to other things as well."
13,C1,"
""What do you do for a living?"" This is a common question in small talk, and we often define ourselves by our jobs. However, if you are a philosopher like me, it can be a bit more complicated. Saying you are a philosopher can sound pretentious. It feels better to say you study or teach philosophy, but calling yourself a philosopher might make people think you claim to know some special truth. This idea is not true; philosophers are just like everyone else, but this stereotype makes me think twice before answering.
One reason this stereotype exists is that philosophers are seen as people who judge others and value intellectual life. The Greek philosopher Aristotle believed that a life of contemplation, or philosophical thinking, was the best kind of life. While few modern philosophers agree with this, philosophy is still often linked to deep thinking. Another Greek philosopher, Socrates, said that ""the unexamined life is not worth living,"" meaning that simply accepting what society says can lead to an unsatisfying life. Our ability to reflect on the world helps us take control of our lives and make our own choices.
However, living an examined life does not mean you have to read many philosophical books or dedicate your life to deep thinking. It simply means paying more attention to the everyday experiences that shape our lives to make sure they are meaningful. You don’t have to be a wise person living away from society to evaluate life fairly. In fact, examining life can be very practical and should involve sharing knowledge, as it is important for a good life.
Another reason people misunderstand philosophers is that academic philosophy has become more distant from real-life experiences, especially for those without formal training. This is not entirely the philosophers' fault; university funding and evaluations are often linked to expensive academic journals. Because of this, philosophers have fewer chances to share their ideas with anyone outside their small group of experts. For many people, philosophy can seem disconnected from reality. While some philosophers used to challenge this view, many have accepted it. The academic environment has created a separation, and as philosophy has focused on specific technical debates, it has become harder for those outside the field to understand.
I sometimes call myself an ""ethicist"" because most of my work is in ethics, which is the part of philosophy that looks at human actions. Recently, I have felt that this title does not fully describe my work, as ethics is often linked to formal rules and laws. There is a new development in philosophical thinking: applied ethics, which examines the fairness of specific social practices. The role of an ethicist today is to decide if certain actions are ""ethical"" or acceptable. These are important questions that I often explore.
However, philosophy is more than just this. A typical discussion might start by asking if illegally downloading movies is unethical (it is) and then move on to questions about responsibility, our views on art, and the effects of consumerism. In this way, philosophy can help people think more deeply about the actions and behaviors that shape their lives. Sometimes we might confirm what we already know; other times, we may find that our beliefs are hard to defend. Either way, by examining these ideas, we can benefit everyone."
14,C1,"Food lovers, chefs, and anyone who enjoys food might think that thinking deeply about what and how we eat can make eating more enjoyable. However, throughout history, discussions about food in philosophy have often been less important than other philosophical topics. When philosophers talk about eating, they usually use it as a way to discuss something else, like gaining knowledge. Sometimes, there are discussions that seem to be about food, but when we look closer, we see they are actually about different, but related, ideas.
One interesting example is the ancient Greek philosopher Epicurus, who believed in seeking pleasure and avoiding pain in many areas of life. Although he talked about various pleasures, his name has become closely linked to enjoying food and drink. You can see this in the names of restaurants and food websites that use his name, likely hoping to attract customers by connecting to a famous philosopher.
Food is also a social and cultural experience. The food we eat comes from history and is shared with people in our communities. These communities help us by providing dining partners and the systems needed to grow and distribute food. We interact with food more often and in more important ways than with other products, making it a clear topic for philosophical discussion.
Once food is prepared, critics begin to talk and write about it. But why do these critics have such a special role? One philosopher pointed out that tasting food is not a special skill; rather, good food critics are just better at describing flavors. This area of taste has not been studied much in philosophy, as most research has focused on how we see things. This should change.
Another important aspect of food is its beauty. We often describe paintings or music as beautiful, but we rarely talk about the taste of food in the same way. Some philosophers believe that, with the growth of modern cooking, food deserves to be discussed in terms of beauty, just like art or poetry. However, food is consumed when we enjoy it, unlike art, which remains after we experience it. Because of this, it is incorrect to think of food as something that can be considered beautiful in the same way as lasting art.
There are also many ethical questions related to food. For example, we can ask what we should eat: organic, free-range, locally grown, vegetarian, or non-genetically modified foods? The answers often reflect our ethical beliefs, and philosophers will always question why we choose what we do. This kind of academic discussion about food is important.
Another topic is the difference between cooking at home and in restaurants. Home cooks have a special responsibility to their guests because of their personal relationships. In a home kitchen, everyone comes together to share food and companionship. In contrast, professional cooks have responsibilities to their employers and the food itself. Their kitchens are not open to everyone, and their relationships are more about colleagues than friends.
A recent essay called ""Diplomacy of the Dish"" looks at how food and dining can help connect different cultures. This happens in two ways: first, we can learn to appreciate others by enjoying their food, and second, we can build personal connections with people from other cultures by sharing meals. This practice has a long history in international relations. The essay includes many interesting examples and stories that make this part of food philosophy engaging for readers."
15,C1,"Rob Daviau, from the US, creates ‘legacy’ board games. He felt that the traditional board games he played as a child were not exciting or challenging enough, so he began to think about how board games could change. He wondered if they could have a story and if choices made in one game could affect future games. He took a classic game called Risk and made a new version called Risk Legacy. In this game, decisions made during play have lasting effects. Players might have to tear up cards, mark the board, or open packets with new rules at important moments. The game is played over several sessions, and players’ past actions become part of the game’s story. Daviau said, “You could point to the board and say: ‘Right here! You did this!’”
Later, he was asked to work on a legacy version of Pandemic, a very popular cooperative board game where players try to cure diseases. His next project was a game called SeaFall. While Pandemic Legacy was very well received, SeaFall was seen as a true test of the legacy format because it was the first game that did not follow an earlier version. Set in the age of sailing (16th to mid-19th century), players become sailors exploring a new world. Designers of legacy board games must think about all possible player choices to keep the story from falling apart. To do this, they use testers to see how the games will play out. Jaime Barriga, a tester for SeaFall, said, “It takes a lot of time. The first few games might go well, but then after a few more games, it can start to break. Then you have to go back and fix everything.”
Legacy board games were not expected to become very popular. When Daviau was developing the idea, he thought it would appeal to only a small group of people. He said, “I thought it was different and pretty good, but it’s weird and breaks many rules. I thought I would be known as the guy who did this strange project.” However, many players, like Russell Chapman, loved the idea. He believes it is one of the biggest improvements in game design in years. He said, “It’s a new level of commitment, intensity, and excitement. There’s nothing more thrilling for a board gamer than learning a new game, and you get that constantly with legacy games.”
Another fan, Ben Hogg, said that the excitement of an unfolding story was more important than worries about how long the game would last. He mentioned that people usually don’t watch the same movie twice in a row. “You’re buying into an experience,” he said. “It adds a storytelling aspect similar to video games.” The legacy format is inspired not only by video games but also by the popularity of episodic TV shows. While in college, Daviau wanted to be a television writer but later moved into advertising and game design. Still, he loved telling stories. Matt Leacock, the creator of Pandemic, compared the legacy design process to writing a novel. He said, “You need to know how you want it to end and have a good idea of where to start, but you also need more than just an outline.”
Daviau feels proud of his work but is curious to see how others will use the idea. He also thinks that gamers will soon want something new. Colby Dauch, the studio manager for the publisher of SeaFall, is not so sure. He believes the legacy format has changed how people think about board games."
16,C1,"One night not long ago, an octopus named Inky escaped from his tank at New Zealand’s National Aquarium. He crawled across the floor and squeezed into a narrow drain that led to the Pacific Ocean. This story was exciting and was shared widely online. Many people enjoy stories of animals escaping, like those of rats and llamas, because they like to think of animals as being similar to humans. Octopuses are especially interesting because they are very intelligent and look very different from us. They can open jars, recognize faces, use coconut shells for protection, and even play in complex ways.
Some people think that comparing animals to humans is not scientific. However, Dr. Frans de Waal, who studies primates like gorillas and chimpanzees, believes that it is actually unscientific to ignore the human-like qualities of animals. He calls this idea ""anthropodenial."" After studying many years of research on animal intelligence, he shows that animals can do many things that we thought only humans could do, like remembering the past and future, showing empathy, and understanding what others want.
In the past, people believed that animals were smart enough to be put on trial for crimes. Even in the 1800s, many scientists looked for similarities between human and animal intelligence. Charles Darwin, who changed how we see our place in the world, said that the difference between human and animal minds is one of degree, not kind. However, in the 20th century, the focus shifted to seeing animals as simple machines that only respond to rewards and punishments. This view made it hard for people to believe that animals have complex thoughts and feelings.
Dr. de Waal thinks we are starting to change our views about animal intelligence. He believes that we are beginning to see that animal thinking is similar to human thinking, even if they are not the same. He notes that there is a lot of new information about animal intelligence available online. The best tests for animal intelligence consider the unique skills of each species. For example, squirrels may not do well on human memory tests, but they can remember where they hid their nuts.
In her book, The Soul of an Octopus, naturalist Sy Montgomery suggests that if an octopus were to test human intelligence, it might look at how well we can change the colors of our skin. If we failed, the octopus might think we are not very smart. Dr. de Waal is not completely convinced that Inky's escape was a happy ending. He believes it is unlikely that Inky knew how to find the drain to the ocean. However, he understands that stories like Inky's help people appreciate animal intelligence.
He once conducted an experiment to see if capuchin monkeys can feel envy. When some monkeys received cucumbers (which they like) and others received grapes (which they like even more), the monkeys with cucumbers became upset when they saw their friends getting grapes. This study was published in a scientific journal, but what really made people believe the findings was a short video of the experiment released ten years later. This shows how our understanding of animal minds is still developing."
17,C1,"
Robotics, once just a part of science fiction, is now becoming a major change in technology, similar to what happened during industrialization. Robots have been used in car manufacturing and other industries for many years, but experts believe we are about to see a big increase in their use in many areas. Many people think that robots will take over most jobs done by humans in the next 50 years. However, about 80% of people also believe their current jobs will still exist in the same way during that time. This shows a common belief that our jobs are safe, but they are not. Every industry will be affected by robots in the coming years.
For example, an Australian company called Fastbrick Robotics has created a robot named Hadrian X that can lay 1,000 bricks in one hour. This job would take two human workers a whole day or more. Another example is a robot called Tally, developed by a startup in San Francisco. Tally moves around supermarkets to check that products are in stock and correctly priced, rather than cleaning the aisles.
Supporters of robotic automation say that robots cannot yet fix or program themselves, which could lead to new, skilled jobs for technicians and programmers. However, critics warn that we should not forget the importance of human skills in the workplace. They believe society is not ready for the changes that will come from reducing human interaction.
Dr. Jing Bing Zhang, an expert in robotics, studies how robots are changing the workforce. His recent report predicts that in two years, nearly one-third of robots will be smarter and able to work safely with humans. In three years, many top companies will have a chief robotics officer, and some governments will create laws about robots. In five years, salaries in the robotics field will rise by at least 60%, but many jobs will remain unfilled because there aren’t enough skilled workers.
Zhang says that automation will affect lower-skilled workers, which is unfortunate. He believes these workers should not wait for the government to protect their jobs but should find ways to retrain themselves. People can no longer expect to do the same job for their entire lives.
At the same time, advances in technology will lead to new types of robots for consumers, such as robots that can walk and interact with us in our homes. This presents a great opportunity for companies, but it also brings challenges, like the need for new rules to keep us safe and protect our privacy.
With many jobs at risk and a global employment crisis approaching, it is important to focus on education to prepare for the future workforce that will include robots. Developed countries need more graduates in science, technology, engineering, and math (STEM) to stay competitive."
18,C1,"George Mallory, a famous mountaineer, is said to have answered a reporter's question about why he wanted to climb Mount Everest with, ""Because it’s there."" This response reflects a common curiosity: why do people take part in such dangerous activities? Mallory's answer might inspire some to pursue their dreams and face the unknown. It also suggests that the main reason for climbing could simply be for adventure and enjoyment.
In 1967, Bolivian writer Tejada-Flores wrote an important essay called ""Games that Climbers Play."" He identified seven different climbing activities, or ""games,"" ranging from climbing on boulders to scaling high mountains, each with its own rules. He argued that the way climbers approach a climb—like how much equipment they use—depends on the rules of that specific game. Over the years, this idea has become popular in climbing discussions in the West, appearing in climbing magazines and casual conversations.
Many climbers love the feeling of freedom that climbing gives them. However, climbing can also be very limiting. For example, being stuck in a tent during a storm is not what most people think of as freedom. This creates a paradox in climbing. Yet, some argue that having fewer choices can simplify a climber's life, which can also feel like a form of freedom.
When asked ""Why climb?"", US rock climber Joe Fitschen suggests a different question: ""Why do people climb?"" He believes that climbing is part of our nature; humans are built to take on challenges and test their limits, even if it involves risks. Therefore, the joy we find in climbing is more about our biology than logical reasoning.
US academic Brian Treanor also contributed to this discussion. He argues that climbing can help develop important qualities like courage, humility, and respect for nature. While he acknowledges that not all climbers show these qualities, he believes they are especially important in today's risk-averse society. Climbing, then, can have practical benefits by helping people develop traits that allow them to thrive in everyday life.
Another interesting idea is that climbers who do not depend on others or technology must be fully committed to succeed. Expert climbers Ebert and Robinson stirred some debate by claiming that climbs done independently are more challenging and deserve more recognition than those completed with large teams or with the help of tools like bottled oxygen. However, this view may encourage some climbers to take unnecessary risks.
We can also explore climbing outside of the Western perspective. Many climbers report experiences of being fully present, or ""in the zone."" The physical effort of climbing, the focused meditation during a climb, and the intuitive problem-solving involved are all key aspects of the activity. Some argue that climbing techniques are similar to those used in Zen philosophy, which aims to achieve a state of perfect peace. This offers another perspective on why people are drawn to climbing."
79,C2,"One of the most interesting changes in education in the UK and around the world in recent years is the push for new students to get involved in research as early as possible. This change shows that research is not just for famous scholars at old universities or scientists who are discovering new things, but it is also a natural way to build knowledge and skills. Research skills will help you not only in your studies but also in your future job, as they teach you how to think about the world and how to approach your work.
As a student, you contribute to knowledge. You do not just learn and repeat what others say; you create new knowledge. Creating knowledge starts with asking questions instead of just accepting things as they are. You might ask: Why? How? When? What does this mean? What if things were different? How does it work in this situation? What should we think about the facts, opinions, or beliefs we are given? Why is this important? These questions are at the heart of what we call research.
Research can be seen as a range of activities. On one end, there is complex, groundbreaking research done by highly trained experts, which leads to significant changes and new knowledge. However, research can also start from simple questions and everyday inquiries. This type of research involves careful work, asking thoughtful questions about issues, following leads, and making practical suggestions.
Most students have always been researchers in some way. You have likely done some research for school projects or answered questions at work since you started. You have asked questions that led you to investigate topics that interest you. You have also developed research skills when planning a holiday, growing plants, fixing things at home, training your dog, finding the right music system, or shopping online.
In college and higher education, you are expected to have a curious mind, identify problems and questions, critically explore and evaluate information, and create your own responses and knowledge. Some students may find this challenging because, in some cultures, knowledge is seen as fixed, and learning comes from listening to teachers and texts. It may feel disrespectful to question established knowledge or authority, and you might think you should be told what is important to learn.
However, in higher education in the UK, US, much of Europe, and Australia, questioning established knowledge and authorities is encouraged. This process of inquiry and knowledge creation can seem overwhelming. Critical thinking is especially important in research. The research done by others is valuable for students, academics, and future jobs, but we need to engage with it, think critically, and test it. We should not just accept everything we read as true; instead, we must analyze it to see if it is logical and supported by evidence. We should avoid blindly trusting the facts and information we receive from others."
80,C2,"Cities have always been important places for ideas and discussions. In the 18th century, coffee houses in London were popular spots where people talked about science and politics. Similarly, in modern Paris, cafés on the Left Bank were where artists like Pablo Picasso shared their thoughts on art. However, living in a city can be challenging. The same cafés that encouraged conversation also helped spread diseases like cholera, and even Picasso eventually moved to the countryside.
Today, cities are known for their creativity, but they can also feel overwhelming and unnatural. Scientists are now studying how city life affects our brains, and the findings are concerning. While it has been known that city life can be tiring, new research shows that living in cities can actually make us think less clearly. One major reason for this is the lack of nature in urban environments. Studies have shown that hospital patients heal faster when they can see trees from their windows. Even small views of nature can help our brains relax and improve our thinking.
As more people live in cities than ever before, we are surrounded by concrete and many strangers instead of open spaces. This crowded environment can have serious effects on our mental and physical health and change how we think. For example, when walking down a busy street, our brains must pay attention to many things: other people, traffic, and confusing street layouts. These everyday tasks can drain our mental energy because cities are full of distractions that require us to constantly focus and redirect our attention.
This idea is explained by attention restoration theory, developed by psychologist Stephen Kaplan. He suggested that being in nature can help restore our ability to focus. Natural environments have things that naturally attract our attention without causing stress, unlike loud noises like police sirens. In nature, our minds can relax and recharge.
Long before scientists studied this, philosophers and landscape designers warned about the negative effects of city life and sought ways to bring nature into urban areas. Parks, like Central Park in New York, provide a break from city life and can quickly improve brain function. While many people look for ways to boost their mental performance, such as energy drinks or office redesigns, simply taking a walk in nature may be the most effective solution.
Given the many mental challenges of city living, we must ask: Why do cities keep growing? Even in our digital age, cities remain centers of creativity. Research from the Santa Fe Institute shows that the same crowded features of cities that can hurt our attention and memory also promote innovation. The close social interactions in cities are key to their creative energy. Just as the crowded streets of 18th-century London led to new ideas, the busy environment of 21st-century Cambridge, Massachusetts, supports its success in technology.
The challenge is to find ways to reduce the negative effects of city life while keeping its unique advantages. After all, there will always be times when people feel tired of nature and long for the excitement of the city."
81,C2,"**Where Should We Look for the Mind?**
This question might seem strange: we usually think that thinking happens inside our heads. Today, we have advanced brain imaging techniques that support this idea. However, I believe we should not limit the study of the mind to just the brain or the body. There is a lot of evidence, from ancient times to now, showing that objects and tools also play a role in how we think. 
From an archaeological perspective, items like stone tools, jewelry, carvings, clay tokens, and writing systems have been important in human evolution and the development of our minds. Therefore, what is outside our heads might also be part of our minds. 
It is easy to connect the mind with the brain because most of what we know about the mind comes from studying people without the objects they usually use. This is practical for neuroscientists who need to use brain scans. However, this approach often overlooks the fact that much of our thinking happens with the help of things around us. 
I do not want to deny the importance of the brain in thinking, but I want to emphasize that the mind is more than just the brain. It is useful to explore the idea that human intelligence extends beyond our bodies into culture and the material world. This is where my new theory, called Material Engagement Theory (MET), comes in. 
MET looks at how objects can become extensions of our minds and bodies. For example, when we make numbers from clay or use a stone to create a tool, we are engaging with the material world. This theory also examines how these interactions have changed over time and what those changes mean for our thinking. 
Consider a blind person using a stick. Where does this person’s self begin? The connection between the blind person and the stick shows how minds and objects can be linked. The stick helps the blind person to feel and understand their surroundings, and the brain treats the stick as part of the body. 
This example reminds us that human intelligence can adapt and change by using new technologies. I see the human mind as a work in progress, always evolving. Regardless of what form the “stick” has taken throughout history—whether it’s a simple stone tool or a modern smartphone—its main purpose is to help us explore and understand our environment. 
For humans, tools are not just for survival; they satisfy our curiosity. This unique ability to engage with material culture explains why humans create things and how those things shape our minds. I call this idea metaplasticity, meaning our minds are flexible and change as we interact with the world around us. 
I want to emphasize the importance of material objects in understanding the mind. MET provides a new perspective on how different types of material culture, from ancient tools to modern devices, have influenced who we are and how we think. While the idea of technology changing our minds may sound futuristic, humans have been using such tools since the beginning of our existence."
82,C1,"Few inventions have changed our way of understanding the world as much as photography. This is especially true in the United States, where photography quickly became part of everyday life, from practical uses to art and entertainment. Historians believe that photography is one of the greatest contributions of the US to the visual arts. This is a big statement, but it shows how important photography has become in American culture. To understand its impact, we need to look at its beginnings in the mid-19th century.
Why was photography so attractive? First, it was a mechanical process that matched the growing interest in technology. Just like steam power and railroads made communication and travel easier, photography brought the wonders of the world into people's homes in a very immediate way. Second, the camera became a popular tool for showing one's identity in a country where personal and national identities were always changing. Third, the camera helped families create a record of their lives, even if that record was somewhat idealized. Finally, the realistic nature of photographs matched the trend towards realism in American art.
Every photograph draws attention to something specific, showing what the photographer wants us to see. Because photographs are made by machines, they are records of events, people, and things, which gives them a sense of objectivity. However, since each photograph is taken by a person, it also carries a personal point of view. We might think we understand a photograph because we recognize its subject, which is why photography is often called a ""universal language."" But the meaning of a photograph is more complicated than that. 
No image speaks for itself; it is always presented with some context that affects how we understand it, like a caption in a newspaper or its placement in a gallery. To understand a photograph historically, we need to consider its purpose and the culture it comes from: Why did the photographer take the picture? How did people first see it? The same image can have different meanings depending on where and when it is viewed.
For a long time, the importance of the camera to artists was not well known, but today it is clear. In recent decades, artists like Andy Warhol have used photographs in their work in many ways. In summary, photography has become an essential part of art, working alongside painting and shaping our ideas of representation before the camera was invented."
83,C1,"In 1890, William James, an American philosopher and one of the founders of modern psychology, defined psychology as the ""science of mental life."" This definition is still relevant today. Everyone has a mental life, which means we have some understanding of what this term means. Although we can study mental life in animals like rats and monkeys, it is still a complex idea. James focused on human psychology, which he believed included basic elements: thoughts, feelings, the physical world, and a way of knowing about these elements. Our knowledge is mostly personal and comes from our own experiences, which may or may not align with scientific facts.
Because of this, we often make judgments about psychological issues based on our own experiences. We act like amateur psychologists when we share opinions on complicated topics, such as whether brainwashing works or why people behave in certain ways, like feeling insulted or unhappy. Problems can arise when two people understand things differently. Formal psychology aims to provide methods to determine which explanations are most likely correct in different situations. Psychologists help us separate subjective thoughts, which can be biased, from objective facts.
Psychology, as defined by William James, focuses on the mind and brain. While psychologists study the brain, they do not fully understand how it works in relation to our feelings, hopes, and behaviors. It is often difficult to study the brain directly, so psychologists learn more by observing behavior and forming hypotheses about what happens inside us. A challenge in psychology is that scientific facts should be objective, but the mind's workings are not easily observable. We can only infer them from behavior, similar to solving a crossword puzzle by interpreting clues.
Psychology aims to describe, understand, predict, and control the processes it studies. When these goals are met, psychology can help us understand our experiences and apply findings to real life. Psychological research has been useful in many areas, such as improving reading methods for children, designing safer machines, and helping people express their feelings when they are emotionally distressed.
Although psychological questions have been discussed for centuries, scientific investigation of these questions has only occurred in the last 150 years. Early psychologists used introspection, or self-reflection, to find answers. They aimed to identify mental structures, which is still important today, but introspection has limitations. As Sir Francis Galton noted, it only allows us to see a small part of what the brain does automatically. William James compared trying to understand the mind through introspection to quickly turning up the gas to see how darkness looks. Today, psychologists prefer to base their theories on careful observations of others' behavior rather than on their own experiences."
84,C1,"In a storehouse in a regular business park in Michigan, USA, there is a unique collection known as the Museum of Failed Products. This museum, owned by a company called GfK, displays products that were taken off the market because very few people wanted to buy them. The owner, Carol Sherry, believes that each product has its own sad story related to the designers, marketers, and salespeople involved.
What is surprising is that this museum exists as a successful business. You might think that any good manufacturer would keep a collection of their failed products, but many executives who visit the museum show that this is not common. They often avoid thinking about their failures and do not keep samples of their unsuccessful products. The museum was started by Robert McMath, who originally wanted to create a library of consumer products, not just failed ones. Since the 1960s, he began collecting every new item he could find. He did not realize that most products fail, which led to his collection being mostly made up of unsuccessful items.
Today’s culture of optimism may contribute to the products in the museum. Each product likely went through meetings where people did not see that it would fail. Even if they did, marketers might invest more money into a failing product to try to save face. Little effort is made to understand why these products failed, and people involved often choose not to talk about it.
This focus on positivity is also common in the self-help industry. One popular idea is ""positive visualization,"" which suggests that if you imagine good outcomes, they are more likely to happen. Research by neuroscientist Tali Sharot shows that our brains may be wired to see the chances of success as higher than they really are. People who are mentally healthy often have a more optimistic view of their ability to influence events compared to those who are depressed.
However, how effective are these positive thoughts about the future? Psychologist Gabriele Oettingen and her team have studied this question. Their findings are surprising: thinking too much about how well things could go can actually reduce people's motivation to reach their goals. For example, people who were encouraged to imagine having a very successful week at work ended up achieving less.
Psychologist Carol Dweck explains that our experiences with failure depend on our beliefs about ability. People with a ""fixed mindset"" believe that ability is something you are born with, while those with a ""growth mindset"" think that ability can develop through effort and challenges. Fixed-mindset individuals find failure very frightening because it shows they did not meet expectations. For instance, a sports star who thinks of himself as naturally talented may not practice enough to succeed. On the other hand, growth-mindset individuals see failure as a sign that they are pushing their limits. Dweck compares this to weight training, where muscles grow stronger by being challenged. Overall, having a growth mindset is a healthier way to approach life, regardless of whether it leads to success."
85,C1,"Everyone knows that sports teams have an advantage when they play at home. But why is that? Many people think they understand, but professional sports are changing quickly, and what we used to believe is now being questioned. Two main factors are challenging the idea of home advantage: science and money. Sports scientists want to find out what helps players perform their best, and they have come up with many theories about home advantage. At the same time, those who invest money in sports want to know why home advantage is still important. If players are paid well, shouldn’t they feel comfortable playing anywhere?
This raises a question for fans. Would it matter if a team like Manchester United played some of their home games far away, like in the Far East, to reach more fans? It would matter to British fans who believe their team needs their support and loyalty. Fans often confuse the order of events; they think that when they cheer, it leads to a goal. They forget the many times they cheered but no goal was scored.
However, there is one common belief among fans that science seems to support. Home fans not only cheer for their team but also direct a lot of their noise at the referee. In a well-known study, referees watched a match with crowd noise and another version without sound. The referees who heard the crowd were less likely to call fouls against the home team. This suggests that referees try to avoid making calls that would upset the home crowd, rather than just doing what the crowd wants.
Recent studies show that home advantage has decreased in all major sports, but not as much as expected. For example, in the early years of the Football League in the 1890s, home teams won about 70% of the points, while today they win about 60%. One reason for this change could be travel. In the past, traveling was difficult, but now players travel in comfort, and if their hotel isn’t good enough, the club will find them a better one. Stadiums also look more similar now, losing some of their unique character.
Despite these changes, the reasons for home advantage are still unclear. Different sports have different levels of home advantage. Basketball has the highest home advantage, followed by football, while baseball has the least. This might be because baseball focuses more on individual matchups rather than teamwork. In team sports like basketball, players rely on each other, and playing at home boosts their confidence.
Another interesting point is that players’ testosterone levels are higher before home games, which might relate to a natural instinct to protect their home ground. In recent Rugby World Cup games, underdog teams won at home against stronger opponents, showing the power of home advantage. As one referee said, “It’s the law of the jungle out there.”"
86,C1,"""The more I practice, the luckier I get,"" said golfer Gary Player about fifty years ago. This saying is popular among many dedicated athletes and their coaches. The reason is clear. First, there is some debate about who originally said it, as many golfers from that time have been suggested. Second, the meaning is not straightforward. Player did not mean ""lucky"" in a real sense; he was being ironic. His true message was: ""The more I practice, the better I become."" 
However, there is a limit to this idea. The question of where that limit is, or if it even exists, relates to the nature versus nurture debate. This debate discusses whether talent is something we are born with or something we develop through practice. It is not really a balanced debate; nurture has clearly won. The idea that practice is more important than natural talent in sports, music, business, and other areas comes from psychologist Anders Ericsson. He is known for the theory that anyone can become an expert in any field with 10,000 hours of practice. This theory has inspired many popular books, including Malcolm Gladwell's ""Outliers,"" as well as ""Talent Is Overrated"" and ""Bounce: The Myth of Talent and the Power of Practice."" All these books generally agree that practice is the most important factor, suggesting that 10,000 hours of practice is enough to make someone an expert.
However, as Epstein discusses in his interesting book, this idea is often not true. He points out a major problem in much of the research on what makes people excellent: it usually focuses only on successful individuals. He asks if there are better ways to understand why some people have certain abilities while others do not, and how to separate natural talent from factors like environment, support, opportunity, and determination. 
Epstein travels around the world, from Kenya to Sweden and the Caribbean to the Arctic, to find answers. In Alaska, he discovers that the drive and desire of sled dogs in a tough race may be influenced by their genes, suggesting a link between genetics and traits we think are voluntary. This raises the question: can determination be a choice, or is it partly genetic, as seen in the case of the huskies? 
This complexity is important. Epstein makes a distinction between ""hardware"" (nature) and ""software"" (nurture), agreeing that both are necessary for elite athletes. He does not deny the importance of training or environment. For example, he suggests that if Usain Bolt had grown up in the US, he might have become a good basketball player instead of the fastest man in history. However, he also looks at cases where genetics play a significant role that cannot be ignored. He also discusses race and gender, asking, ""If only practice matters, why do we separate men and women in sports?"" Sometimes, the best questions are the most obvious ones."
87,C1,"A recent international report shows that many children in rich, industrialized countries feel lonely and unhappy. Jay Griffiths asks a simple question: why are today’s children so unhappy? In her new book, Kith, she explains that these children spend too much time indoors, stuck in front of screens like televisions and computers, and have lost touch with nature – the woods, mountains, rivers, and streams. She believes this is the main problem.
A follow-up study interviewed some children to understand their feelings better. The children did not express a strong desire for new technology; instead, many said they would be happier if they could spend more time outside. Older generations often remember their own childhoods fondly, recalling how they explored nature, swam in ponds, and built forts. They believe this was healthier than the protected lives of today’s children. However, they often forget that they created these protective environments for their kids, avoiding risks like building forts in old sheds.
Griffiths’ book includes strong arguments about the fear parents have regarding outdoor dangers and how this fear benefits the toy and gadget industry, which sells products to keep children entertained at home. She also criticizes trends like giving medication to restless children or requiring safety goggles for playground games. While it’s unclear how common these rules are, Griffiths expresses her frustration passionately.
However, Griffiths sometimes goes too far with her ideas and ignores important counterarguments. At one point, she even compares the treatment of children today to a form of racism, which seems extreme. She tends to have a romantic view of children, believing they are naturally good and creative, but this view may not reflect reality, especially when dealing with energetic groups of young kids.
One of the main ideas in her book is that children should have the freedom to explore and take risks. Griffiths argues that children need to experience small accidents to learn how to handle bigger dangers later. However, she does not explain how to create these safe accidents. Additionally, not all children may want the adventurous lifestyle she admires. What about those who are shy, prefer staying at home, or are more cautious? Perhaps the real issue is that society often tries to fit all children into the same mold, whether by keeping them too safe or pushing them too hard to explore."
88,C1,"I am a research bio-psychologist with a PhD, which means I have studied a lot. I am good at solving problems in my work and life, but this skill does not come only from my education. Many life problems cannot be solved with complicated formulas or memorized answers from school. They need judgment, wisdom, and creativity that come from real-life experiences. For children, these experiences often happen through play. 
My recent research focuses on how important play is for children's development. All young mammals, including humans, play, and those who have the most to learn tend to play the most. Carnivores, like lions, play more than herbivores, like cows, because hunting is harder to learn than grazing. Primates, like monkeys, play more than other mammals because they rely more on learning than on instincts. Children, who have a lot to learn, play more than any other young primates when they are allowed to.
Play is a natural way for both adults and young mammals to learn. The most important skills children need to live happy and productive lives cannot be taught in school. These skills are learned and practiced through play. They include thinking creatively, getting along with others, cooperating, and controlling emotions. Creativity is essential for success in today’s economy. We no longer need people who just follow instructions or do simple calculations. We need people who can ask new questions and solve new problems. If we can encourage creative thinkers, we will have a strong workforce.
However, we cannot teach creativity directly. School can sometimes take away creativity by focusing on fixed questions and answers instead of encouraging children to ask their own questions. It is also very important for children to learn how to get along with others, care for them, and cooperate. Children naturally want to play with each other, and through play, they learn social skills, fairness, and morality, which are important for their future.
Play is voluntary, meaning players can choose to stop whenever they want. If they cannot quit, it is not really play. Players understand that to keep the game fun, they must make sure everyone is happy. This ability to quit makes play a very democratic activity.
Unfortunately, school has become more stressful. There are fewer breaks, more homework, and greater pressure for high grades. Outside of school, organized sports have taken the place of spontaneous games. Supervised playdates have replaced unsupervised neighborhood play, and adults often feel they need to step in instead of letting children solve their own problems. These changes have happened slowly but have had a big impact over time. They are caused by various social factors, including parents' fears, warnings from experts about dangers, less connected neighborhoods, and the belief that children learn more from adults than from each other.
Our children do not need more school; they need more play. If we care about our children and future generations, we must change the negative trend of the past fifty years. We need to give childhood back to children. They should be allowed to play and explore freely so they can grow into strong adults ready for an unpredictable future."
89,C1,"In many countries, more young people in their twenties are using social media to find jobs. Websites like Twitter and LinkedIn allow them to connect directly with potential employers, which is much easier than the old way of standing outside an office with a sign saying ""hire me."" However, this increased access also means there is a higher chance of making mistakes.
For example, a young job seeker in the US reached out to a senior marketing executive on LinkedIn. This executive had many important contacts, and the job seeker thought she might help him. But the executive was upset by the request. She felt it was wrong to share her contacts with someone she didn’t know. Instead of just rejecting his request, she sent a harsh and sarcastic message that became very popular online. Many people were shocked by her response, and she might regret how she handled the situation. However, if this incident makes young people think more carefully about using social media for work, it could actually help them.
Social media can be risky for job seekers who don’t know how to use it properly. Many young people are making mistakes. Ironically, social networking sites like Facebook and Twitter have been a big part of young people's social lives for years. When older generations were teenagers, social media was a way to escape from parents and teachers. It was a place to show off and create a different image of themselves. You could have long conversations online and then ignore the same person at school. With the right pictures and songs on Facebook, you could seem like a more interesting person overnight. 
However, using social media for professional networking is very different. For some young people, the line between personal and professional use is not clear. They may still think that being bold and confident online is a good idea. Just because many people liked your posts on Facebook doesn’t mean you can impress employers on LinkedIn. Young people need to understand that the rules of social networking have changed, and they must meet employers’ expectations to succeed in the job market.
One common complaint from employers is that young job seekers are too casual in their messages and come off as arrogant. This reinforces the idea that young people feel entitled. In reality, many young people are struggling to find jobs, which is why they are using social media. This perception of arrogance can hurt their chances, even if they have the skills and motivation to be valuable employees.
So, how should you contact someone on a professional networking site? First, clearly introduce yourself and explain what you can offer them, like doing some research or helping in another way. This approach increases your chances of getting a positive response. Avoid sending generic messages, and keep your tone respectful to leave a good impression. Remember, social media can be a great way to make important connections, but it requires careful use to avoid being ignored."
90,C1,"Anyone who claims they can accurately predict the future of newspapers is either lying or mistaken. The numbers show that newspapers are in trouble. Since 2000, the circulation of most UK national daily newspapers has dropped by 30 to 50 percent. According to the Pew Research Centre in the USA, only 26 percent of Americans now get their news from newspapers, down from 45 percent in 2001. Many people confidently say that printed newspapers will disappear within 15 years. However, history shows that old forms of media often survive. 
For example, in 1835, a journalist in New York claimed that books and theater were finished and that newspapers would become the most important source of information. Yet, theater has survived alongside newspapers, cinema, and television. Radio has also thrived during the television era, and cinema has continued to do well despite the rise of videos and DVDs. Even vinyl records have made a comeback, with online sales increasing by 745 percent since 2008.
Newspapers were once a new form of media themselves, but it took centuries for them to become the main source of news. This change happened in the mid-19th century when new technologies like the steam press, railways, and telegraphs made it possible to produce and distribute news quickly. People began to expect regular updates about the world around them, which was a new idea compared to earlier times when life was seen as more predictable and cyclical.
Before the 19th century, journalism was not a full-time job that could support a living. Even then, many people did not see the need for regular news updates. The fixed schedule and format of newspapers can be limiting. In contrast, online news allows readers to choose when and how they want to access information. Advanced search engines and algorithms help us find news that matches our interests. When major news events happen, online news sources can provide real-time updates. Mistakes can be corrected quickly, and there are no space limits for stories.
However, many news providers seem focused on being the first to report news rather than helping people understand it. This creates confusion, similar to how news was shared in medieval times, where truth often mixed with rumors and misunderstandings. Newspapers have not always done a great job of explaining how the world works. They may be facing extinction, or perhaps, as the internet adds to our feeling of living in a chaotic world, newspapers will find a way to help us gain wisdom and understanding."
91,C1,"If humans were truly comfortable under the moon and stars, we would walk in the dark happily, just like many animals that thrive at night. However, we are daytime creatures, with eyes that are made for sunlight. This fact is deeply rooted in our genes, even if we don’t often think about being daytime beings, just like we don’t think about being primates or mammals. This is important to understand because it explains what we have done to the night. We have changed the night by filling it with artificial light. This control is similar to building a dam on a river. While there are benefits to this, it also leads to problems known as light pollution, which scientists are just starting to study.
Light pollution mainly comes from poor lighting design that allows artificial light to shine into the sky instead of focusing it down where it is needed. Bad lighting brightens the night, changing the light levels that many living things, including us, have adapted to. When artificial light spreads into nature, it affects important life activities like migration, reproduction, and feeding.
For most of human history, the term ‘light pollution’ would not have made sense. Imagine walking towards London on a moonlit night around 1800, when it was the most populated city in the world. Nearly a million people lived there, using candles, torches, and lanterns for light. Only a few homes had gas lighting, and there were no public gaslights in the streets for another seven years. From a distance, you would have been more likely to smell London than to see its faint glow.
We have brightened the night as if it were empty, but that is far from the truth. There are many nocturnal species among mammals. Light is a strong force in nature, attracting many species to it. Scientists have observed that songbirds and seabirds can be ‘captured’ by bright lights, circling around them until they fall. Birds that migrate at night often crash into brightly lit tall buildings, and young birds on their first journey are especially affected. Some birds, like blackbirds and nightingales, sing at odd hours because of artificial light.
It was once believed that light pollution only bothered astronomers, who need to see the night sky clearly. While most of us may not need a perfect view of the stars for our work, we still need darkness. Rejecting darkness is pointless. It is just as important for our health as light is. Changing our natural sleep and wake cycles can lead to health problems. The regular pattern of waking and sleeping is a biological reflection of the natural light changes on Earth. These patterns are so essential to our existence that disrupting them is like changing our center of balance.
In the end, humans are just as affected by light pollution as frogs living near a bright highway. By living in our own artificial light, we have disconnected ourselves from our natural and cultural roots – the light of the stars and the natural rhythms of day and night. Light pollution makes us forget our true place in the universe and the vastness of our existence, which is best understood against the backdrop of a dark night filled with the Milky Way above us."
92,C1,"The founder of a large international company recently said that his business will stop tracking how much paid holiday time employees take. This decision seems to be inspired by a similar policy from a smaller internet company. The founder mentioned that he got the idea from a cheerful email from his daughter, which many newspapers have shared. However, this way of announcing the change feels like an attempt to make a serious policy seem friendlier.
But is this idea practical? The internet company has 2,000 employees and offers one service, while the multinational corporation has 50,000 employees and many different services, such as finance, transport, and healthcare. The idea of ""take as much time off as you want"" might work better in a smaller company where employees know each other's workloads. In a big company, it is hard to be sure that taking time off won’t hurt the business or their careers.
The founder said employees can take as much leave as they want, as long as they feel completely sure that their team is up to date on all projects and that their absence won’t harm the business. But is it really possible to be that sure? No matter how much work someone finishes before a holiday, there will always be more work waiting when they return. This means that employees might feel guilty about taking time off, which can lead to stress. If workers don’t take enough leave, it could lower their productivity over time.
There may also be pressure from coworkers and office gossip about who is taking time off and for how long. In many companies, especially in the corporate world, there is a culture of working late, which could lead to a situation where employees feel they cannot take holidays, especially if they want to get promoted. If the security and rights that come with regular leave are taken away, employees might feel they cannot take the time off they need, fearing they will seem lazy.
This policy could create a situation where workers feel stuck and do not take their entitled leave. They might still use their legal rights as a guideline, making the new policy pointless. Modern technology allows employees to receive work messages anytime and anywhere, which makes it hard to separate work from personal time. The internet company started its unlimited leave policy because employees wanted to know how to balance this new way of working with the old time-off rules.
However, having no set working hours can lead to a situation where every hour could be considered working time. Employees might feel they are always being watched by their employer, which can lead to stress and unhealthy self-discipline. Employment laws exist for a reason: workers have the right to a minimum amount of paid leave because rest is important for their mental and physical health. The benefits of unlimited leave, like better morale and creativity, can happen without sacrificing worker well-being. Therefore, I am not convinced that allowing employees to take as much holiday as they want is truly the goal or will be the result of this policy."
93,C1,"Journal-based peer review is the process where experts in a field evaluate a scientific research paper before it is published. This process is seen as a way to ensure the quality of research. It is believed to help prevent the publication of flawed or nonsensical papers, and scientists often mention it positively when speaking to the media or the public. However, reviewing a paper can delay its publication by up to a year. Is this delay worth it to ensure the trustworthiness of published research? The answer is both yes and no. 
The current state of scientific publishing is changing. While I still believe that all papers should go through some form of review before being published, I see that things are shifting. The increasing use of preprints—drafts of papers shared online without peer review—plays an important role in this change. Preprints allow researchers to quickly share new findings so that others can read, critique, and build on them. 
Publishing in journals has become more about gaining recognition and advancing careers, which has affected both authors and reviewers. The competition for publication in prestigious journals encourages scientists to produce high-quality work, and these journals do publish excellent research. However, the pressure to publish in top journals can lead to shortcuts, where important data is left out to make the research look better. Reviewers are often more focused on whether a paper is good enough for a specific journal rather than its overall quality. For top journals, the decision can depend on how relevant or newsworthy the research is, rather than just its scientific merit.
These issues are well-known, but many people are hesitant to change the current system. However, as biologist Ron Vale recently suggested, preprints could offer a solution because they do not require a major change from the existing system. Although preprint archives have existed for twenty years, they have not been widely accepted. This slow adoption is partly due to scientists' conservative nature and the misconception that journals will not accept papers that have been posted as preprints. There is also a concern that publishing papers without peer review could lead to the spread of poor-quality research, but this has not happened so far.
Preprints may not be peer-reviewed, but authors understand that their work will be open to critique and discussion from a global community of reviewers. For example, psychology professor Tanya Elks shared her experience of critiquing a published paper. In the traditional peer review system, the original authors could block critical papers, or if they were not chosen as reviewers, they might complain about misrepresentation. By posting a preprint, both sides could share their views openly, allowing readers to assess the quality of the arguments. 
Preprint archives facilitate informal scientific discussions that used to happen only between individuals. They can also provide a platform for sharing negative results, which are often overlooked by journals focused on new discoveries. Additionally, being on preprint archives increases the chances that papers will be read and cited by others, showing the effectiveness of sharing research this way. By embracing the internet's culture of openness and collaboration, preprints can help shift the focus back to the quality of the research itself, rather than where it is published."
94,C1,"When I ask my literature students what a poem is, they often say it is ""a painting in words."" However, this answer usually does not satisfy them or me. One day, I asked a group of students to choose an object and write two paragraphs about it. The first paragraph was a scientific description, and the second was from the object's point of view, titled ""Poem."" One student wrote about an oxygen mask, saying it may look strange or scary, but it helps people breathe in emergencies. This unusual choice helped the class understand how poetry works in a unique way. 
In school, I found poetry confusing. I thought every poem was a silly puzzle that made it hard to understand feelings. After school, many people pay less attention to poetry. Sometimes, a poem stands out because it is not continuous prose. It challenges you to read it, but often you feel let down by its simplicity or difficulty. Still, you feel good for trying. 
What do we want from poems? We might expect them to express deep feelings, beautiful images, or clever humor. While this is true, we often turn to movies for tears, articles for information, novels for escape, and music for enjoyment. However, one important quality of poetry is its ambiguity, which reflects the uncertainty of everyday life. 
If you look up ""poem"" in an online encyclopedia, it will lead you to ""poetry,"" defined as a form of literary art that uses the beauty and rhythm of language. This definition is fine, but it does not capture the essence of the word. ""Poem"" comes from the Greek word ""poí?ma,"" meaning ""a thing made,"" and a poet is someone who makes things. So, what kind of thing is a poem? 
Some poets compare poems to wild animals, which are unpredictable, or to machines, which are precise. However, these comparisons can break down when examined closely. The real value in trying to define a poem lies in the discussion it creates. Whether you see a poem as a machine or a wild animal, this debate can change how you think about both. It encourages us to see ordinary things in new ways. 
We can easily think of poems as mental objects, especially since song lyrics often stick in our minds. The combination of words and melody has a strong impact, similar to schoolyard rhymes like ""Sticks and stones may break my bones, but words can never hurt me."" But sometimes, words can hurt, just like sticks and stones. 
When you see a poem in a newspaper or magazine, it can have a powerful effect, even though it is just ink on paper like the prose around it. The empty space around the poem could have been used for a longer article or an ad. A poem is created through writing and rewriting, just like an article or story, but it is not usually made to be sold. Publishers promote poetry collections, but few expect them to make money. A poem exists for its own sake, and because of its unique place in a magazine or book, it can still surprise us, even if just for a moment."
