text_id,text_level,text
1,C2,"Some time ago, a website discussed the dangers of public check-ins, which are online updates about where you are. The website's message was clear: while you might think you are simply saying, ""I’m at this place,"" you are also revealing your location to many people, not all of whom you would want to meet. This highlights a growing understanding that sharing everything online can have negative consequences. The internet offers many exciting opportunities to share our lives with a global audience, which can lead to wealth and fame. However, as we share personal stories and photos, we may find ourselves in a crowded and risky environment.
This situation may seem discouraging, but there is hope. The future has been mapped out for us by early internet pioneers. In the beginning, these individuals explored the internet and identified its dangers. They faced job losses, friendship changes, and the challenges of fame long before social media existed. These early bloggers experienced what many of us are now going through. It is important to learn from their experiences, as those who do not learn from history are likely to repeat it.
In January 1994, Justin Hall, a 19-year-old student, started posting on the World Wide Web, which was mainly used by graduate students and scientists. The web was created at CERN, a physics lab in Switzerland, to help researchers share their work. Hall saw a different opportunity: to share his life. He created a detailed online autobiography filled with links, photos, and art. In January 1996, he began a daily blog, attracting many readers who were fascinated by his bold use of this new medium. Hall's approach was open; anyone who crossed his path could be featured on his site, and no topic was off-limits. While some may see this as exhibitionism, there was also a unique beauty to his work.
However, one day, visitors to Hall’s site found it replaced by a single, emotional video titled ""Dark Night."" In it, he shared that he had fallen deeply in love, but when he wrote about it online, he was told, ""either the blog goes, or I do."" Hall felt that sharing his life online made people distrust him. The blog was removed, but the issue remains. Sharing online can be enjoyable, but if you think it will make people want to be around you, you might be disappointed.
In 2002, Heather Armstrong, a young web worker in Los Angeles, had a blog called Dooce. She sometimes wrote about her job at a software company. One day, an anonymous colleague shared her blog's address with the company's vice presidents, including some she had criticized, which led to her losing her job. Researchers studying online behavior have a term for this: the ""online distribution effect,"" which describes how people often feel they can say things online that they would never say in person. However, the internet is not a separate reality where we can speak freely without consequences. Our online lives are connected to our real lives, and ignoring this can lead to serious mistakes.
Armstrong's story had a positive outcome. Although she was upset and stopped blogging for a while, she eventually got married and restarted her blog, focusing on her new family. Today, she is a well-known ""mommy blogger,"" and her writing supports her family. Once a cautionary tale of online mistakes, she has become skilled at sharing her life in a thoughtful way. Armstrong's experience teaches us an important lesson: while the internet allows us to say anything, that doesn’t mean we should."
2,C2,"Some scientists recently started a campaign to warn people about a low-budget film called *What the Bleep Do We Know?* This film mixes documentary and drama to suggest that there is much we do not understand about our universe. While scientists can sometimes be arrogant, even the most confident physicist would not claim to know everything about the cosmos. However, some scientists felt it was necessary to publicly criticize the film, calling it everything from ""atrocious"" to ""very dangerous."" This made me curious to see the movie.
At first, I didn’t understand the controversy. The film featured various scientists making harmless statements about how new discoveries are showing that the universe is stranger than we thought. It was only when the film discussed specific discoveries that I began to understand the concern—like the claim that water molecules can be influenced by human thoughts. I had heard about a Japanese researcher who suggested that the shape of water molecules could change based on the thoughts of people nearby. However, the film only provided images of ice crystals that looked beautiful after being spoken to positively and unattractive after being exposed to negative emotions. Many people find this kind of evidence convincing because it is simple and easy to understand. But scientists typically respond with skepticism, saying, ""Give me a break."" 
I understand their point. The idea that thoughts can affect water is fascinating and suggests new forces in the universe. However, we need solid proof that this effect is real. Beautiful pictures of crystals are not enough. The real issue is that the film's claims were not strange enough. For example, water molecules might have properties linked to a mysterious energy that could be driving the expansion of the universe. This idea is supported by decades of research from laboratories and observatories worldwide.
In reality, discoveries are being made that confirm the film's claim that the universe is much stranger than we ever imagined. Astronomers have discovered that the universe contains an unknown type of matter and is expanding due to a mysterious force called ""dark energy."" Additionally, researchers in other fields are making remarkable discoveries. Neuroscientists have found that our awareness of events is delayed by about half a second, a delay we do not notice because our brains edit it out. Anthropologists believe they have located the origin of modern humans and understand how and why they spread across the globe. Some theorists even suggest connections between life on Earth and the fundamental structure of the universe.
Despite what some may think, science is far from complete. In fact, we seem to be further from knowing everything than ever before. Many natural phenomena may never be fully understood. The ideas of chaos and quantum uncertainty have shown that there are limits to our knowledge. This has led some of the world's top theoretical physicists to work on a ""Theory of Everything,"" which aims to explain all the forces and particles in the universe with a single equation. Ultimately, many of us believe that the universe can be described in one word: incredible."
3,C2,"To put it simply, I find writing novels challenging, while writing short stories is a joy. If writing novels is like planting a forest, then writing short stories is more like planting a garden. Both processes work together to create a beautiful landscape that I cherish. The trees provide shade, and the wind rustles their leaves, which can turn a bright gold. In the garden, flowers bloom, and their colorful petals attract bees and butterflies, showing the gentle change of seasons.
Since I started my career as a fiction writer, I have often switched between writing novels and short stories. My routine is this: after finishing a novel, I want to write short stories; after completing a set of short stories, I feel ready to focus on a novel. I never write short stories while working on a novel, and I don’t write a novel while creating short stories. These two types of writing likely engage different parts of my brain, and it takes time to switch from one to the other.
I began my career with two short novels in 1975, and from 1984 to 1985, I started writing short stories. At that time, I knew little about writing short stories, so it was challenging, but I found the experience memorable. It expanded my fictional world, and readers seemed to enjoy this different side of my writing. One of my early works, “Breaking Waves,” was included in my first short-story collection, Tales from Abroad. This marked my beginning as a short-story writer.
One of the joys of writing short stories is that they don’t take long to complete. Usually, it takes me about a week to shape a short story (though revisions can go on forever). This is different from the long commitment needed for a novel, which can take a year or two. When I write a novel, it can feel like it goes on forever, and I sometimes wonder if I will finish. Therefore, writing short stories provides a necessary change of pace.
Another great thing about short stories is that you can create them from the smallest ideas—like a thought, a word, or an image. Often, it feels like jazz improvisation, where the story leads me where it wants to go. Plus, with short stories, you don’t have to worry too much about failure. If an idea doesn’t work out, you can just accept it and move on. Even famous writers like F. Scott Fitzgerald and Raymond Carver didn’t write a masterpiece every time. This thought comforts me. You can learn from your mistakes and apply that knowledge to your next story.
When I write novels, I try to learn from both my successes and failures in short stories. In this way, short stories serve as a kind of experimental space for me as a novelist. It’s hard to experiment the way I want to within a novel, so without short stories, writing novels would be even more challenging.
My short stories are like soft shadows I have left in the world, gentle reminders of my feelings at the time of writing them. They guide me and bring me joy as a writer, allowing me to share these personal emotions with my readers."
4,C2,"At its core, science is closely related to philosophy, but it also has practical applications, such as curing diseases. Science has improved our lives but also poses risks. It seeks to understand everything from tiny atoms to the vast universe, yet it often struggles to do so. Science influences poets, politicians, philosophers, and even frauds. Its beauty is often recognized only by those who study it deeply, while its dangers are frequently misunderstood. The significance of science has been both exaggerated and underestimated, and its mistakes, as well as those of its practitioners, are often overlooked or unfairly highlighted.
The history of science is marked by constant conflict. Established theories are frequently revised or completely rejected, similar to how new music styles are initially mocked before becoming accepted. This battle between old and new ideas is rarely respectful. Scientists can be driven by jealousy and anger. The history of science is fundamentally about conflict. This book will explore scientific ideas that have transformed not just science, but also many areas of human thought. While science has practical uses, our main focus will be on its ideas—their beauty and the limitations of human understanding.
Science is inherently changeable. There is always a scientist challenging the ideas of another. Most of the time, these changes do not disrupt society. However, sometimes they can lead to significant shifts in our beliefs. For example, in the seventeenth century, science introduced the idea of a mechanical universe, like a giant clock. Three hundred years later, physics began to question basic assumptions, leading us to a complex understanding where our observations can influence the universe, and we struggle to grasp the true meaning of fundamental concepts.
Some people view the instability of scientific theories as a sign that science cannot explain the universe. However, scientific changes usually enhance our ability to understand and predict natural phenomena. For instance, Isaac Newton could explain much more than Aristotle, and Albert Einstein advanced our understanding even further. Science may falter, but it continues to progress. 
At the end of the nineteenth century, many physicists believed there was little left to discover in their field. Then came groundbreaking discoveries like radioactivity, X-rays, the electron, and many more. Biology has also made significant advancements. Today, some claim we are close to a ""theory of everything,"" a complete understanding of the universe's origins and workings. 
Science is not just an academic exercise. Over the last two centuries, we have shifted from merely observing nature to influencing it, sometimes disrupting its balance in ways we do not fully understand. It is crucial for non-scientists to engage with scientific advancements, as these developments will shape the world their children will live in. Science is now a key part of how humanity envisions and shapes its future. The implications of scientific progress can impact everything from government budgets to the health of future generations and the long-term survival of life on Earth."
5,C2,"From around 2015, after many years of growth, major publishers began to notice that ebook sales had either leveled off or even decreased in some cases. This raised new doubts about the long-term future of ebooks in the publishing industry. One anonymous publishing executive recently admitted that the excitement around ebooks may have led to poor investments, causing his company to lose faith in the value of printed books. Despite the clear idea that digital and print books can coexist, the debate about whether ebooks will replace print books continues. Whether people are trying to predict or dismiss this possibility, the idea of books disappearing still sparks our imagination and leads to intense discussions. 
Why is this idea so compelling? Why do we see the relationship between ebooks and print books as a conflict, even when evidence suggests otherwise? The answers to these questions go beyond ebooks and reveal much about our mixed feelings of excitement and fear regarding innovation and change. In my research, I have explored how the notion of one medium replacing another often arises with new technologies. Even before digital technology, critics predicted the end of existing media. For example, after television was invented, many believed radio would disappear. However, radio adapted and found new ways to be used, such as in cars and factories. 
The idea that books might disappear is not new either. As early as 1894, people speculated that the phonograph would replace books with what we now call audiobooks. This pattern has repeated itself many times. Movies, radio, television, hyperlinks, and smartphones have all been thought to threaten print books as sources of culture and entertainment. Some argued that the end of books would lead to cultural decline, while others exaggerated the benefits of ebooks in a hopeful digital future. 
The idea of the death of the book often arises during times of technological change. This narrative reflects our mixed feelings about technology, combining hope and fear. To understand why these feelings are so common, we must recognize that we form emotional connections with different media as they become part of our lives. Many studies show that we develop strong attachments to objects like books, televisions, and computers, even naming our cars or expressing frustration at our laptops. 
When new technology, like e-readers, emerges, it not only signals economic and social change but also forces us to rethink our relationship with something that has become essential in our daily lives. As technology evolves, we often find ourselves missing what we once knew but no longer have. This longing is why industries often form around retro products and older technologies. For instance, the spread of the printing press in 15th-century Europe led people to seek out original manuscripts. The transition from silent to sound movies in the 1920s created nostalgia for the older format. The same occurred with the shift from analog to digital photography and from vinyl records to CDs. 
E-readers have also sparked a renewed appreciation for the physical qualities of traditional books, even their unique smell. This should reassure those who worry about the decline of print books. However, the idea of a disappearing medium will continue to be an attractive story about the power of technology and our resistance to change. One way we cope with change is by using familiar narrative patterns, such as stories of tragedy and endings. These narratives are easy to remember and share, reflecting both our excitement for the future and our fear of losing parts of our familiar world—and ultimately, ourselves."
6,C2,"For a year and a half, I woke up every weekday at 5:30 AM. I brushed my teeth, made coffee, and wrote about how some of the greatest thinkers of the last four hundred years managed their time to do their best work. I focused on the everyday details of their lives—when they slept, ate, worked, and worried—to offer a fresh perspective on their personalities and careers. The French writer Jean Anthelme Brillat-Savarin once said, “Tell me what you eat, and I shall tell you what you are.” I would say, “Tell me what time you eat and if you take a nap afterward.” In this way, my book is somewhat superficial. It looks at the conditions for creativity rather than the creative work itself. However, it is also personal. The novelist John Cheever believed that even a business letter reveals something about the writer’s inner self, and I agree.
The main questions I explore in this book are ones I face in my own life: How can you do meaningful creative work while making a living? Is it better to focus entirely on one project or to work on many things a little each day? When time is limited, do you have to give up things like sleep, income, or a tidy home, or can you learn to do more in less time? I don’t claim to answer these questions definitively—some may only be resolved through personal compromise—but I provide examples of how many successful people have dealt with similar challenges. I wanted to show how big creative ideas can be broken down into small daily actions and how our habits affect our work and vice versa.
The title of the book is Daily Rituals, but I really focus on people’s routines. Routines can seem ordinary and thoughtless, like being on autopilot. However, a good routine can help you use limited resources like time, willpower, and optimism effectively. A strong routine creates a path for your mental energy and helps you manage your moods. The psychologist William James believed that putting parts of your life on autopilot through good habits allows us to focus on more interesting activities. Ironically, he struggled with procrastination and could not stick to a regular schedule.
Interestingly, it was a moment of procrastination that inspired this book. One Sunday afternoon, while working at a small architecture magazine, I was supposed to write a story due the next day. Instead of working, I found myself cleaning my workspace and making coffee. As a morning person, I can concentrate well in the early hours but struggle in the afternoon. To feel better about this, I started looking online for information about other writers’ schedules. I found many entertaining stories and thought it would be great to collect them, which led to the Daily Routines blog I started that day and now this book.
The blog was informal; I shared people’s routines from biographies, magazine articles, and obituaries. For the book, I have gathered a much larger and better-researched collection while trying to keep the variety of voices that made the blog enjoyable. I let my subjects speak for themselves through quotes from their letters, diaries, and interviews. In some cases, I summarized their routines from other sources. I must acknowledge that this book would not have been possible without the research of many biographers, journalists, and scholars. I have listed all my sources in the Notes section, which I hope will guide further reading."
79,C2,"One of the most interesting changes in education in the UK and around the world in recent years is the push for new students to get involved in research as soon as they start their studies. This change acknowledges that research is not just for famous scholars at old universities or scientists making new discoveries. Instead, research is a natural and essential way to build knowledge and skills. The skills you learn from research will be valuable not only in your studies but also in your future jobs, as they help you think critically about the world and approach your work effectively.
As a student, you contribute to knowledge. You do not just memorize and repeat information; you create it. Creating knowledge involves asking questions rather than accepting things as they are. You might wonder: Why? How? When? What does this mean? What if things were different? How does it work in this situation? What should we think about the facts, opinions, or beliefs we encounter? Why is this important? These questions are at the heart of what we call research.
Research can be seen as a range of activities. On one end, there is complex, groundbreaking research done by highly trained experts, which leads to significant changes and new knowledge. However, research can also start from simple inquiries that involve careful work, thoughtful questions about various issues, following leads, and making practical suggestions.
Most students have always been researchers in some way. You have likely done some research for school projects or answered questions at work since you began your education. You have asked questions that required investigation, whether you were curious about studying or figuring out how to do everyday tasks, like planning a holiday, growing plants, fixing things at home, training a pet, or shopping online.
In college and higher education, you are expected to have an inquisitive mind, identify problems and questions, critically evaluate information and ideas, and create your own responses and knowledge to contribute to discussions. Some students may find this challenging because, in some cultures, knowledge is seen as fixed, and learning often involves listening to teachers and texts without questioning them. It may feel disrespectful to challenge established knowledge or authority, and you might think you should be told what is important to learn.
However, in higher education in the UK, US, much of Europe, and Australasia, questioning established knowledge and authorities is encouraged. Engaging with and constructing knowledge can seem intimidating, but critical thinking is crucial in research. The research done by others is valuable for students, academics, and professionals, but we must not simply repeat what we read or accept everything as fact. Instead, we should think critically about it, test it, and determine whether the information is logical, well-reasoned, and supported by evidence. We should avoid relying blindly on the facts and information provided by others."
80,C2,"Cities have always been centers of intellectual activity. In the 18th century, coffee houses in London were places where people discussed science and politics. Similarly, in modern Paris, cafés on the Left Bank were popular among artists like Pablo Picasso, who talked about modern art. However, living in a city can be challenging. The same London cafés that encouraged discussion also contributed to the spread of diseases like cholera, and Picasso eventually moved to the countryside. 
Today, cities are known for their creativity, but they can also feel overwhelming and unnatural. Recent scientific studies have started to explore how city life impacts our brains, and the findings are concerning. While it has long been understood that city living can be tiring, new research indicates that it can actually impair our thinking, sometimes significantly. One major factor is the lack of nature in urban environments. Research shows that hospital patients recover faster when they can see trees from their windows. Even brief views of nature can enhance brain function by providing a mental break from the stress of city life.
This research comes at a significant time: for the first time in history, more people live in cities than in rural areas. Instead of enjoying open spaces, we find ourselves in crowded concrete environments, surrounded by many strangers. It has become clear that these unnatural settings can greatly affect our mental and physical health and change the way we think. 
Consider what your brain has to manage while walking down a busy street: distracted people, dangerous road crossings, and the complex layout of the city. These everyday tasks can drain our mental energy because they require us to constantly shift our focus. The brain is like a powerful computer, but paying attention uses a lot of its energy. In contrast, natural environments do not demand as much mental effort. This concept is known as attention restoration theory, introduced by psychologist Stephen Kaplan. He suggested that being in nature can help restore our attention. Natural settings have elements that naturally attract our focus without causing stress, unlike loud noises like police sirens. This allows our mental resources to recover.
Long before scientists studied this, philosophers and landscape architects warned about the negative effects of city life and sought ways to incorporate nature into urban areas. Parks, like Central Park in New York, provide a much-needed escape from city life. A well-designed park can enhance brain function in just a few minutes. While many have looked for ways to boost mental performance, such as energy drinks or office redesigns, nothing seems to be as effective as simply walking in nature.
Given the many mental challenges caused by city living, one might wonder why cities keep growing. Even in our digital age, cities remain vibrant centers of intellectual activity. Research from the Santa Fe Institute shows that the same urban features that can lead to attention and memory problems—like crowded streets—are also linked to innovation. Scientists believe that the “concentration of social interactions” is key to urban creativity. Just as the crowded streets of 18th-century London led to new ideas, the busy environment of 21st-century Cambridge, Massachusetts, fosters creativity in technology. Conversely, less crowded areas may produce less innovation over time.
The challenge, then, is to find ways to reduce the negative psychological effects of city life while preserving its unique advantages. After all, there will always be moments when someone might say, “I’m tired of nature; take me to the city!”"
81,C2,"Where should we search for the mind? This question may seem strange because we usually think that thinking happens inside our heads. Today, we have advanced brain imaging techniques that support this idea. However, I believe there is no strong reason to limit the study of the mind to just the brain or the body. There is a lot of evidence, from ancient times to now, showing that objects, as well as brain cells, play a role in how we think. 
From an archaeological perspective, it is clear that items like stone tools, jewelry, carvings, clay tokens, and writing systems have significantly influenced human evolution and the development of our minds. Therefore, I propose that what exists outside our heads may also be part of our minds. 
It is understandable why we often connect the mind with the brain. Most of our knowledge about the mind comes from studying people in isolation from their surroundings. This approach is practical for neuroscientists, especially when using brain scanners. However, it often overlooks the fact that much of our thinking occurs outside our heads. I do not intend to deny the importance of the brain in thinking, but I want to emphasize that the mind is more than just the brain. 
Instead, we should consider the idea that human intelligence extends beyond our bodies into culture and the material world. This is where my new theory, called Material Engagement Theory (MET), comes into play. MET explores how objects can become extensions of our thinking or be integrated into our bodies. For example, when we create numbers and symbols from clay or use a stone to make a tool, we are engaging with materials in a cognitive way. 
MET also looks at how these interactions have changed since ancient times and what these changes mean for our thinking. This perspective provides new insights into what minds are and how they function by examining the role of objects in our thinking. 
Consider a blind person using a stick. Where does this person's self begin? The connection between the blind person and the stick illustrates how minds and objects can be seen as interconnected. It also highlights the flexibility of the human mind: by using the stick, the blind person transforms touch into a form of sight, and the stick itself plays an active role. The brain perceives the stick as part of the body. 
The blind person’s stick reminds us that human intelligence can adapt and change significantly by incorporating new technologies. I view the human mind as an ongoing project, constantly evolving. It is essential to remember that, regardless of the form that tools have taken throughout our history—from ancient stone tools to modern technology—their main purpose has been to connect us with our environment rather than to limit us. 
For humans, tools are used to satisfy our curiosity and desire for knowledge. This unique ability to engage with material culture explains why humans create more than any other species, and how these creations shape our minds. I refer to this concept as metaplasticity—our minds are flexible and develop as we interact with the material world. 
I aim to reintroduce the importance of materiality in understanding cognition. MET provides a new perspective on how various forms of material culture, from ancient tools to smartphones, have played a crucial role in defining and transforming who we are and how we think. While the idea of technology changing our minds may sound futuristic, it is important to recognize that humans have been using such technology since our earliest days."
