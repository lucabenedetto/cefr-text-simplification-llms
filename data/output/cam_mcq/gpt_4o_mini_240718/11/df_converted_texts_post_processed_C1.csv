text_id,text_level,text
1,C2,"'Some time ago, a website pointed out the risks of public check-ins, which are online announcements of where you are. The website's message was clear: while you might think you are just saying, ""Hey, I’m at this place,"" you are also letting many people know your location – not all of whom you might want to meet. This highlighted the growing understanding that there can be negative effects to the constant sharing that the internet encourages. The internet offers many chances to share every part of our lives with a global audience, which can seem exciting: wealth, fame, and more! So, we dive into the chaos of the internet, sharing personal stories and photos. However, we soon realize that the online world can be crowded and dangerous, and we can feel lost. This might sound discouraging, but there is hope. The future has a guide, created years ago by a group of early internet users. In the beginning of the web, they explored these risky waters and identified many dangers. They faced job losses, changes in friendships, and the challenges of fame long before social media existed. These early users, known as bloggers, have already experienced what many of us are now facing. It is important to learn from their experiences. In January 1994, Justin Hall, a 19-year-old student, started posting on the 'WWW', which was mostly used by graduate students, scientists, and a few talented teens. The web was created at CERN, a physics lab in Switzerland, to help researchers share their work. Hall saw a different opportunity: to share his life. He created a complex online autobiography filled with his thoughts, photos, and art. In January 1996, he began a daily blog, attracting many readers who were fascinated by his bold use of this new medium. Hall's approach was simple: anyone who crossed his path could be featured on his site, and no topic was off-limits. While some might see this as attention-seeking, there was also a depth and beauty to his work that many would recognize as art. However, one day, visitors to Hall’s site found his homepage replaced by a single, emotional video titled Dark Night. He shared that he had fallen deeply in love, but when he wrote about it online, he was told, ""either the blog goes, or I do."" He felt that sharing his life online made people distrust him. The blog ended, but the issue remains. Sharing online can be wonderful, but if you think that sharing your life will make people want to be around you, you might be disappointed. In 2002, Heather Armstrong, a young web worker in Los Angeles, had a blog called Dooce. Sometimes, she wrote about her job at a software company. One day, an anonymous coworker sent the link to her blog to all the vice presidents at her company, including some she had criticized, which led to her losing her job. Researchers who study online behavior have a term for this: the 'online distribution effect,' which describes the feeling that we can say things online that we would never say in person. However, the internet is not a separate reality where we can speak freely without consequences. Our online lives are connected to our real lives, and ignoring this can lead to serious mistakes. Armstrong's story had a positive outcome. Although she was upset and stopped blogging for a few months, she eventually got married and restarted her blog, focusing on her new family. Today, she is a well-known 'mommy blogger,' and her writing supports her family. Once a symbol of the risks of online sharing, she has become skilled at managing her self-presentation. What Armstrong has learned is something we should all remember: the internet allows us to say anything, but that doesn’t mean we should.'"
2,C2,"'Some scientists recently started a campaign to warn people about a low-budget film called What the Bleep Do We Know? This film mixes documentary and drama to suggest that there is much about our universe that we do not understand. How can any scientist disagree with that? Scientists can sometimes be arrogant, but even the most enthusiastic physicist would not claim to know everything about the universe. However, some scientists felt it was important to warn the public about the film, calling it everything from ‘atrocious’ to ‘very dangerous’. Clearly, this film was controversial. 
At first, I did not understand why there was so much concern. Various scientists made harmless statements about how new discoveries were showing that the universe is stranger than we thought. It was only when the film discussed some of these discoveries that I began to understand the concern – discoveries like the idea that water molecules can be influenced by thoughts. I had heard before about a Japanese researcher who claimed that the shape of a water molecule could change based on the thoughts of people nearby. However, the film only provided pictures of ice crystals that looked beautiful after being spoken to by happy people and ugly after being exposed to angry people. 
Many people find this kind of evidence convincing because it is simple and easy to understand. However, scientists typically respond with skepticism. I understand their point. The idea that water can be influenced by thoughts is fascinating and suggests new forces in the universe. But before we get too excited, we need solid proof that this effect is real. Beautiful pictures of crystals, while nice, are not enough. 
The real issue is that the film’s claims were not strange enough. Consider this: water molecules might have properties due to a type of energy that seems to come from nowhere, which could be connected to a force that is causing the universe to expand. The evidence for this is not just pretty pictures; it comes from years of research in labs and observatories worldwide. 
In fact, discoveries are being made that confirm the film’s claim that the universe is stranger than we ever imagined. Astronomers have discovered that the universe is made of an unknown type of matter and is driven by a mysterious force called ‘dark energy’. Additionally, discoveries in other fields are equally surprising. Neuroscientists have found that our awareness of events happens about half a second after they occur, a delay we do not notice because our brains edit it out. Anthropologists believe they have found where modern humans originated and how they spread across the world. Some theorists even suggest links between life on Earth and the fundamental design of the universe. 
Despite what some may think, science is not close to being complete. In fact, we seem to be further from knowing everything than ever before. Many natural phenomena may never be fully understood. The ideas of chaos and quantum uncertainty have shown that there are limits to what we can know. This has led some of the world’s top theoretical physicists to work on the so-called Theory of Everything, which aims to explain all the forces and particles in the universe with one equation. Overall, many of us believe that the universe can be described in one word: incredible.'"
3,C2,"""In simple terms, I find writing novels challenging, while writing short stories is a joy. If writing novels is like planting a forest, then writing short stories is like planting a garden. Both processes work together to create a beautiful landscape that I cherish. The trees provide shade, and the wind rustles the leaves, which can turn a bright gold. In the garden, flowers bloom, and colorful petals attract bees and butterflies, showing the gentle change of seasons. 
Since I started my career as a fiction writer, I have often switched between writing novels and short stories. My routine is this: after finishing a novel, I want to write short stories; after completing a group of short stories, I feel ready to focus on a novel. I never write short stories while working on a novel, and I never write a novel while working on short stories. The two types of writing may use different parts of the brain, and it takes time to switch from one to the other. 
I began my career with two short novels in 1975, and from 1984 to 1985, I started writing short stories. I knew little about writing short stories then, so it was challenging, but I found the experience memorable. I felt my fictional world expand, and readers seemed to enjoy this different side of me as a writer. One of my early works, 'Breaking Waves,' was included in my first short-story collection, Tales from Abroad. This marked my beginning as a short-story writer.
One of the joys of writing short stories is that they don’t take long to finish. Usually, it takes me about a week to shape a short story (though revisions can take longer). It’s not the same level of commitment as writing a novel, which can take a year or two. You go into a room, finish your work, and leave. For me, writing a novel can feel endless, and I sometimes wonder if I will make it through. So, writing short stories provides a necessary change of pace.
Another nice thing about short stories is that you can create a story from the smallest ideas—like a thought, a word, or an image. Often, it feels like jazz improvisation, with the story leading me where it wants to go. Plus, with short stories, you don’t have to worry about failing. If an idea doesn’t work out, you can just accept it and move on. Even great writers like F. Scott Fitzgerald and Raymond Carver didn’t write a masterpiece every time. This thought comforts me. 
You can learn from your mistakes and use that knowledge in your next story. When I write novels, I try to learn from both my successes and failures in writing short stories. In this way, short stories serve as a kind of experimental space for me as a novelist. It is hard to experiment the way I want to within a novel, so without short stories, writing novels would be even more difficult.
My short stories are like soft shadows I have placed in the world, faint traces I have left behind. I remember exactly where I created each one and how I felt at that moment. Short stories are like guideposts to my heart, and it makes me happy as a writer to share these personal feelings with my readers."""
4,C2,"At its core, science is closely related to philosophy, but it also has practical applications, such as curing diseases. It has improved our lives but also posed threats to our existence. Science seeks to understand everything from tiny ants to the vast universe, but it often struggles to do so. It has influenced poets, politicians, philosophers, and even frauds. The beauty of science is often recognized only by those who study it deeply, while its dangers are frequently misunderstood. Its significance has been both overestimated and underestimated, and the mistakes made by scientists are often overlooked or exaggerated.
The history of science is marked by constant conflict. Established theories are often changed or completely rejected, similar to how new music styles are initially mocked before becoming accepted. The battle between old and new ideas is rarely respectful. Scientists can be driven by jealousy and anger. The history of science is essentially a history of conflict.
This book will explore scientific ideas that have transformed not just science, but also many areas of human thought. While science has practical benefits, this book will focus on the ideas themselves, appreciating their beauty and creativity, while also being ready to question them. We must recognize both the cleverness and the limitations of human understanding.
Science is inherently changeable. There is always a scientist challenging the ideas of another. Most of the time, these changes do not disrupt society. However, sometimes they can lead to significant shifts in our beliefs. For example, in the seventeenth century, science introduced the idea of a mechanical universe, like a giant clock. Three hundred years later, physics began to question basic ideas, leading us to a complex understanding where our observations can influence the universe, and we struggle to grasp the true meaning of fundamental concepts.
Some people view the instability of scientific theories as a sign that science cannot explain the universe. However, scientific changes usually improve our ability to understand and predict natural events. For instance, Isaac Newton, a seventeenth-century scientist, could explain much more than the ancient Greek thinker Aristotle, and Albert Einstein, the founder of modern physics, could explain even more than Newton. Science may falter, but it continues to progress.
It is important to remember that at the end of the nineteenth century, many physicists believed there was little left to discover in physics. Then came breakthroughs like radioactivity, X-rays, the discovery of the electron and the nucleus, and many new particles, along with quantum mechanics and relativity. Biology has also made significant advancements. Today, some claim that a complete theory explaining the universe's origins and workings is on the horizon. 
Science is not just an academic exercise. In the last two centuries, we have shifted from merely observing nature to controlling it in small but growing ways. This has sometimes disrupted the natural balance in ways we do not fully understand. It is crucial for everyone, not just scientists, to be aware of scientific advancements that will shape the future for their children and future generations. Science has become a key part of how humanity envisions and influences its future. The implications of scientific discoveries can impact national budgets, the health of future generations, and even the long-term survival of life on Earth."
5,C2,"'Since around 2015, after many years of growth, major publishers began to notice that ebook sales had leveled off or even declined in some cases. This raised new questions about the long-term future of ebooks in the publishing industry. One anonymous publishing executive recently admitted that the excitement around ebooks may have led to poor investments, causing his company to lose faith in “the power of the word on the page.” Despite the clear idea that digital and print books can coexist in the market, the debate about whether ebooks will “kill” print books continues. Whether people are trying to predict or dismiss this idea, it is important to note that the potential disappearance of books still sparks our imagination and leads to intense discussions. Why is this idea so compelling? Why do we often see the relationship between ebooks and print books as a conflict, even when evidence suggests otherwise? The answers to these questions go beyond ebooks and reveal much about our mixed feelings of excitement and fear regarding innovation and change. In my research, I have explored how the idea of one medium “killing” another often appears with new technologies. Even before digital technologies, critics predicted the end of existing media. For example, after television was invented, many believed radio would disappear. However, radio survived by finding new ways to be used; people began listening in cars, on trains, and in factories. The idea of the disappearing book is not new either. As early as 1894, people speculated that the phonograph would replace books with what we now call audiobooks. This pattern has repeated itself many times. Movies, radio, television, hyperlinks, and smartphones have all been thought to threaten print books as sources of culture and entertainment. Some believed the end of books would lead to cultural decline, while others, imagining a perfect digital future, exaggerated the benefits of ebooks. It is no coincidence that the idea of the death of the book arises during times of technological change. This narrative reflects our mixed hopes and fears about technology. To understand why these reactions are so common, we must consider that we form emotional connections with different media as they become part of our lives. Many studies show how we develop strong ties to objects like books, televisions, and computers, even naming our cars or expressing frustration at our laptops. Therefore, the arrival of new technology, like e-readers, does not just signal economic and social change; it also forces us to rethink our relationship with something that has become essential in our daily lives. As technology evolves, we often find ourselves missing what we once knew but no longer have. This is why entire industries form around retro products and older technologies. For instance, the spread of the printing press in 15th-century Europe led people to seek original manuscripts. The transition from silent to sound movies in the 1920s created nostalgia for the older format. The same occurred with the shift from analog to digital photography and from vinyl records to CDs. Unsurprisingly, e-readers have sparked a renewed appreciation for the physical qualities of “old” books, including their often distinct smell. This should reassure those who worry about the decline of print books. However, the idea of a disappearing medium will continue to be an attractive story about both the transformative power of technology and our resistance to change. In fact, one way we cope with change is by using familiar narrative patterns, such as stories of tragedy and endings. Easy to remember and share, the story of the death of the book reflects both our excitement for the future and our fear of losing parts of our familiar world – and ultimately, of ourselves.'"
6,C2,"'For a year and a half, I woke up nearly every weekday morning at 5:30. I brushed my teeth, made a cup of coffee, and sat down to write about how some of the greatest thinkers of the last four hundred years managed their time. I wanted to explore how they organized their days to be creative and productive. By discussing the everyday details of their lives—when they slept, ate, worked, and worried—I aimed to offer a fresh perspective on their personalities and careers, showing them as people with habits like ours. The French gastronome Jean Anthelme Brillat-Savarin once said, “Tell me what you eat, and I shall tell you what you are.” I would say, “Tell me what time you eat, and if you take a nap afterward.” In this way, the book is somewhat superficial. It focuses on the conditions of creative work rather than the work itself; it looks at the process rather than the meaning. However, it is also personal. The novelist John Cheever believed that even a business letter reveals something about the writer—this is true. My main concerns in the book reflect my own struggles: How can you do meaningful creative work while making a living? Is it better to focus entirely on one project or to work a little each day? When time is limited, do you have to give up things like sleep, income, or a tidy home, or can you learn to do more in less time, to “work smarter, not harder,” as my dad says? I do not claim to answer these questions in the following pages—some may not have clear answers, or may only be resolved through personal compromises—but I have tried to show how various brilliant and successful people have faced similar challenges. I wanted to illustrate how big creative ideas can be broken down into small daily actions and how one’s habits can affect their work and vice versa. The book is titled Daily Rituals, but I really focused on people’s routines. The word suggests something ordinary and thoughtless; following a routine can feel automatic. However, a good routine can help you use limited resources like time, willpower, and optimism effectively. A strong routine creates a path for your mental energy and helps manage your moods. The psychologist William James believed that putting part of your life on autopilot is beneficial; by forming good habits, we can free our minds for more interesting activities. Ironically, James himself struggled with procrastination and could not stick to a regular schedule. Interestingly, it was a moment of procrastination that inspired this book. One Sunday afternoon, while working at a small architecture magazine, I was supposed to write a story due the next day. Instead of focusing on my work, I found myself cleaning my cubicle and making coffee, wasting time. I am a “morning person,” able to concentrate well in the early hours but not so much after lunch. That afternoon, to feel better about my tendency to procrastinate, I started looking online for information about other writers’ schedules. I found many entertaining stories. I thought it would be great to collect these anecdotes, which led to the Daily Routines blog I started that day and now this book. The blog was informal; I simply shared descriptions of people’s routines from biographies, magazine articles, and obituaries. For the book, I have gathered a much larger and better-researched collection while trying to keep the variety of voices that made the original blog enjoyable. I have let my subjects speak for themselves through quotes from letters, diaries, and interviews. In some cases, I summarized their routines from other sources. I should mention that this book would not have been possible without the research of many biographers, journalists, and scholars whose work I used. I have listed all my sources in the Notes section, which I hope will guide further reading.'"
79,C2,"
In recent years, there has been an interesting trend in further and higher education in the UK and around the world. This trend encourages new students to get involved in research as early as possible in their studies. It acknowledges that research is not just for famous scholars at old universities or scientists making new discoveries, but is also a natural way to build knowledge and skills. Research skills are valuable not only for your studies but also for your future job, as they help you think critically about the world and your work.
As a student, you contribute to knowledge. You do not just learn and repeat information; you create it. Creating knowledge involves asking questions instead of accepting things as they are. You might wonder: Why? How? When? What does this mean? What if things were different? How does it work in this situation? What should we think about the facts, views, or beliefs we encounter? Why is this important? These questions are at the heart of what we call research.
Research can be seen as a range of activities. On one end, there is complex, groundbreaking research done by highly trained experts, which leads to significant changes and new knowledge. However, research can also start from simple inquiries that involve careful work, thoughtful questions about issues, and practical suggestions. Most students have always been researchers in some way. You have likely done research for school projects or answered questions at work since you began studying. You have asked questions that led you to investigate topics, whether it was planning a holiday, growing plants, fixing things at home, training a pet, or shopping online.
In college and higher education, it is expected that you will have a curious mind, identify problems and questions, critically explore and evaluate information, and create your own responses and knowledge. Some students may find this challenging because, in some cultures, knowledge is seen as fixed, and learning comes from listening to teachers and texts. It may feel disrespectful to question established knowledge and authority, and you might think you should be told what is important to learn. However, in higher education in the UK, US, much of Europe, and Australasia, questioning established knowledge and authorities is encouraged.
This process of inquiry and knowledge creation can seem overwhelming. Critical thinking is especially important in research. The research done by others is valuable for students, academics, and professionals, but we must not simply repeat what we read as if it is always true. We need to engage with it, think critically, test ideas, and determine if what we are told is logical and supported by evidence. We should avoid relying uncritically on the facts and information provided by others in our readings or discussions."
80,C2,"'Cities have always been centers of intellectual life, from the coffee houses of 18th-century London, where people discussed chemistry and politics, to the cafés of modern Paris, where artist Pablo Picasso talked about modern art. However, city life can be challenging. The same London cafés that encouraged discussion also helped spread diseases like cholera; Picasso eventually moved to the countryside. While modern cities can be creative places, they can also feel unnatural and overwhelming. Scientists are now studying how city life affects our brains, and the findings are concerning. It has long been known that city life is tiring, but new research shows that living in cities can actually reduce our ability to think clearly. One major reason for this is the lack of nature, which is surprisingly good for our brains. Studies have shown that hospital patients recover faster when they can see trees from their windows. Even small views of nature can improve brain function because they provide a break from the busy city life. This research comes at a time when more people than ever live in cities. Instead of living in open spaces, we are packed into concrete environments, surrounded by many strangers. Recent studies show that these unnatural surroundings can significantly affect our mental and physical health and change how we think. Think about everything your brain has to manage while walking down a busy street: distracted people, dangerous crossings that require attention to traffic, and the confusing layout of the city. These seemingly simple tasks can drain our energy because they take advantage of a weak point in our brains. Cities are filled with stimuli, so we must constantly shift our focus. This kind of attention management – deciding what to ignore and what to pay attention to – requires energy. The brain is like a powerful computer, but focusing uses a lot of its processing power. In contrast, natural environments do not demand as much mental effort. This idea is known as attention restoration theory, developed by psychologist Stephen Kaplan. It has been known for a long time that human attention is limited – focusing in the morning makes it harder to concentrate in the afternoon. Kaplan suggested that being in nature could help restore our attention. Natural settings have many things that naturally attract our focus, but they do not cause negative feelings – unlike a police siren, for example. In nature, our attention can relax and recharge. Long before scientists studied this, philosophers and landscape architects warned about the effects of city life and sought ways to include nature in urban areas. Vibrant parks, like Central Park in New York, provide a break from city life. A well-designed park can enhance brain function in just a few minutes. While people have looked for various ways to boost cognitive performance, from energy drinks to office redesigns, it seems that few methods are as effective as simply taking a walk in nature. Given the many mental challenges caused by city life, one question remains: Why do cities keep growing? And why, even in the digital age, do they continue to be sources of intellectual life? Recent research at the Santa Fe Institute showed that the same urban features that cause attention and memory problems – crowded streets and close proximity to others – are also linked to innovation. Scientists believe that the ‘concentration of social interactions’ is mainly responsible for creativity in cities. Just as the crowded environment of 18th-century London led to new ideas, the crowded nature of 21st-century Cambridge, Massachusetts, contributes to its success as a technology hub. Conversely, less crowded urban areas may produce less innovation over time. The challenge, then, is to find ways to reduce the psychological stress of city life while keeping its unique advantages. Because there will always come a time when someone will say: ‘I’m tired of nature; take me to the city!’'"
81,C2,"'Where should we look for the mind? This may seem like a strange question: thinking happens in people’s heads, right? Today, we have advanced brain imaging techniques to support this idea. However, I believe there is no strong reason to limit the study of the mind to just the inside of our bodies. There is a lot of evidence, from ancient times to now, showing that objects, as well as our brains, play a role in how we think. From an archaeological perspective, it is clear that tools, decorations, carvings, clay tokens, and writing systems have actively contributed to human evolution and the development of our minds. Therefore, I propose that what exists outside our heads may also be part of our minds. 
It is easy to understand why we often connect the mind with the brain. Most of what we know about the human mind comes from studying people in isolation from their surroundings. This makes sense for neuroscientists, especially when using brain-scanning machines. However, this approach often overlooks the fact that much of our thinking happens outside our heads. I do not intend to deny the importance of the brain in thinking, but I want to emphasize that the mind is more than just the brain. It is more useful to explore the idea that human intelligence extends beyond our bodies into culture and the material world. This is where my new theory, Material Engagement Theory (MET), comes in. 
MET aims to investigate how objects can become extensions of our thinking or be integrated into our bodies. For example, when we create numbers and symbols from clay or use a stone to make a tool. It also looks at how these interactions have changed since ancient times and what these changes mean for our thinking. This approach provides new insights into what minds are and how they are shaped by the things around us. 
Consider a blind person using a stick. Where does this person’s self begin? The connection between the blind person and the stick helps us understand that minds and objects can be seen as interconnected. It also illustrates the flexibility of the human mind: by using the stick, the blind person transforms touch into a form of sight, while the stick itself plays an important role. The sensation felt at the tip of the stick connects the person to the environment, and the brain treats the stick as part of the body. 
The blind person’s stick reminds us that human intelligence can adapt and change significantly by incorporating new technologies. I view the human mind as an ongoing project, always evolving. It is essential to remember that, regardless of what form the ‘stick’ has taken throughout our history—whether it be ancient stone tools or modern technology—its main purpose is to connect us to the world, not to separate us from it. 
For humans, tools are used to satisfy our curiosity. This unique ability to engage with material culture explains why we create things more than any other species, and how these creations, in turn, shape our minds. I call this metaplasticity—our minds are flexible and develop as we interact with the material world. I want to reintroduce the importance of material objects in understanding cognition. MET provides a new perspective on how various forms of material culture, from ancient tools to the latest smartphones, have influenced and transformed who we are and how we think. While the idea of technology changing our minds may sound futuristic, it is important to realize that humans have been using such tools since our earliest days.'"
