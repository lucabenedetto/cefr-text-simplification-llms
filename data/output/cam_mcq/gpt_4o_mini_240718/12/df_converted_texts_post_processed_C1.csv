text_id,text_level,text
1,C2,"
Some time ago, a website discussed the dangers of public check-ins, which are online announcements of where you are. The website's message was clear: while you might think you are simply saying, ""Hey, I’m at this place,"" you are also letting many people know your location – not all of whom you may want to meet. This highlights a growing understanding that there can be negative consequences to the constant sharing enabled by the internet. The internet offers many exciting opportunities to share our lives with a global audience, which can lead to wealth and fame. However, as we dive into the online world, sharing personal stories and photos, we may find ourselves in a crowded and risky environment.
This situation may seem discouraging, but there is hope. The future has been mapped out for us by early internet pioneers. In the beginning, these pioneers explored the internet and identified its dangers. They faced job losses, changes in friendships, and the challenges of fame long before social media existed. These early bloggers have experienced what many of us are now facing. It is important to learn from their experiences, as those who do not learn from history are likely to repeat it.
In January 1994, Justin Hall, a 19-year-old student, started posting on the World Wide Web, which was mostly used by graduate students, scientists, and a few talented teenagers. The web was created at CERN, a physics lab in Switzerland, to help researchers share their work. Hall saw a different opportunity: to share his life. He created a complex online autobiography filled with personal stories, photos, and art. In January 1996, he began a daily blog, attracting many readers who were fascinated by his bold use of this new medium. Hall's approach was straightforward: anyone who crossed his path could be featured on his site, and no topic was off-limits. While some may see this as exhibitionism, there was also a unique beauty to his work that many would consider art.
However, one day, visitors to Hall’s site found his homepage replaced by a single, emotional video titled ""Dark Night."" In it, he shared that he had fallen deeply in love, but when he wrote about it online, he was told, ""either the blog goes, or I do."" Hall felt that sharing his life online made people distrust him. The blog was removed, but the issue remains. Sharing online can be wonderful, but if you think that sharing your life will make people want to be around you, you may be disappointed.
In 2002, Heather Armstrong, a young web worker in Los Angeles, had a blog called Dooce. She sometimes wrote about her job at a software company. One day, an anonymous colleague shared the link to her blog with all the vice presidents at her company, including some she had criticized, which led to her losing her job. Researchers who study online behavior have a term for this: the ""online distribution effect,"" which describes the feeling that we can say things online that we would never say in person. However, the internet is not a separate reality where we can speak freely without consequences. Our online lives are connected to our real lives, and ignoring this can lead to serious mistakes.
Armstrong's story had a positive outcome. Although she was upset and stopped blogging for a few months, she eventually got married and restarted her blog, focusing on her new family. Today, she is a well-known ""mommy blogger,"" and her writing supports her family. Once a symbol of the risks of online sharing, she has become skilled at managing her self-revelation. Armstrong's experience teaches us an important lesson: while the internet allows us to say anything, that doesn’t mean we should."
2,C2,"'Some scientists recently started a campaign to warn people about a low-budget film called What the Bleep Do We Know? This film mixes documentary and drama to suggest that there is much we do not understand about our universe. How can any scientist disagree with that? While scientists can sometimes be arrogant, even the most enthusiastic physicist would not claim to know everything about the cosmos. Nevertheless, some scientists felt it was necessary to warn the public about the film, using strong language to describe it, from calling it ‘atrocious’ to saying, ‘It is a very dangerous piece of work.’ Clearly, this film was generating a lot of attention. However, for the first twenty minutes, I did not understand why. Various scientists made harmless statements about how new discoveries were showing that the universe is stranger than we thought. It was only when the film discussed some of these discoveries that I began to understand the excitement – discoveries like the idea that water molecules can be influenced by thoughts. I had heard before about a Japanese researcher who claimed that the shape of a water molecule could change based on the thoughts of people nearby. However, the film only provided pictures of ice crystals that looked beautiful after being spoken to by someone happy, and unattractive after being exposed to someone in a bad mood. Many people find this kind of evidence convincing because it is simple and can easily evoke positive reactions from those who are not well-informed, like saying, ‘Hey, man, far out.’ But among scientists, the common reaction has been: ‘Give me a break.’ I understand their point. The idea that water can be influenced by thoughts is fascinating and has significant implications. However, without solid proof, we should be cautious. Pretty pictures of crystals, while nice, are not enough. The real issue is that the film’s claims were not strange enough. Consider this: water molecules have properties due to a type of energy that seems to appear from nowhere, which may be connected to a force that is causing the universe to expand. The evidence for this is not just a few pretty pictures; it comes from years of research in labs and observatories worldwide. The truth is that discoveries are being made that confirm the film’s claim – the universe is indeed stranger than we could have imagined. Astronomers have discovered that the universe consists of an unknown type of matter and is driven by a mysterious force called ‘dark energy.’ Additionally, discoveries on a more practical level are equally surprising. Neuroscientists have found that our awareness of events happens about half a second after they occur – a delay we do not notice because our brains edit it out. Anthropologists believe they have found the origin of modern humans and understand how and why they spread across the globe. Some theorists even suggest links between the existence of life on Earth and the fundamental design of the universe. Contrary to what some may think, science is not close to being complete. In fact, we seem to be further from knowing everything than ever before. It is now clear that many natural phenomena cannot be understood as we once thought. The ideas of chaos and quantum uncertainty have set limits on our knowledge. This has led some of the world’s top theoretical physicists to work on the so-called Theory of Everything, which aims to explain all the forces and particles of the universe in one equation. Beyond theories, many of us believe that the universe can be best described in one word: incredible.'"
3,C2,"To put it simply, I find writing novels challenging, while writing short stories brings me joy. If writing novels is like planting a forest, then writing short stories is more like planting a garden. Both processes work together to create a beautiful landscape that I cherish. The trees provide shade, and the wind rustles their leaves, which can turn a bright gold. In the garden, flowers bloom, and their colorful petals attract bees and butterflies, showing the gentle change of seasons.
Since I began my career as a fiction writer, I have often switched between writing novels and short stories. My routine is this: after finishing a novel, I feel the urge to write short stories; once I complete a set of short stories, I want to focus on a novel again. I never write short stories while working on a novel, and I don’t write a novel while creating short stories. The two types of writing seem to use different parts of my brain, and it takes time to switch from one to the other.
I started my career with two short novels in 1975, and from 1984 to 1985, I began writing short stories. At that time, I knew little about writing short stories, so it was challenging, but I found the experience memorable. I felt my fictional world expand, and readers appreciated this different side of me as a writer. One of my early works, “Breaking Waves,” was included in my first short-story collection, Tales from Abroad. This marked my beginning as a short-story writer.
One of the joys of writing short stories is that they don’t take long to complete. Usually, it takes me about a week to shape a short story (though revisions can go on forever). It’s not the same level of commitment required for the year or two it takes to write a novel. I can go into a room, finish my work, and leave. Writing a novel can feel endless, and I sometimes wonder if I will make it through. Therefore, writing short stories provides a necessary change of pace.
Another great thing about short stories is that you can create a story from the smallest ideas—like a thought, a word, or an image. Often, it feels like jazz improvisation, where the story leads me where it wants to go. Plus, with short stories, you don’t have to worry about failing. If an idea doesn’t work out, you can simply accept it and move on. Even great writers like F. Scott Fitzgerald and Raymond Carver didn’t write a masterpiece every time. This thought comforts me. You can learn from your mistakes and apply that knowledge to your next story.
When I write novels, I try to learn from both the successes and failures I experience in writing short stories. In this way, short stories serve as a kind of experimental space for me as a novelist. It’s hard to experiment the way I like within a novel, so without short stories, writing novels would be even more challenging.
My short stories are like soft shadows I have left in the world, faint traces of my journey. I remember exactly where I placed each one and how I felt at that moment. Short stories are like guideposts to my heart, and it makes me happy as a writer to share these personal feelings with my readers."
4,C2,"At its most basic level, science connects with philosophy; at its most practical, it helps to cure diseases. Science has improved our lives but also poses threats to our existence. It seeks to understand everything from tiny ants to the vast universe, from the smallest atom to the immense cosmos. Science has influenced poets, politicians, philosophers, and even frauds. Its beauty is often recognized only by those who study it deeply, while its dangers are frequently misunderstood. The significance of science has been both exaggerated and underestimated, and its mistakes, as well as those of its creators, are often overlooked or unfairly highlighted.
The journey to explain the physical universe has been marked by ongoing conflict. Established theories are often changed or completely rejected, similar to how new music styles are initially mocked before becoming accepted. The battle between old and new ideas is rarely respectful. Scientists can be driven by jealousy and anger. The history of science is fundamentally about conflict. This book presents science as a collection of ideas that have transformed not just science itself but also many areas of human thought. While science has practical benefits, our main focus will be on its ideas—appreciating their beauty, being amazed by creativity, but always ready to question them. We must recognize both the cleverness and the significant limitations of the human mind.
Science is inherently changeable. There is always a scientist somewhere disproving another's explanation. Usually, these changes do not disrupt society. However, sometimes, they can challenge our established beliefs. For example, in the seventeenth century, science introduced the idea of a mechanical universe, like a giant clock. Three hundred years later, physics began to question basic ideas, leading us into a complex situation where our observations can influence the universe, and we struggle to understand our fundamental concepts.
Some people view the instability of scientific theories as a sign that science cannot explain the universe. However, scientific changes often improve our ability to understand and predict natural events. The seventeenth-century scientist Isaac Newton could explain much more than the ancient Greek thinker Aristotle, and Albert Einstein, the father of modern physics, could explain even more than Newton. Science may falter, but it continues to advance. 
It is important to remember that at the end of the nineteenth century, many physicists believed there was little left to discover in physics. Then came breakthroughs like radioactivity, X-rays, the discovery of the electron and the nucleus, and many new fundamental particles, along with quantum mechanics and relativity. Biology has also made significant progress. Today, some claim that a complete theory explaining the universe's origins and workings is on the horizon. 
Science is not just a harmless intellectual activity. In the last two centuries, we have shifted from merely observing nature to controlling it in small but growing ways. Sometimes, we have disrupted nature in ways we do not fully understand. Science must be monitored. Non-scientists can no longer afford to remain uninformed about advancements that will shape the world their children will live in and the kind of future they will have. Science has become a crucial part of how humanity envisions and shapes its future. The questions about our future should not be left solely to philosophers, as the answers can impact national budgets, the health of future generations, and even the long-term survival of life on Earth."
5,C2,"
Since around 2015, after many years of growth, major publishers began to notice that ebook sales had either stabilized or even decreased in some cases. This raised questions about the long-term future of ebooks in the publishing industry. One anonymous publishing executive recently admitted that the excitement around ebooks may have led to poor investment decisions, causing his company to lose faith in the value of traditional printed books. Despite the clear idea that digital and print books can coexist, the debate about whether ebooks will replace print books continues. Whether people are trying to predict or dismiss this idea, it is clear that the potential loss of printed books continues to spark our imagination and provoke strong discussions. 
Why is this idea so compelling? Why do we often see the relationship between ebooks and print books as a conflict, even when evidence suggests otherwise? The answers to these questions go beyond ebooks and reveal much about our mixed feelings of excitement and fear regarding innovation and change. In my research, I have explored how the idea of one medium replacing another often arises with new technologies. Even before digital technology, critics predicted the end of existing media. For example, after television was invented, many believed that radio would disappear. However, radio adapted and found new ways to be used, such as in cars and factories. 
The idea that books might disappear is not new either. As early as 1894, people speculated that the phonograph would replace books with what we now call audiobooks. This pattern has repeated itself many times. Movies, radio, television, hyperlinks, and smartphones have all been thought to threaten print books as sources of culture and entertainment. Some believed that the end of books would lead to cultural decline, while others, imagining a perfect digital future, exaggerated the benefits of ebooks. 
It is no coincidence that the idea of the death of the book appears during times of technological change. This narrative reflects our mixed feelings of hope and fear about new technology. To understand why these feelings are so common, we must recognize that we form emotional connections with different media as they become important in our lives. Many studies show that we develop strong attachments to objects like books, televisions, and computers, even naming our cars or expressing frustration at our laptops. 
Therefore, the arrival of new technology, like e-readers, does not just signal economic and social change; it also forces us to rethink our relationship with something that has become a significant part of our daily lives. As technology evolves, we often find ourselves missing what we once knew but no longer have. This is why entire industries emerge around nostalgic products and older technologies. For instance, the spread of the printing press in 15th-century Europe led people to seek out original manuscripts. The transition from silent to sound movies in the 1920s created a longing for the earlier format. The same nostalgia occurred with the shift from analog to digital photography and from vinyl records to CDs. 
Not surprisingly, e-readers have sparked a renewed appreciation for the physical qualities of traditional books, even their unique smell. This should reassure those who worry about the decline of print books. However, the idea of disappearing media will continue to be an attractive story about the power of technology and our resistance to change. One way we cope with change is by using familiar narrative patterns, such as stories of tragedy and endings. The narrative of the death of the book is easy to remember and share, reflecting both our excitement for the future and our fear of losing parts of our familiar world—and ultimately, ourselves."
6,C2,"
For a year and a half, I woke up every weekday at 5:30 AM, brushed my teeth, made coffee, and wrote about how some of the greatest thinkers of the last four hundred years managed their time to do their best work. I focused on the daily lives of these individuals – when they slept, ate, worked, and worried – to offer a fresh perspective on their personalities and careers. I wanted to show that artists, like everyone else, have routines. The French writer Jean Anthelme Brillat-Savarin once said, “Tell me what you eat, and I shall tell you what you are.” I would add, “Tell me what time you eat and if you take a nap.” 
This book may seem superficial because it looks at the conditions for creativity rather than the creative work itself. However, it is also personal. The novelist John Cheever believed that even a business letter reveals something about the writer. My main concerns in this book reflect my own struggles: How can you do meaningful creative work while making a living? Is it better to focus entirely on one project or to work on many things a little each day? When time is limited, do you have to give up things like sleep or a clean house, or can you learn to do more in less time? I do not claim to answer these questions definitively; some may only be resolved through personal compromise. Instead, I provide examples of how many successful people have faced similar challenges. 
The title of the book is Daily Rituals, but I really focus on people’s routines. Routines can seem ordinary and thoughtless, but they can also be powerful tools for managing limited resources like time, willpower, and optimism. A good routine can help direct our mental energy and reduce the impact of our moods. The psychologist William James believed that we should automate parts of our lives through good habits, allowing us to focus on more interesting activities. Ironically, he struggled with procrastination and maintaining a regular schedule himself. 
This book was inspired by my own procrastination. One Sunday afternoon, while working at a small architecture magazine, I was supposed to write a story due the next day. Instead of working, I found myself cleaning my workspace and making coffee. As a “morning person,” I can concentrate well in the early hours but struggle in the afternoon. To feel better about this, I started looking online for information about other writers’ schedules. I found many interesting stories and thought it would be great to collect them, which led to the Daily Routines blog I started that day and now this book. 
The blog was informal; I shared descriptions of people’s routines from biographies, magazine articles, and obituaries. For the book, I have created a more detailed and researched collection while keeping the variety of voices that made the blog enjoyable. I have let my subjects speak for themselves through quotes from letters, diaries, and interviews. In some cases, I summarized their routines from other sources. I must acknowledge that this book would not have been possible without the work of many biographers, journalists, and scholars. I have listed all my sources in the Notes section, which I hope will guide further reading."
79,C2,"
In recent years, there has been an interesting trend in further and higher education in the UK and around the world. This trend encourages new students to get involved in research as early as possible in their studies. It acknowledges that research is not just for famous scholars at old universities or scientists making new discoveries. Instead, research is a natural way to build knowledge and skills. 
Research skills are valuable not only for your studies but also for your future job. They help you think critically about the world and how to approach your work. As a student, you contribute to knowledge. You do not just memorize and repeat information; you create it. Creating knowledge involves asking questions rather than accepting things as they are. You might wonder: Why? How? When? What does this mean? What if things were different? How does it work in this situation? What should we think about the facts, opinions, or beliefs we encounter? Why is this important? These questions are at the heart of what we call research.
Research can be seen as a range of activities. On one end, there is complex, groundbreaking research done by highly trained experts, which leads to significant changes in knowledge. However, research can also start from simple inquiries that involve careful work and thoughtful questions about various issues, practices, and events. Most students have always been researchers in some way. You have likely done research for school projects or answered questions at work since you began your education. You have asked questions that led you to investigate topics, whether it was planning a holiday, growing plants, fixing things at home, training a pet, or finding the right products online.
In college and higher education, it is expected that you will have a curious mind, identify problems and questions, critically evaluate information, and create your own responses and knowledge. Some students may find this challenging because, in some cultures, knowledge is seen as fixed, and learning often involves listening to teachers and texts without questioning them. It may feel disrespectful to challenge established knowledge or authority, and you might think you should be told what is important to learn. However, in higher education in the UK, US, much of Europe, and Australasia, questioning established knowledge and authorities is encouraged.
This process of inquiry and knowledge creation can seem overwhelming. Critical thinking is especially important in research. The research done by others is valuable for students, academics, and professionals, but we must not simply repeat what we read as if it is always true. We need to engage with the information, think critically about it, test it, and determine if it is logical and supported by evidence. We should avoid blindly accepting facts and information from our readings or conversations."
80,C2,"'Cities have always been centers of intellectual activity, from the coffee houses of 18th-century London, where people discussed science and politics, to the cafés of modern Paris, where artist Pablo Picasso talked about art. However, living in a city is not easy. The same London cafés that encouraged discussion also helped spread diseases like cholera; Picasso eventually moved to the countryside. While modern cities can be creative places, they can also feel unnatural and overwhelming. 
Scientists are now studying how city life affects our brains, and the findings are concerning. It has long been known that city life is tiring, but new research shows that living in cities can actually reduce our ability to think clearly. One major reason for this is the lack of nature in urban environments. Studies have shown that hospital patients recover faster when they can see trees from their windows. Even small views of nature can improve brain function because they provide a break from the stress of city life.
This research comes at a significant time: for the first time in history, more people live in cities than in rural areas. Instead of living in open spaces, we are packed into concrete environments, surrounded by many strangers. It has become clear that these unnatural surroundings can greatly affect our mental and physical health and change how we think. 
Think about everything your brain has to manage while walking down a busy street: distracted people, dangerous crossings that require attention to traffic, and the confusing layout of the city. These seemingly simple tasks can drain our energy because they force our brains to constantly shift focus. A city is filled with so many stimuli that we must continually decide what to pay attention to. This process of managing our focus takes a lot of mental energy. In contrast, natural environments do not require as much cognitive effort. 
This idea is known as attention restoration theory, developed by psychologist Stephen Kaplan. It has been recognized that human attention is limited – focusing in the morning makes it harder to concentrate in the afternoon. Kaplan suggested that being in nature could help restore our attention. Natural settings have elements that naturally attract our focus but do not cause negative feelings, unlike loud noises like police sirens. This allows our mental focus to relax and recharge.
Long before scientists studied this, philosophers and landscape architects warned about the negative effects of city life and sought ways to include nature in urban areas. Urban parks, like Central Park in New York, provide a much-needed escape from city life. A well-designed park can enhance brain function in just a few minutes. While people have tried many methods to boost cognitive performance, such as energy drinks or redesigning office spaces, it seems that simply taking a walk in nature is one of the most effective solutions.
Given the many mental challenges caused by city life, one might wonder: Why do cities keep growing? And why do they remain important centers of intellectual activity, even in the digital age? Recent research from the Santa Fe Institute shows that the same urban features that can lead to attention and memory problems – like crowded streets and close proximity to others – are also linked to innovation. Scientists believe that the ‘concentration of social interactions’ is a key factor in urban creativity. Just as the crowded environment of 18th-century London led to new ideas, the busy atmosphere of 21st-century Cambridge, Massachusetts, supports its role as a hub for technology and creativity. 
The challenge, then, is to find ways to reduce the negative effects of city life while keeping its unique advantages. Because there will always be moments when someone feels, as the song says, ‘I’m tired of the trees, take me to the city!’'"
81,C2,"
Where should we look for the mind? This question may seem strange because we usually think that thinking happens inside our heads. Today, we have advanced brain imaging techniques that support this idea. However, I believe there is no strong reason to limit the study of the mind to just the brain or the body. There is a lot of evidence, from ancient times to now, showing that objects, as well as our brain cells, play a role in how we think. 
From an archaeological perspective, it is clear that items like stone tools, jewelry, carvings, clay tokens, and writing systems have actively contributed to human evolution and the development of our minds. Therefore, I propose that what exists outside our heads may also be part of our minds. It is understandable that the mind and the brain are often seen as the same because most of our knowledge about the mind comes from studying people in isolation from their surroundings. This is practical for neuroscientists, especially when using brain-scanning machines. However, this often overlooks the fact that much of our thinking happens outside our heads. 
I do not want to deny the importance of the brain in thinking, but I want to emphasize that the mind is more than just the brain. It is more useful to consider the idea that human intelligence extends beyond our bodies into culture and the material world. This is where my new theory, called Material Engagement Theory (MET), comes in. MET explores how objects can become extensions of our thinking or be integrated into our actions, like when we create numbers from clay or use a stone to make a tool. It also looks at how these interactions have changed since ancient times and what these changes mean for our thinking.
This approach provides new insights into what minds are and what they consist of by examining how objects assist our thinking. For example, think of a blind person using a stick. Where does this person’s self begin? The connection between the blind person and the stick helps us understand that minds and objects can be seen as connected. It also illustrates the flexibility of the human mind: by using the stick, the blind person transforms touch into a form of sight, and the stick itself plays an important role. The sensation felt through the stick is projected onto the environment, making the brain treat the stick as part of the body.
The blind person’s stick reminds us that human intelligence can adapt and change significantly by incorporating new technologies. I view the human mind as an ongoing project that is always evolving. It is important to remember that, regardless of what form the ‘stick’ has taken throughout our history—whether it is a simple stone tool or a modern smartphone—its main purpose is to connect us to the world rather than to limit us. 
Through the ‘stick’, humans explore, learn, and understand their environment, while also finding new paths forward. This is different from how monkeys use sticks to get food. For humans, ‘sticks’ are tools for satisfying our curiosity. This unique human ability to engage with material culture explains why we create things and how those things shape our minds. I call this metaplasticity—our minds are flexible and change as we interact with the material world. I want to emphasize the importance of material objects in understanding cognition. MET provides a new perspective on how various forms of material culture, from ancient tools to modern devices, have influenced who we are and how we think. While the idea of technology changing our minds may sound futuristic, it is important to realize that humans have been using such technology since our early evolution."
