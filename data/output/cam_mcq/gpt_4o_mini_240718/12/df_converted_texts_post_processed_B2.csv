text_id,text_level,text
1,C2,"
Some time ago, a website warned about the risks of public check-ins, which are online announcements of where you are. The website's message was clear: when you say, ""Hey, I’m at this place,"" you are also telling many people about your location, including those you might not want to meet. This shows that sharing everything online can have negative effects. The internet gives us many chances to share our lives with a global audience, which can seem exciting: wealth and fame are possible! However, we often realize too late that sharing too much can be dangerous and confusing.
But don’t lose hope. There is guidance from early internet users who explored these challenges before us. In the beginning of the web, they faced many difficulties, lost jobs, and dealt with the temptations of fame long before social media existed. These early users, known as bloggers, have valuable experiences to share. It’s important to learn from their stories.
In January 1994, Justin Hall, a 19-year-old student, started posting on the web, which was mostly used by graduate students and scientists. The web was created at CERN, a physics lab in Switzerland, to help researchers share their work. Hall saw a chance to share his own life. He created a personal website filled with his stories, photos, and art. In January 1996, he began a daily blog, and many people were interested in his adventures as he explored this new medium. Hall was open about everything; he would post about anyone who crossed his path. While some saw him as an exhibitionist, others appreciated the art in his work.
One day, visitors to Hall’s site found it changed. His homepage was replaced with a video called ""Dark Night."" In it, he shared that he had fallen in love, but when he wrote about it, he was told to choose between his blog and his relationship. He felt that sharing his life online made people distrust him. The blog ended, but the problem remains: sharing online can be great, but if you think it will make people want to be with you, you might be disappointed.
In 2002, Heather Armstrong, a young worker in Los Angeles, had a blog called Dooce. She sometimes wrote about her job at a software company. One day, a colleague sent her blog link to the company’s vice presidents, including some she had criticized, and she lost her job. Experts call this the ""online distribution effect,"" which means people often say things online that they wouldn’t say in person. However, the internet is not a separate world where we can say anything without consequences. Our online lives are connected to our real lives, and ignoring this can lead to serious mistakes.
Armstrong’s story had a happy ending. Although she was upset and stopped blogging for a while, she later got married and started a new blog about her family. Today, she is successful as a ""mommy blogger,"" and her writing supports her family. Once known for her online mistakes, she has learned to share her life carefully. What Armstrong teaches us is important: the internet lets us say anything, but that doesn’t mean we should."
2,C2,"
Some scientists recently started a campaign to warn people about a low-budget film called *What the Bleep Do We Know?* This film mixes documentary and drama to suggest that there is much about our universe that we do not understand. How can any scientist disagree with that? Scientists can sometimes be arrogant, but even the most enthusiastic physicist would not claim to know everything about the universe. However, some scientists felt it was important to warn the public about the film, calling it everything from ‘atrocious’ to ‘very dangerous.’ This made me curious to see the movie.
At first, I didn’t understand why there was so much concern. Various scientists made simple statements about how new discoveries show that the universe is stranger than we thought. But when the film discussed some of these discoveries, I began to understand the concern. One claim was that water molecules can be influenced by thoughts. I had heard about a Japanese researcher who suggested that the shape of a water molecule could change based on the thoughts of people nearby. However, the film only showed pictures of ice crystals that looked beautiful after being spoken to by happy people and ugly after being exposed to angry people. Many people find this kind of evidence convincing because it is clear and easy to understand. But scientists often respond with skepticism, saying, “Give me a break.” 
I understand their point. The idea that water can be affected by thoughts is fascinating and could mean there are new forces in the universe. But before we get too excited, we need solid proof that this effect is real. Beautiful pictures of crystals are not enough. 
The real issue is that the movie’s claims were not strange enough. For example, water molecules might have properties linked to a mysterious energy that is causing the universe to expand. This idea is supported by decades of research from scientists around the world. In fact, discoveries are being made that show the universe is indeed stranger than we ever thought. Astronomers have found that the universe is made of an unknown type of matter and is driven by a mysterious force called ‘dark energy.’ 
On a more practical level, scientists are making amazing discoveries. Neuroscientists have found that our awareness of events happens about half a second after they actually occur, but we don’t notice this delay because our brains edit it out. Anthropologists believe they have found where modern humans first appeared and how they spread across the world. Some theorists even suggest there are connections between life on Earth and the design of the universe.
Despite what some people might think, science is not close to being complete. In fact, we seem to know less than we once believed. Many natural phenomena may never be fully understood. The ideas of chaos and quantum uncertainty show that there are limits to what we can know. Some of the world’s top physicists are trying to create a Theory of Everything, which would explain all the forces and particles in the universe with one equation. 
In summary, many of us believe that the universe can be described in one word: incredible."
3,C2,"""In simple terms, I find writing novels challenging, while writing short stories is a joy. If writing novels is like planting a forest, then writing short stories is more like planting a garden. Both processes work together to create a beautiful landscape that I value. The trees provide shade, and the wind moves the leaves, which can turn a bright gold. In the garden, flowers bloom, and colorful petals attract bees and butterflies, showing the change of seasons.
Since I started my career as a fiction writer, I have often switched between writing novels and short stories. My routine is this: after finishing a novel, I want to write short stories; after completing a group of short stories, I feel ready to focus on a novel. I never write short stories while working on a novel, and I don’t write a novel while working on short stories. The two types of writing may use different parts of the brain, and it takes time to switch from one to the other.
I began my career with two short novels in 1975, and from 1984 to 1985, I started writing short stories. I knew little about writing short stories then, so it was difficult, but I found the experience memorable. I felt my fictional world expand, and readers seemed to enjoy this different side of me as a writer. One of my early works, ‘Breaking Waves’, was included in my first short-story collection, Tales from Abroad. This was my beginning as a short-story writer.
One of the joys of writing short stories is that they don’t take long to finish. Usually, it takes me about a week to shape a short story (though revisions can take longer). It’s not the same level of commitment as writing a novel, which can take a year or two. For me, writing a novel can feel endless, and I sometimes wonder if I will make it through. So, writing short stories is a nice change of pace.
Another great thing about short stories is that you can create a story from the smallest ideas – a thought, a word, an image, or anything. Often, it feels like jazz improvisation, where the story leads me. Also, with short stories, you don’t have to worry about failing. If an idea doesn’t work out, you can just accept it and move on. Even famous writers like F. Scott Fitzgerald and Raymond Carver didn’t write a masterpiece every time. This gives me comfort. You can learn from your mistakes and use that knowledge in your next story.
When I write novels, I try to learn from both the successes and failures I have in writing short stories. In this way, short stories are like a testing ground for me as a novelist. It is hard to experiment the way I want to within a novel, so without short stories, writing novels would be even more difficult.
My short stories are like soft shadows I have left in the world, faint traces of my thoughts. I remember exactly where I placed each one and how I felt when I did. Short stories are like guideposts to my heart, and it makes me happy as a writer to share these personal feelings with my readers."""
4,C2,"
Science can be very abstract, like philosophy, but it is also very practical, helping to cure diseases. It has made our lives easier but can also pose threats to our existence. Science tries to understand everything from tiny atoms to the vast universe, but it often struggles to do so. It has influenced poets, politicians, and thinkers, and while its beauty is clear to some, its dangers are often misunderstood. People sometimes overestimate or underestimate its importance, and the mistakes made by scientists are often ignored or exaggerated.
The history of science is full of conflict. Old theories are constantly being changed or replaced, similar to how new music styles are sometimes mocked before becoming popular. Scientists can be jealous and angry, and the history of science shows that conflict is common. This book will focus on the ideas in science that have changed not just science itself but also how we think about many things. While science has practical uses, we will mainly explore its ideas and their beauty, while also being aware of the limitations of human understanding.
Science is always changing. There is always a scientist challenging the ideas of another. Most of the time, these changes do not disrupt society, but sometimes they can shake our established beliefs. For example, in the 17th century, science described the universe as a giant clock. Now, physics has questioned many basic ideas, leading us to understand that observing the universe can change it, and we often do not fully understand our own concepts.
Some people think that the changing nature of scientific theories shows that science cannot explain the universe. However, these changes usually help us understand and predict nature better. For instance, Isaac Newton explained more than Aristotle, and Albert Einstein explained even more than Newton. Science may make mistakes, but it continues to progress. 
At the end of the 19th century, many physicists believed there was nothing important left to discover. Then came major breakthroughs like radioactivity, X-rays, and the discovery of the electron. Biology has also made significant advances. Today, some people claim that a complete theory explaining everything about the universe is close. 
Science is not just a harmless activity. In the last two centuries, we have moved from being observers of nature to having some control over it. This has sometimes upset the balance of nature in ways we do not fully understand. It is important for everyone, not just scientists, to understand scientific advances because they will shape the future for our children and the world they will live in. The way we manipulate the future is not just a philosophical question; it can have real effects on budgets, health, and the future of life on Earth."
5,C2,"From around 2015, after many years of growth, major publishers began to notice that ebook sales had stopped increasing or even decreased in some cases. This raised new doubts about the future of ebooks in the publishing industry. One anonymous publishing executive recently admitted that the excitement around ebooks may have led to poor investments, causing his company to lose faith in the value of printed books. Despite the clear idea that digital and print books can exist together, the question of whether ebooks will replace print books still comes up. Whether people are trying to predict or dismiss this idea, it is important to note that the potential loss of printed books continues to spark our imagination and create strong discussions. 
Why is this idea so strong? Why do we see the relationship between ebooks and print books as a battle, even when evidence suggests otherwise? The answers to these questions go beyond ebooks and reveal much about our mixed feelings of excitement and fear regarding change and innovation. In my research, I have explored how the idea of one medium replacing another often appears with new technologies. Even before digital technology, critics predicted the end of existing media. For example, after television was invented, many believed that radio would disappear. However, radio survived by finding new ways to be used, such as listening in cars or during commutes.
The idea of books disappearing is not new either. As early as 1894, people speculated that the phonograph would replace books with what we now call audiobooks. This pattern has repeated many times. Movies, radio, television, hyperlinks, and smartphones have all been thought to threaten print books as sources of culture and entertainment. Some believed that the end of books would lead to cultural decline, while others exaggerated the benefits of ebooks in a hopeful digital future. 
The idea that books might disappear often arises during times of technological change. This narrative reflects our mixed feelings about technology. To understand why we react this way, we must consider that we form emotional connections with different media as they become important in our lives. Many studies show that we develop strong bonds with objects like books, televisions, and computers, even naming our cars or getting frustrated with our laptops. Therefore, when new technology, like e-readers, appears, it not only signals economic and social change but also forces us to rethink our relationship with something that has become a part of our daily lives. As technology evolves, we often miss what we used to know but no longer have. This is why entire industries grow around older products and technologies.
For example, the spread of the printing press in 15th-century Europe made people seek out original manuscripts. The change from silent to sound movies in the 1920s created nostalgia for the older format. The same happened when photography changed from analog to digital and from vinyl records to CDs. Not surprisingly, e-readers have led to a new appreciation for the physical qualities of printed books, even their unique smell. This should reassure those who worry about the decline of print books. However, the idea of disappearing media will continue to be an interesting story about the power of technology and our fear of change. 
One way we try to understand change is by using familiar stories, like those of tragedy and endings. These stories are easy to remember and share, and the narrative of the death of the book reflects both our excitement for the future and our fear of losing parts of our familiar world—and ultimately, ourselves."
6,C2,"
For a year and a half, I woke up every weekday morning at 5:30. I brushed my teeth, made a cup of coffee, and wrote about how some of the greatest thinkers of the last four hundred years managed their time. I wanted to explore how they organized their days to be creative and productive. By writing about the everyday details of their lives—when they slept, ate, worked, and worried—I aimed to show a different side of their personalities and careers. 
The French writer Jean Anthelme Brillat-Savarin once said, “Tell me what you eat, and I shall tell you what you are.” I believe, “Tell me what time you eat, and whether you take a nap afterward.” This book is not just about the results of creative work; it focuses on the process of how that work is done. It is also personal. The novelist John Cheever believed that even a simple business letter reveals something about the writer. 
I face similar questions in my own life: How can you do meaningful creative work while making a living? Is it better to focus entirely on one project or to work on it a little each day? When time is limited, do you have to give up things like sleep or a clean house, or can you learn to do more in less time? I do not claim to answer these questions, as some may only be resolved individually. However, I provide examples of how many successful people have dealt with these challenges. 
The title of the book is Daily Rituals, but I really focus on people’s routines. A routine can seem ordinary and thoughtless, but it can also be a useful tool for managing limited resources like time, willpower, and optimism. A good routine helps direct our mental energy and can prevent us from being controlled by our moods. The psychologist William James believed that having good habits allows us to focus on more interesting activities. Ironically, he struggled with procrastination and could not stick to a regular schedule.
This book was inspired by my own procrastination. One Sunday afternoon, while working at a small architecture magazine, I was supposed to write a story due the next day. Instead of working, I found myself cleaning my workspace and making coffee. I am a “morning person,” able to concentrate well in the early hours but not so much after lunch. To feel better about this, I started looking online for information about other writers’ schedules. I found many interesting stories and thought it would be great to collect them, which led to the Daily Routines blog I started that day and now this book.
The blog was simple; I shared descriptions of people’s routines from biographies, magazine articles, and obituaries. For the book, I have gathered a much larger and better-researched collection while keeping the variety of voices that made the blog enjoyable. I have let my subjects speak for themselves through quotes from their letters, diaries, and interviews. In some cases, I summarized their routines from other sources. I want to acknowledge that this book would not have been possible without the work of many biographers, journalists, and scholars. I have listed all my sources in the Notes section, which can also guide further reading."
7,C1,"
Howard became a palaeontologist because of a change in interest rates when he was six years old. His father, who was careful with money and had a big mortgage, said that their planned holiday to Spain was no longer possible. Instead, they rented a chalet on the English coast. One rainy August afternoon, Howard found an ammonite on the beach. He had always wanted to be a palaeontologist, and by the end of university, he knew what kind he wanted to be. He was not interested in dinosaurs or the Jurassic period; instead, he was fascinated by the very beginnings of life and the ancient creatures found in grey rocks.
After finishing his doctoral thesis, Howard worried about finding a job. He was confident in his abilities, but he knew that hard work does not always lead to success. When a job opened at Tavistock College in London, he applied, but did not expect much. On the day of his interview, the professor who was supposed to lead the panel had a fight with his wife, drove his car into a gatepost, and ended up in the hospital. The interview happened without him, and a colleague who did not like Howard’s former professor took his place on the panel. This colleague helped Howard get the job, even though Howard was surprised by this unexpected support.
At first, Howard was grateful, but later he learned the truth about how he got the job. He felt a little disappointed because he wanted to believe he was chosen for his skills. However, what mattered most was that he had the job and could do the work he loved. He often thought about how organized his professional life was, where he could plan and achieve goals, compared to the chaos of personal life. 
One day, Howard's briefcase, which contained notes for a lecture, was stolen at an Underground station. Angry, he returned to the college, called to explain, and postponed the lecture. After reporting the theft, he went for a coffee and met a colleague who was with a curator from the Natural History Museum in Nairobi. This conversation led Howard to learn about a new collection of fossils that needed to be studied, which would be a great opportunity for him. Because of the theft, he changed his plans. He decided not to go to a conference in Stockholm or take students on a field trip to Scotland. Instead, he would find a way to visit the museum in Nairobi."
8,C1,"Charles Spence is a professor at Oxford University who enjoys trying unusual foods. He mentions that he has ice cream made from bee larvae at home, which may look strange but supposedly tastes a bit nutty and floral. Spence and his team are working on making bug-eating more acceptable. His research focuses on how our senses work together to shape our experience of taste. This research influences what we eat and drink, from large food companies to high-end restaurant menus.
Spence studies many factors that affect how we taste food, such as who we eat with, how food is presented, and even the color and weight of plates and cutlery. His book, The Perfect Meal, co-written with Betina Piqueras-Fiszman, shares interesting facts about food. For example, the first person to order in a restaurant usually enjoys their meal more, and we tend to eat 35% more food when dining with one other person, and 75% more with three others.
Spence's lab is simple and not high-tech. It has soundproof booths and old audio-visual equipment. By keeping costs low, he can work with chefs who cannot afford to fund research. Much of his work is supported by a large food company. In the past, research funded by the food industry was often looked down upon in universities. However, now it is seen as valuable because universities need to show their work has an impact.
Spence helps food brands reduce salt and sugar in their products, which is important for keeping customers healthy. Interestingly, many companies make these changes slowly so that customers do not notice. Spence explains that when people know about these changes, they tend to focus on the taste and may not like it as much.
Spence met famous chef Heston Blumenthal while working on a project for a food company. At that time, people thought combining science and food was strange, but Spence believed it was important. Their collaboration led to the creation of a dish called ""Sound of the Sea,"" which uses sound to enhance the dining experience. Spence notes that even in the early 1900s, artists were experimenting with sounds and food, but it did not become popular then.
Now, the food industry is using Spence's research in many ways. For example, he found that higher-pitched music can make food taste sweeter, while lower-pitched sounds can make it taste bitter. An airline is planning to match music with the food served to passengers, and a well-known brand recently released an app that plays music while ice cream melts, although they did not match the music to the taste.
At home, Spence has had some unique dinner parties. One time, they ate rabbit with the fur still on the cutlery, and another time, they used remote-controlled colored lights. He has even hosted dinners with sound generators and different drinks to see how they affect taste. For Spence, home, sweet shops, food conventions, and gastronomy conferences are all part of his research."
9,C1,"
Our brains are working harder than ever. We are overwhelmed with facts, false information, and rumors that we must sort through to find what is important. At the same time, we are doing more tasks ourselves. Thirty years ago, travel agents booked our flights and salespeople helped us in stores. Now, we handle most of these tasks on our own. We are trying to manage many responsibilities, including our families, jobs, hobbies, and favorite TV shows, often with the help of our smartphones. These devices have become part of our busy lives, filling every free moment with activities.
However, there is a problem. Although we believe we are multitasking—doing several things at once—this is a misleading idea. Earl Miller, a neuroscientist at MIT, explains that our brains are not designed to multitask effectively. When we think we are multitasking, we are actually switching quickly between tasks. Each time we switch, it costs us mental energy. Instead of being like expert jugglers, we are more like amateur plate spinners, moving from one task to another and worrying about what we might forget.
Even though we feel productive, multitasking can make us less efficient. It increases stress hormones in our bodies, which can lead to confusion and difficulty thinking clearly. Multitasking also creates a desire for constant stimulation, making it hard to focus. The part of our brain that helps us concentrate can easily be distracted by new things, like shiny objects that attract attention.
Just having the chance to multitask can hurt our ability to think clearly. Glenn Wilson, a psychologist, found that trying to focus on a task while seeing an unread email can lower our IQ by almost 10 points. He discovered that the mental losses from multitasking are even worse than those caused by being tired. Russ Poldrack, a neuroscientist at Stanford, found that learning while multitasking can send information to the wrong part of the brain. For example, if students do homework while watching TV, the information may not be stored properly, making it harder to remember later.
Additionally, multitasking often involves making decisions, like whether to reply to a text message. Making decisions can be tiring for our brains, and small decisions can use as much mental energy as big ones. After making many small choices, we may struggle to make good decisions about important matters.
In discussions about information overload, email is often mentioned as a problem. It is not that email itself is bad, but the huge amount of messages we receive can be overwhelming. When a 10-year-old boy was asked what his father does, he said, ""He answers emails."" His father agreed that this was quite accurate. We feel we must respond to emails, but it can be hard to do that and accomplish other tasks."
10,C1,"In a zoo in Sweden, a chimpanzee named Santino spent his nights breaking concrete into pieces to throw at visitors during the day. Was he being mean? In caves in the US, female bats help other fruit bat mothers if they can’t find the right position to give birth. Are they being kind? Fifty years ago, these questions were not considered important. Animals had behaviors that led to measurable results, and science focused on those results. The idea that animals have thoughts, feelings, and moral systems was seen as sentimental. However, this view has started to change. Research on the behavior of bats, chimps, rats, dolphins, and chickens has begun to explore animal emotions, which was once a taboo subject. This change has influenced popular science books, like Mark Bekoff’s ""Wild Justice"" and Victoria Braithwaite’s ""Do Fish Feel Pain?"". This has sparked a debate that may never be fully answered: do animals have consciousness? This debate leads to another question about conscience, which is a person’s sense of right and wrong that guides their actions. 
In a recent experiment with cows, it was found that cows that opened a locked gate to get food showed more happiness—by jumping and kicking—than those that had the gate opened for them. If this research suggests that cows enjoy solving problems, what does it mean for how we produce and eat beef? While the observations are not disputed, their meaning is debated. Dr. Jonathan Balcombe, author of ""Second Nature,"" believes that the logical response to this research is to stop eating meat. He thinks humanity is close to a major change in ethics, similar to the end of slavery. Aubrey Manning, a professor at Edinburgh University, believes we should reconsider how we view animal thinking. He argues that animals likely have a simpler understanding of the mind than humans. Professor Euan MacPhail suggests we should avoid attributing human feelings to animals. These three views may never agree because the main issue is not just scientific or moral, but philosophical. Since defining consciousness is very difficult, can we ever truly understand what it is like to be a bat, as philosopher Thomas Nagel asks?
Balcombe describes an important experiment he conducted that seems to show that starlings, a type of bird, can feel depressed. In a study at Newcastle University, starlings were divided into two groups. One group lived in spacious, comfortable cages, while the other group lived in small, empty cages. Both groups were initially fed tasty worms from one box and unpleasant worms from another, and they learned to only take from the tasty box. However, when only unpleasant worms were offered, only the starlings in the comfortable cages would eat. Balcombe concluded that living in a bad cage made the starlings pessimistic about life. Balcombe, who has worked with animal rights groups, has a clear bias. He says, “We look back with horror on a time of racism. Our view of animals will someday be the same. We can’t support animal rights while eating a cheeseburger.” If he were the only one with this view, it might be easy to dismiss him. However, Professor Aubrey Manning shares similar beliefs. Manning has written a textbook called ""An Introduction to Animal Behaviour."" He says, “What we are seeing is a swing in ideas. At the start of the 20th century, some people thought animals thought just like us, and there was a reaction against that. Now we are moving back in that direction. But this is a very controversial topic, and we should be careful of academics with strong personal opinions.”"
11,C1,"
Critical thinking is a way to engage with what we read or hear to understand it better. Adrian West, a research director at the Edward de Bono Foundation in the U.K., believes that arguing can help us find the truth. While technology helps us store and process information, there are worries that it is changing how we solve complex problems and making it harder to think deeply. West points out that we are often exposed to poor but attractive ideas, which can overwhelm our ability to reason. He notes that having more information does not always lead to better knowledge or decision-making. 
According to the National Endowment for the Arts, fewer people are reading literature, and this decline is speeding up. Patricia Greenfield, a psychology professor, thinks that focusing more on visual media, like videos, may be hurting our critical thinking skills. She says that people are multitasking and not concentrating on one thing. However, we still do not have a clear answer on how technology affects critical thinking. 
Technology has changed how we think, and it can have both positive and negative effects. For example, a computer game might help improve critical thinking or make it worse. Reading online can enhance our analytical skills, but constantly clicking on links can prevent deeper thinking. Greenfield has studied how learning and technology interact and found that reading helps develop imagination and critical thinking, while visual media like video games and TV do not engage the imagination in the same way. 
However, she also found that visual media can improve some types of information processing. Unfortunately, most visual media do not give us time to reflect or analyze. This can lead to many young people not reaching their full potential. How society views technology affects how we think about critical thinking. 
James Paul Gee, a professor of educational psychology, points out that video games are often seen as harmful to children. However, he argues that they can be a good learning tool. Evidence shows that playing video games can help children develop better reasoning skills. Games like Sim City and Civilization teach decision-making and analytical skills in ways that feel real. These games also allow players to explore ideas that they might not be able to in real life. 
In today's digital age, as reading and math scores go down, it is important to examine how technology affects our thinking and analysis."
12,C1,"When Matthew Crawford is not writing about how we should live, he works as a motorcycle mechanic. He chose this job after becoming unhappy with office work and his role in social policy. His first book praised the benefits of manual work, while his latest book discusses how to deal with modern life. He was inspired to write it when he noticed advertisements appearing on a credit card machine while he was shopping. Crawford realized that these distractions are hard to avoid. What we want to think about at any moment is personal, but we often cannot decide this for ourselves because of things we do not notice. It is becoming harder to think or remember conversations. To avoid constant interruptions, we often stop talking to strangers. Crawford says, “We increasingly encounter the world through representations that are often manipulative, like video games and apps.” These representations reflect our desires and can take over our lives.
Crawford is not alone in his concerns. Many office workers complain about emails but still spend their free time checking them. Studies show that our attention can wander if a phone is just visible on the table. Although there is no scientific proof yet that our attention spans have changed, we are more aware of other things we could be doing. Some people think this problem is caused by technology, but Crawford believes technology has made it easier for us to focus on ourselves. With so many choices, it is hard to control our attention, which affects our social lives. We often prefer to text friends instead of talking to them. By only interacting with representations of people, we risk losing important connections in society.
Crawford gives an example from his gym. In the past, there was one music player for everyone, which could create tension because people had different tastes. Now, people listen to their own music with earbuds, and the gym has lost its social atmosphere. Real connections often happen through conflict, like discussing different music preferences.
Crawford suggests two solutions. First, we need to manage noise and distractions in public spaces. More importantly, he believes we should engage in skilled activities to connect with the world in a more meaningful way. He mentions cooks, ice-hockey players, and motorcycle racers as examples of people who deal with real materials. No representation can replace the feeling of a hockey puck on ice or gravel under motorcycle tires. These roles require good judgment and the ability to interact with others.
Crawford argues that when we engage with the world this way, we see that manufactured experiences are not as fulfilling as real ones. This does not mean everyone should become a chef, but it is important to use our judgment. There are many benefits to this approach, including professional satisfaction. Constantly fighting distractions can be tiring and make it harder to focus on what is important. In contrast, paying attention to one thing helps us pay attention to others."
13,C1,"
""What do you do for a living?"" This is a common question in small talk, and we often define ourselves by our jobs. However, if you are a philosopher, like me, it can be a bit more complicated. Saying you are a philosopher can sound pretentious. It feels more acceptable to say you study or teach philosophy, but claiming to be a philosopher might suggest you think you know some special truth. This idea is not true; philosophers are just like everyone else. Still, this stereotype makes me think twice before answering.
One reason for this stereotype is that philosophers are seen as people who judge others and value intellectual life. The Greek philosopher Aristotle believed that a life of contemplation, or philosophical thinking, was the best kind of life. While few modern philosophers agree with him, philosophy is still often linked to deep thinking. Another Greek philosopher, Socrates, said that ""the unexamined life is not worth living,"" meaning that simply accepting what society says can lead to an unsatisfying life. Our ability to reflect on the world helps us take control of our lives and make our own choices.
However, living an examined life does not mean you have to read many philosophical books or dedicate your life to deep thinking. It just means paying more attention to the everyday experiences that shape our lives. You don’t have to be a wise person living away from society to evaluate life fairly. In fact, examining life can be very practical and should involve sharing knowledge, as it is important for a good life.
Another reason people misunderstand philosophers is that academic philosophy has become more distant from real-life experiences, especially for those without formal training. This is not entirely the philosophers' fault; university funding and evaluations are often linked to expensive academic journals. Because of this, philosophers have fewer chances to share their ideas with anyone outside their small group of experts. For many people, philosophy can seem disconnected from reality. While some philosophers used to challenge this view, many have accepted it. The academic environment has created a sense of isolation, and as philosophy has become more focused on specific debates, it has become harder for those outside the field to understand.
I sometimes call myself an ""ethicist"" because most of my work is in ethics, which is the part of philosophy that looks at human actions. Recently, I have felt that this title does not fully describe my work, as ethics is often linked to formal rules and laws. There is a new trend in philosophy where ethics is seen as applied ethics, which examines the fairness of specific social practices. The role of an ethicist today is to decide if certain actions are ethical or acceptable. These are important questions that I often explore.
However, philosophy is more than just this. A typical discussion might start by asking if illegally downloading movies is unethical (it is) and then move on to questions about responsibility, our views on art, and the effects of consumerism. In this way, philosophy can help people think more deeply about the actions and behaviors that shape their lives. Sometimes we may confirm what we already believe; other times, we might find that our beliefs are hard to defend. Either way, by examining these ideas, we can benefit everyone."
14,C1,"
People who love food, like foodies, chefs, and gourmands, might think that thinking deeply about what and how we eat can make eating more enjoyable. However, throughout history, discussions about food in philosophy have often been less important than other philosophical topics. When philosophers talk about eating, they often use it as a way to discuss something else, like gaining knowledge. Sometimes, conversations that seem to be about food actually explore different but related ideas.
One interesting example is the ancient Greek philosopher Epicurus. He believed in seeking pleasure and avoiding pain in many areas of life. Although he talked about many activities, his name has become closely linked to enjoying food and drink. You can see his name used in restaurants, food shops, and recipe websites. These businesses likely hope to attract customers by connecting their products to a famous philosopher.
Food is also a social and cultural experience. The food we eat comes from history and is shared with people in our communities. These communities help us find dining partners and provide the systems for growing and distributing food. We interact with food more often and in more important ways than with other products, making it a clear topic for philosophical discussion.
Once food is prepared, critics begin to talk and write about it. But why do these critics have such a special status? One philosopher pointed out that tasting food is not a special skill; rather, good food critics are just better at describing flavors. This area has not been studied much in philosophy, as most research on perception has focused on sight. This should change.
Another important aspect of food is its beauty. We often describe paintings or music as beautiful, but we rarely talk about the taste of food in the same way. Some philosophers believe that, with the growth of modern cooking, food deserves to be discussed in terms of beauty, just like art or poetry. However, food is eaten and disappears, unlike paintings or music, which can be enjoyed over time. Because of this, it is not correct to think of food as a true object of beauty.
There are also many ethical questions related to food. For example, we can ask what we should eat: organic, free-range, locally grown, vegetarian, or non-genetically modified foods? The answers often reflect our ethical beliefs, and philosophers will always question why we choose what we do. This kind of academic discussion about food is important.
Another topic is how cooking in restaurants differs from cooking at home. Home cooks have a special responsibility to their guests because of their personal relationships. In a home kitchen, everyone shares food and companionship. In contrast, professional cooks have responsibilities to their employers and the food itself. Their kitchens are not open to everyone, and their relationships are more like colleagues than friends.
A recent essay called ""Diplomacy of the Dish"" looks at how food and dining can help connect different cultures. This happens in two ways: first, we can learn to appreciate others by enjoying their food, and second, we can build personal connections with people from other cultures by sharing meals. This practice has a long history in international relations. The essay includes many interesting examples and stories that make this part of food philosophy engaging for readers."
15,C1,"Rob Daviau, from the US, creates ‘legacy’ board games. He felt that the traditional board games he played as a child were not exciting or challenging enough, so he began to think about how board games could change. Could they have a story? Could choices made in one game affect future games? He modified a classic game called Risk to create a new version called Risk Legacy. For the first time, decisions made during the game had lasting effects. Players might have to tear up cards, mark the board, or open packets with new rules at important moments. The games are played over a set number of sessions, and long-lasting rivalries become part of the game. Daviau said: ‘You could point to the board and say: “Right here! You did this!”’
Daviau was then invited to work on a legacy version of Pandemic, a very popular cooperative board game where players try to cure diseases that threaten humanity. His next project was a game called SeaFall. While Pandemic Legacy received a lot of praise, SeaFall was seen as a true test of the legacy format because it was the first game that did not come from an earlier version. Set in the age of sailing (16th to mid-19th century), players become sailors exploring a new world. Designers of legacy board games must think about all possible player choices to make sure the story stays strong. To do this, they use testers to see how the games will play out. Jaime Barriga was a tester for SeaFall. He said, ‘It takes a lot of time. The first few games might go well, but then after a few more games, it can start to fall apart, and you have to go back and fix everything.’
Legacy board games were not expected to become very popular. When Daviau was developing the idea, he thought it would appeal to only a small group of people. He said, ‘I thought it was different and pretty good, but it’s unusual and breaks many rules. I expected it to be successful with just a few people. I thought I would be known as the guy who did this strange project.’ However, many players, like Russell Chapman, loved the idea. He believes it is one of the biggest improvements in game design in recent years. He said, ‘It’s a new level of commitment, intensity, and excitement. There’s nothing more thrilling for a board gamer than to learn a new game or a new way to play, and you get that constantly with legacy games.’
Another fan, Ben Hogg, said that the excitement of an unfolding story outweighed any worries about the game lasting a long time. He mentioned, ‘At first, I was worried about making changes and possibly ruining my board, but that concern was eased by Pandemic Legacy. Most people don’t watch the same movie twice in the cinema, right? You’re buying into an experience. It adds a storytelling aspect similar to video games.’ The legacy format is influenced not only by video games but also by the popularity of episodic entertainment from TV series. While in college, Daviau wanted to be a television writer but later moved into advertising and then game design. Still, he loved telling stories.
Pandemic creator Matt Leacock compared the legacy design process to writing a novel. He said, ‘You need to know how you want it to end and have a good idea of where to start, but it’s not enough to just have a general plan.’ While Daviau feels proud of his work, he is curious to see how others will use the idea. He also tries to downplay its importance, thinking that gamers will soon ask, ‘What’s next?’ Colby Dauch, the studio manager for the publisher of SeaFall, is not so sure. He believes the legacy format has changed how people think about board games."
16,C1,"
Not long ago, an octopus named Inky escaped from his tank at New Zealand’s National Aquarium. He crawled across the floor and squeezed into a small drain that led to the Pacific Ocean. This story was exciting and was shared a lot online. People enjoy these escape stories about animals, like rats and llamas, because they like to think of animals as being similar to humans. Octopuses are especially interesting because they are very intelligent and look very different from us. They can do things like open jars, recognize faces, use coconut shells for protection, and even play in clever ways.
Some people think that comparing animals to humans is not scientific. However, Dr. Frans de Waal, who studies animals like gorillas and chimpanzees, believes that it is actually wrong to ignore the human-like qualities of animals. He calls this idea ""anthropodenial."" He has studied many years of research on animal intelligence and found that, except for having a fully developed language, animals show many behaviors that we thought only humans could do. These behaviors include remembering the past and future, showing empathy, and understanding what others might want.
In the past, people believed that animals were smart enough to be put on trial for crimes. Even in the 1800s, many scientists looked for similarities between human and animal intelligence. Charles Darwin, who changed how we see our place in the world, said that the difference between humans and higher animals is one of degree, not kind. 
In the 20th century, a new way of thinking called behaviorism became popular. This view saw animals as machines that respond to rewards and punishments. Many people who thought animals had feelings were seen as unrealistic. This change in thinking happened at the same time that humans were destroying animal habitats and polluting the environment.
Dr. de Waal believes we are starting to understand animal intelligence better now. He says that we are learning to see animal thinking as similar to human thinking, even if they are not the same. He notes that there is a lot of new information about animal intelligence available online.
To test animal intelligence, it is important to consider what each species is good at. For example, squirrels might not do well on human memory tests, but they can remember where they hide their nuts. In her book, The Soul of an Octopus, naturalist Sy Montgomery suggests that if an octopus were to test human intelligence, it might look at how well we can change the colors of our skin. If we failed, the octopus might think we are not very smart.
Dr. de Waal is not completely convinced that Inky’s escape had a happy ending. He thinks it is too hopeful to believe that Inky knew how to find the drain to the ocean. However, he understands that stories like Inky’s help people appreciate animal intelligence. He once did an experiment to see if capuchin monkeys can feel envy. When some monkeys received cucumbers (which they like) and others received grapes (which they like even more), the monkeys with cucumbers got very upset when they saw the others getting grapes. This study was published in a well-known scientific journal, but what really made people believe it was a short video of the experiment released ten years later. This shows how our understanding of animal minds can be surprising."
17,C1,"
Robotics, once just a part of science fiction, is now becoming a major change in technology, similar to what happened during industrialisation. Robots have been used in car manufacturing and other industries for many years, but experts believe we are about to see a big increase in the use of robots in many areas. Many people think that robots will take over most jobs done by humans in the next 50 years. However, about 80% of people also believe their current jobs will still exist in the same way during that time. This shows a common belief that our jobs are safe, but they are not. Every industry will be affected by robots in the coming years.
For example, an Australian company called Fastbrick Robotics has created a robot named Hadrian X that can lay 1,000 bricks in one hour. This task would take two human workers a whole day or more. Another example is Tally, a robot from a San Francisco startup, which moves around supermarkets to check that products are in stock and correctly priced, rather than cleaning the aisles.
Supporters of robotic automation say that robots cannot yet fix or program themselves, which could lead to new jobs for technicians and programmers. However, critics warn that we should not forget the importance of human skills in the workplace. They believe society is not ready for the changes that will come from reducing human interaction.
Dr. Jing Bing Zhang, an expert in robotics, studies how robots are changing the workforce. His recent report predicts that in two years, nearly one-third of robots will be smarter and able to work safely with humans. In three years, many top companies will have a chief robotics officer, and some governments will create laws about robots. In five years, salaries in the robotics field will rise by at least 60%, but many jobs will remain unfilled because there aren’t enough skilled workers.
Zhang says that automation will affect lower-skilled workers, which is unfortunate. He believes that these workers should not wait for the government to protect their jobs but should find ways to retrain themselves. People can no longer expect to do the same job for their entire lives.
At the same time, advances in technology will lead to new types of robots for consumers, such as robots that can walk and live in our homes and interact with us in new ways. Zhang sees this as a great opportunity for companies, but it also brings challenges, like the need for new rules to keep us safe and protect our privacy.
With many jobs at risk and a global employment crisis approaching, it is important to focus on education to prepare for the future workforce that will include robots. Developed countries need more graduates in science, technology, engineering, and maths (STEM) to stay competitive."
18,C1,"George Mallory, a famous mountaineer, is reported to have answered a question about why he wanted to climb Mount Everest with, ""Because it’s there."" This response reflects a common curiosity: why do people take part in such dangerous activities? Some may find inspiration in his answer, as it encourages individuals to pursue their dreams and face challenges. However, the main point is that climbing can simply be about adventure and enjoyment.
In 1967, Bolivian writer Tejada-Flores wrote an important essay called ""Games that Climbers Play."" He described seven different climbing activities, from playing on small rocks to climbing high mountains, each with its own rules. He argued that the way climbers approach a climb depends on the rules of the specific activity. Over the years, this idea has become popular in climbing discussions in the West, appearing in climbing magazines and casual conversations.
Many climbers love the feeling of freedom that climbing brings. However, climbing can also feel restrictive. For example, being stuck in a tent during a storm is not what most people think of as freedom. This creates a paradox in climbing. Yet, some argue that having limited choices can simplify a climber's life, which can also feel like a form of freedom.
When asked ""Why climb?"", US rock climber Joe Fitschen suggests a different question: ""Why do people climb?"" He believes that climbing is part of our nature; humans are built to take on challenges and test their limits, even if it involves risks. Therefore, the joy of climbing is more about our biology than logical reasoning.
US academic Brian Treanor also contributed to this discussion. He believes that climbing can help develop important qualities like courage, humility, and respect for nature. While he acknowledges that not all climbers show these traits, he thinks they are especially important in today's world, which often avoids risks. Climbing, then, can have practical benefits by helping people grow and thrive outside of climbing.
Another interesting idea is that climbers who do not depend on others or technology must be fully dedicated to succeed. Expert climbers Ebert and Robinson stirred some debate by claiming that climbs done independently are more impressive than those completed with large teams or with the help of tools like bottled oxygen. This claim raised concerns that it might encourage climbers to take unnecessary risks.
We can also explore climbing from a different perspective. Many climbers report experiences like being fully present in the moment or going with the flow. The physical effort of climbing, the focused meditation during a climb, and the intuitive problem-solving involved are all key aspects of the activity. Some argue that climbing techniques are similar to those in Zen philosophy, which aims to achieve a state of peace. This offers another idea to consider when thinking about why people are attracted to climbing."
79,C2,"
In recent years, there has been an interesting change in further and higher education in the UK and around the world. More and more, students are being encouraged to get involved in research early in their studies. This change shows that research is not just for famous scholars or scientists, but is a natural way to build knowledge and skills. Research skills are important not only for your studies but also for your future job, as they help you think about the world and how to do your work.
As a student, you contribute to knowledge. You do not just learn and repeat information; you create it. Creating knowledge starts with asking questions instead of just accepting things as they are. You might wonder: Why? How? When? What does this mean? What if things were different? How does it work in this situation? What should we think about the facts, views, or beliefs we are given? Why is this important? These questions are at the heart of what we call research.
Research can be seen as a range of activities. On one end, there is complex and groundbreaking research done by highly trained experts, which leads to significant changes in knowledge. However, research can also start from simple questions and everyday inquiries. This type of research involves careful work, asking thoughtful questions about issues, and making practical suggestions.
Most students have always done some form of research. You have likely conducted research for school projects or answered questions at work since you started. You have asked questions that led you to investigate topics, whether it was about where to go on holiday, how to grow plants, or how to fix things at home. In college and higher education, it is expected that you will have an enquiring mind, identify problems, critically explore information, and create your own knowledge to contribute to discussions.
Some students may find this challenging because, in some cultures, knowledge is seen as fixed. In these places, students learn by listening to teachers and texts without questioning them. It may feel disrespectful to challenge established knowledge or authority. However, in higher education in the UK, US, much of Europe, and Australia, questioning established knowledge and authorities is encouraged.
This process of inquiry and knowledge creation can seem overwhelming. Critical thinking is very important in research. The research done by others is valuable for students and professionals, but we must not just repeat what we read as if it is always true. We need to engage with the information, think critically about it, and check if it is logical and supported by evidence. We should avoid blindly accepting facts and information from our readings or conversations."
80,C2,"
Cities have always been important places for ideas and creativity. In the 18th century, coffee houses in London were popular spots where people discussed science and politics. In modern Paris, cafés on the Left Bank were where artists like Pablo Picasso talked about art. However, living in a city can be challenging. The same cafés that encouraged discussions also helped spread diseases like cholera, and Picasso eventually moved to the countryside. 
Today, cities are known for their creativity, but they can also feel overwhelming and unnatural. Recent studies show that city life can actually make it harder for us to think clearly. One major reason for this is the lack of nature in cities. Research has shown that hospital patients recover faster when they can see trees from their windows. Even small views of nature can help our brains relax and improve our thinking.
For the first time in history, more people live in cities than in rural areas. Instead of being in open spaces, we are often surrounded by many strangers in crowded places. This unnatural environment can affect our mental and physical health and change how we think. For example, when walking down a busy street, our brains have to pay attention to many things: people, traffic, and confusing street layouts. These small tasks can tire us out because cities are full of distractions that require us to constantly focus.
This idea is explained by attention restoration theory, developed by psychologist Stephen Kaplan. He suggested that being in nature can help restore our attention. Natural environments have things that naturally attract our focus without causing stress, unlike loud noises like police sirens. When we are in nature, our minds can relax and recharge.
Long before scientists studied this, philosophers and landscape designers warned about the negative effects of city life and sought ways to bring nature into urban areas. Parks, like Central Park in New York, provide a break from city life. A well-designed park can help our brains function better in just a few minutes. While people have tried many methods to improve mental performance, such as energy drinks or office redesigns, nothing seems to work as well as simply taking a walk in nature.
Despite the mental challenges of city life, cities continue to grow. Research from the Santa Fe Institute shows that the same crowded features of cities that can hurt our attention and memory also encourage innovation. The close social interactions in cities are key to their creativity. Just as the crowded streets of 18th-century London led to new ideas, the busy environment of 21st-century Cambridge, Massachusetts, helps it thrive as a center for technology. On the other hand, less crowded areas may produce fewer new ideas over time.
The challenge is to find ways to reduce the negative effects of city living while keeping its benefits. As the saying goes, there comes a time when someone might say, ""I'm tired of nature; take me to the city!"""
81,C2,"
Where should we look for the mind? This question may seem strange because we usually think that thinking happens inside our heads. Today, we have advanced brain imaging techniques that support this idea. However, I believe that studying the mind should not be limited to just what is inside our bodies. There is a lot of evidence, from ancient times to now, showing that objects, as well as our brain cells, play a role in how we think. 
From an archaeological perspective, items like stone tools, jewelry, carvings, clay tokens, and writing systems have been important in human evolution and the development of our minds. Therefore, what is outside our heads might also be part of our minds. It is easy to connect the mind with the brain because most of what we know about the mind comes from studying people without the objects they usually use. This makes sense for neuroscientists who need to use brain-scanning machines. However, this often overlooks the fact that much of our thinking happens outside our heads. 
I do not want to deny that our brain is important for thinking, but I want to highlight that the mind is more than just the brain. It is better to explore the idea that human intelligence extends beyond our bodies into culture and the material world. This is where my new theory, called Material Engagement Theory (MET), comes in. MET looks at how objects can become extensions of our thinking or be used by our bodies, like when we make symbols from clay or use a stone to create a tool. It also examines how these interactions have changed over time and what these changes mean for our thinking.
This approach gives us new insights into what minds are and how they work by changing our understanding of the role of objects in our thinking. For example, think of a blind person using a stick. Where does this person’s self begin? The connection between the blind person and the stick shows how minds and objects can be linked, and it illustrates the flexibility of the human mind. By using the stick, the blind person can turn touch into a form of sight, and the stick itself plays an important role. The brain treats the stick as part of the body.
The blind person’s stick reminds us that human intelligence can change significantly by using new technologies. I see the human mind as a work in progress, always evolving. It is important to remember that whatever form the “stick” has taken throughout history—whether it is a simple stone tool or a modern smartphone—its main purpose is to connect us to the world, not to separate us from it. 
For humans, “sticks” are tools we use to satisfy our curiosity. This special ability to engage with objects explains why humans create things more than any other species, and how these creations shape our minds. I call this idea metaplasticity, meaning our minds can change and develop as they interact with the material world. I want to emphasize the importance of material objects in understanding the mind. MET provides a new way to see how different types of objects, from ancient tools to modern devices, have helped define and change who we are and how we think. While the idea of technology changing our minds may sound futuristic, it is important to realize that humans have been using such technology since the beginning of our existence."
82,C1,"""Few inventions have changed our way of understanding the world as much as photography. This is especially true in the United States, where photography quickly became part of culture at all levels – from practical and scientific uses to industrial and artistic ones, as well as entertainment. Historians argue that photography is one of the greatest contributions of the US to the visual arts. No other art form has had such a significant impact from the US. This claim shows that photography has become central to American culture, and to understand its impact, we need to look at its beginnings in the mid-19th century.
Why was photography so appealing? First, photography was a mechanical process that matched the growing interest in technology, reflecting a national attitude that accepted change. Just like steam power, railroads, and electricity made communication and travel easier, photography brought the wonders of the world into people's homes in an exciting way. Second, the camera became a popular tool for representing oneself, allowing people to create images for public viewing in a society where personal and national identities were constantly being formed and reformed. Third, the camera has been important for families, providing a way to keep a record of their lives, even if that record is somewhat idealized. Finally, the realistic nature of photographs matched the trend towards realism in the work of US artists.
Every photograph draws attention to something specific, showing what the photographer wants us to see. Because photographs are made by machines, they are records of events, people, or things, which gives them a sense of objectivity. However, since each photograph is taken by a person, it also carries a personal point of view. We might look at a photograph and think we understand its meaning, leading to the idea that photography is a 'universal language.' But the meaning of a photograph is more complicated than that. Few subjects – whether a person, object, or event – have their own clear meaning, and every image comes with some context that shapes our understanding of it, like a caption in a newspaper or its placement in a gallery.
To understand a photograph historically, we need to consider its purpose and function in its cultural context: Why did the photographer take the picture? How did people first see it? Additionally, the same image can be viewed in different places and times, changing its meaning each time. While the importance of the camera to artists was not widely recognized for a long time, it is now clear. In recent decades, starting with artist Andy Warhol's use of photographs in his work, many artists have included photographs in various ways. In summary, photography has become an essential part of art, complementing painting and shaping our ideas of representation before the camera was invented."""
83,C1,"In 1890, William James, an American philosopher and one of the founders of modern psychology, defined psychology as the ""science of mental life."" This definition is still relevant today. We all have a mental life, which gives us some understanding of what this means. Although we can study mental life in animals like rats and monkeys, it is still a complex idea. James focused mainly on human psychology, which he believed included basic elements: thoughts, feelings, the physical world, and a way to understand these elements. Our knowledge about these things is mostly personal and comes from our own experiences, which may or may not be influenced by scientific facts.
Because of this, we often make judgments about psychological issues based on our own experiences. We act like amateur psychologists when we share our opinions on complex topics, such as whether brainwashing works or why people behave in certain ways. Problems can arise when two people understand things differently. Formal psychology aims to provide methods to determine which explanations are most likely correct in different situations. Psychologists help us separate subjective thoughts, which can be biased, from objective facts.
Psychology, as defined by William James, is about the mind and brain. While psychologists study the brain, they do not yet fully understand how it works in relation to our hopes, fears, and behaviors. It is often difficult to study the brain directly, so psychologists learn more by observing behavior and forming hypotheses about what happens inside us. A challenge in psychology is that scientific facts should be objective, but the mind's workings are not easily observable. We can only understand them indirectly through behavior.
Studying psychology is similar to solving a crossword puzzle. It requires evaluating and interpreting clues, which must come from careful observation and accurate measurement. These observations need to be analyzed rigorously and interpreted logically. Psychology often involves complex interactions, and understanding them requires advanced techniques and theories. Like any other science, psychology aims to describe, understand, predict, and learn how to influence the processes it studies. Achieving these goals can help us understand our experiences and apply this knowledge to improve people's lives.
Psychological findings have been useful in many areas, such as improving teaching methods, designing safer machines, and helping people express their feelings better. Although psychological questions have been discussed for centuries, they have only been studied scientifically for the last 150 years. Early psychologists used introspection, or self-reflection, to answer psychological questions. They aimed to identify mental structures, which is still important today. However, introspection has limitations, as noted by Sir Francis Galton, who said it only allows us to see a small part of what the brain does automatically. William James compared trying to understand the mind through introspection to ""turning up the gas quickly enough to see how the darkness looks."" Today, psychologists prefer to base their theories on careful observations of others' behavior rather than on their own experiences."
84,C1,"In a storehouse in a regular business park in Michigan, USA, there is a unique collection known as the Museum of Failed Products. This museum, owned by a company called GfK, displays products that were taken off the market because very few people wanted to buy them. The owner, Carol Sherry, believes that each product has its own story of disappointment for the designers and marketers involved. 
What is surprising is that this museum exists as a successful business. Many companies do not keep samples of their failed products, showing how uncomfortable they are with failure. The museum was started by Robert McMath, who originally wanted to create a library of consumer products, not just failed ones. He began collecting every new item he could find in the 1960s. He soon realized a key truth: most products fail. By collecting all kinds of products, he ended up with mostly unsuccessful ones.
Today’s positive culture may contribute to the products in the museum. Many products likely went through meetings where no one recognized their potential failure. Even if they did, marketers might invest more money into a failing product to save face. There is often little effort to understand why these products failed, and people involved may choose to forget about it.
This focus on optimism is also common in the self-help industry. One popular idea is ""positive visualization,"" which suggests that imagining good outcomes can help make them happen. Research by neuroscientist Tali Sharot shows that a healthy mind may be inclined to see positive outcomes as more likely than they actually are. Her studies suggest that well-adjusted people often have an overly optimistic view of their ability to influence events compared to those who are depressed.
However, how effective are these positive thoughts about the future? Psychologist Gabriele Oettingen and her team have studied this question. Their findings are surprising: thinking too much about how well things could go can actually reduce people's motivation to reach their goals. For example, people who were encouraged to imagine having a very successful week at work ended up achieving less.
Psychologist Carol Dweck explains that our experiences with failure depend on our beliefs about ability. People with a ""fixed mindset"" believe that ability is something you are born with, while those with a ""growth mindset"" think that ability can develop through effort and challenges. Fixed-mindset individuals find failure very frightening because it shows they did not meet expectations. For instance, a talented athlete who believes they are naturally gifted may not practice enough to reach their full potential. In contrast, growth-mindset individuals see failure as a sign that they are pushing their limits. Dweck compares this to weight training, where muscles grow stronger by being challenged. Overall, having a growth mindset is a healthier way to approach life, regardless of whether it leads to success."
85,C1,"
Everyone knows that sports teams have an advantage when they play at home. But why is that? Many people think they understand it, but professional sports are changing quickly, and what we used to believe is now being questioned. Two main factors are making us rethink home advantage: science and money. Sports scientists want to discover what helps players perform their best, and they have come up with many theories about home advantage. At the same time, those who invest in sports want to know why home advantage is still important. If players are paid well, shouldn’t they feel comfortable playing anywhere?
This raises a question for fans. Would it matter if a team like Manchester United played some of their home games in the Far East to reach more fans there? It would matter to British fans, who believe their team needs their support and loyalty. Fans often think that the order of events explains what happens. For example, when a team attacks, the home fans cheer loudly, and sometimes this leads to a goal. Fans remember the times they cheered and a goal was scored, but they forget the many times when their cheering did not lead to success.
However, there is one common belief among fans that is supported by scientific evidence. Home fans not only cheer for their team but also direct a lot of their noise at the referee. In a well-known experiment, referees watched a match with crowd noise and another group watched the same match in silence. The referees who heard the crowd were less likely to call fouls against the home team than those who watched in silence. Interestingly, the crowd noise did not make them more likely to call fouls against the away team. The researchers concluded that referees try to avoid extra stress from making calls against the home team.
Recent studies show that home advantage has decreased in all major sports, but not as much as expected. For example, in the early years of the Football League in the 1890s in England, home teams won about 70% of the points, while today they win about 60%. One reason for this change could be travel. In the past, traveling was difficult, but now players travel in comfort, and if their hotel is not good enough, the club will find them a better one. Stadiums used to be very different from each other, but now many top stadiums look similar and lack character.
Despite these explanations, none fully explains the data. Home advantage varies by sport. Basketball has the most home advantage, followed by football, while baseball has the least. One reason could be that home advantage is more about teamwork, and baseball focuses more on individual matchups between the batter and pitcher. In team sports like basketball, winning depends on teamwork, which builds confidence when playing at home.
Another possibility relates to the players' emotions. Researchers found that players’ testosterone levels were normal before away games but increased by up to 67% before home games. This feeling of defending their home may connect to a basic instinct to protect their territory. In recent Rugby World Cup games, underdog teams playing at home won against stronger opponents, showing the power of home advantage. As one referee said, ""It’s the law of the jungle out there."""
86,C1,"""‘The more I practice, the luckier I get,’ said golfer Gary Player about fifty years ago. This saying is popular among many dedicated athletes and their coaches. The reason for its popularity is clear. First, there is some debate about who originally said it, as many golfers from that time have been suggested. Second, the meaning of the quote is not straightforward. Player did not mean luck in a literal sense; he was being ironic. His real message was: ‘The more I practice, the better I get.’ 
However, there is a limit to this idea. The question of where that limit is, or if it even exists, relates to the nature-nurture debate. This debate discusses whether talent is something we are born with or something we develop through practice. It is not really a balanced debate; nurture (the influence of environment and practice) has clearly won. The idea that practice is more important than natural talent in sports, music, business, and other areas comes from psychologist Anders Ericsson. He is known for the theory that it takes 10,000 hours of practice to become an expert in any field. This theory has been popularized in books like Malcolm Gladwell’s Outliers and others with titles like Talent Is Overrated and Bounce: The Myth of Talent and the Power of Practice. These books generally agree that practice is the most important factor, suggesting that ‘10,000 hours is both necessary and enough to make anyone an expert in anything.’
However, as Epstein discusses in his interesting book, this idea is often not true. He points out a major problem in much of the research on excellence: it usually focuses only on successful people. He asks if there are better ways to understand why some people have certain abilities while others do not, and how to separate natural talent from what is learned through environment, support, and hard work. Epstein travels around the world, from Kenya to Sweden and the Caribbean to the Arctic, to find answers. In Alaska, he discovers that the success of sled dogs in tough races may depend on their drive and desire, suggesting a link between genetics and traits we think are voluntary.
This raises the question: can determination and commitment be chosen, or are they influenced by genetics? Epstein believes that the topic is complex. He makes a distinction between ‘hardware’ (nature) and ‘software’ (nurture), and he agrees that both are important for elite athletes. He does not ignore the role of training or environment. For example, he suggests that if Usain Bolt had grown up in the US, he might have become a good basketball player instead of the fastest man in history. However, Epstein also looks at cases where genetics play a significant role that cannot be overlooked. He also discusses important issues like race and gender. He asks: ‘If only practice matters, why do we separate men and women in sports competitions?’ Sometimes, the best questions are the most obvious."""
87,C1,"
A recent international report shows that many children living in rich, industrialized countries feel lonely and unhappy. Jay Griffiths asks a simple question: why are today’s children so unhappy? In her new book, *Kith*, Griffiths explains that these children spend too much time indoors, stuck in front of screens like televisions and computers, and have lost touch with nature – the woods, mountains, rivers, and streams. She believes this is the main problem.
A follow-up study interviewed some children to understand their feelings better. The study found that many children do not wish for the latest technology; instead, they said they would be happier if they could go outside more. Many adults over 40 remember their own childhoods fondly, saying they enjoyed long summer days exploring, swimming, and building forts. They think this was much better and healthier than the lives of today’s children. However, they often forget that they are the ones who created a safer environment for their kids, which means no building forts in old sheds because of safety concerns.
Griffiths’ book has many strong arguments. She discusses how the fear of dangers outside makes parents keep their children at home, which benefits the toy and gadget industry that sells entertainment for kids stuck indoors. She also criticizes trends like giving medication to restless children or requiring kids to wear goggles during playground games. While it’s unclear how common these rules are, Griffiths expresses her anger about them in a powerful way. She also talks about other important topics related to childhood, like fairy tales and school rules.
However, Griffiths sometimes goes too far with her ideas and ignores important counter-arguments. At one point, she even compares how children are treated today to a form of racism, which seems extreme. Griffiths has a romantic view of children, believing they are naturally good and creative. While she acknowledges that children can be mischievous, she mostly presents an idealized view of them that may not match reality.
One of the main ideas in the book is that children should have the freedom to explore and take risks. Griffiths argues that children need to experience small accidents to learn how to handle bigger dangers later. However, she does not explain how to create these small accidents safely. Additionally, not all children want to explore the wild as Griffiths suggests. What about the shy, quiet children who prefer to stay at home? Perhaps the real issue is that we try to make all children fit into the same mold, whether by keeping them too safe or pushing them too hard to explore."
88,C1,"I am a research bio-psychologist with a PhD, which means I have studied a lot. I am good at solving problems in my work and life, but this skill does not come only from my education. Many problems in life cannot be solved with complicated formulas or memorized answers from school. They need judgment, wisdom, and creativity that come from life experiences. For children, these experiences often happen through play. 
My recent research focuses on how important play is for children's development. All mammals, including humans, play when they are young, and those that have the most to learn tend to play the most. Carnivores, like lions, play more than herbivores, like cows, because hunting is harder to learn than grazing. Primates, like monkeys, play more than other mammals because they rely more on learning than on instincts. Children, who have a lot to learn, play more than any other young primates when they are allowed to.
Play is a natural way for both adults and mammals to learn. The most important skills children need to live happy and productive lives cannot be taught in school. These skills are learned and practiced through play. They include thinking creatively, getting along with others, cooperating, and controlling emotions. Creativity is essential for success in today’s world. We no longer need people who just follow instructions or do simple calculations. We need people who can ask new questions and solve new problems. If we can encourage creative thinkers, we will have a strong workforce.
However, we cannot teach creativity directly. Schooling can sometimes take away creativity by focusing on fixed questions and answers. More importantly, children need to learn how to get along with others, care for them, and cooperate. Children naturally want to play with each other, and through play, they learn social skills, fairness, and morality, which are important for their future.
Play is voluntary, meaning players can choose to stop at any time. If they cannot quit, it is not play. Players understand that to keep the game fun, they must make sure everyone is happy. This freedom makes play a very democratic activity.
Unfortunately, school has become more difficult. There are fewer breaks, more homework, and more pressure for high grades. Outside of school, organized sports have taken the place of spontaneous games. Supervised playdates have replaced unsupervised neighborhood play, and adults often feel they need to step in instead of letting children solve their own problems. These changes have happened slowly but have had a big impact over time. They are caused by various social factors, including parents' fears, warnings from experts about dangers, and the belief that children learn more from adults than from each other.
Our children do not need more school; they need more play. If we care about our children and future generations, we must change the negative trend that has developed over the past fifty years. We must give childhood back to children. They should be allowed to play and explore freely so they can grow into strong adults ready for an unpredictable future."
89,C1,"In many countries, more young people in their twenties are using social media to find jobs. Websites like Twitter and LinkedIn allow them to connect directly with potential employers, which is much easier than the old way of standing outside an office with a sign saying ""hire me."" However, this increased access also means there is a higher chance of making mistakes.
For example, a young job seeker in the US tried to connect with a senior marketing executive on LinkedIn. This executive had many important contacts, and the job seeker thought she could help him. But the executive was upset by the request. She felt it was wrong to share her contacts with someone she didn’t know. Instead of just rejecting his request, she sent a harsh and sarcastic message that became very popular online. Many people were shocked by her response, and she might regret how she handled the situation. However, if this incident makes young people think more carefully about using social media for work, it could actually help them.
Social media can be risky for job seekers who do not know how to use it properly. Many young people are making mistakes. Ironically, social networking sites like Facebook and Twitter have been a big part of young people's social lives for years. When older generations were teenagers, social media was a way to escape from parents and teachers. It was a place to show off and create a different image of themselves. You could have long conversations online and then ignore those people in real life. With the right pictures and songs on Facebook, you could seem like a more interesting person overnight. 
However, using social media for professional networking is very different. Some young people do not see this difference clearly. They may still think that being bold and confident online is a good idea. Just because many people liked your posts on Facebook does not mean you can impress employers on LinkedIn. Young people need to understand that the rules of social networking have changed, and they must meet employers' expectations to succeed in the job market.
One common complaint from employers is that young job seekers are too casual in their messages and come across as arrogant. This reinforces the idea that young people feel entitled. In reality, many young people are struggling to find jobs, which is why they are using social media. This impression of arrogance can hurt their chances, even if they have the skills and motivation to be valuable employees.
So, how should you contact someone on a professional networking site? First, clearly explain who you are and what you can offer them, like doing some research or helping in another way. This approach increases your chances of getting a helpful response. Avoid sending generic emails, and keep your tone respectful to leave a good impression. Remember, social media can be a great way to make important connections, but it needs to be used carefully to avoid closing doors."
90,C1,"""Anyone who claims they can accurately predict the future of newspapers is either lying or mistaken. The numbers show that newspapers are in trouble. Since 2000, the circulation of most UK national daily newspapers has dropped by one-third to one-half. The Pew Research Centre in the USA reports that newspapers are now the main source of news for only 26% of Americans, down from 45% in 2001. Many people confidently predict that the last printed newspaper will disappear within 15 years. However, history shows that old media often survive. In 1835, a New York journalist claimed that books and theatre were finished and that newspapers would become the most important source of news. Yet, theatre survived not only newspapers but also cinema and television. Radio has continued to thrive even in the age of TV, and cinema has remained popular despite the rise of videos and DVDs. Even vinyl records have made a comeback, with online sales increasing by 745% since 2008. 
Newspapers were once considered new media, but it took centuries for them to become the main source of news. This change happened because producing timely news for a large audience became possible and affordable in the mid-19th century, thanks to the steam press, railways, and the telegraph. It was also important that people began to understand that everything around them was constantly changing, and they needed regular updates. This idea was not common in medieval times, when people mainly noticed the changing seasons and unexpected disasters like famine or disease. Life was seen as a cycle, with repeating events. 
Before the 19th century, journalism as a profession was not well established, and there was no clear need for people to receive news regularly. In some ways, the regular publication of newspapers can be a limitation. Online news allows readers to choose when to read based on the urgency of events. Advanced search engines and algorithms help us customize the news to fit our interests. When important news happens, online news providers can give updates almost instantly. There are no space limits that restrict storytelling or analysis, and readers can often access full documents or events mentioned in news stories. This is very different from the limitations of newspapers. 
However, many news providers do not seem to recognize the potential of the internet to improve understanding. Instead, they focus on being the first to report news, increasing reader comments, and creating excitement, which can lead to confusion. In medieval times, news was often shared in busy marketplaces or taverns, where truth competed with rumors and misunderstandings. In some ways, we seem to be returning to that situation. Newspapers have never been very good at explaining how the world works. They may be facing extinction, or perhaps, as the internet adds to our feeling of living in a chaotic world, newspapers will find a way to help us gain wisdom and understanding."""
91,C1,"
If humans were truly comfortable under the moon and stars, we would walk in the dark happily, just like many animals that thrive at night. However, we are daytime creatures, with eyes that are made for sunlight. This fact is deeply rooted in our genes, even if we don’t often think about being daytime beings, just like we don’t think about being primates or mammals. But this is the only way to understand what we have done to the night. We have changed the night by filling it with artificial light. This control is similar to building a dam on a river. While there are benefits to this, it also leads to problems known as light pollution, which scientists are just starting to study. 
Light pollution mainly comes from poor lighting design, which allows artificial light to shine into the sky instead of focusing it down where it is needed. Bad lighting brightens the night, changing the light levels and patterns that many living things, including us, have adapted to. Wherever human-made light spreads into nature, it affects important life activities like migration, reproduction, and feeding. 
For most of human history, the term ‘light pollution’ would not have made sense. Imagine walking towards London on a moonlit night around 1800, when it was the most populated city on Earth. Nearly a million people lived there, using candles, torches, and lanterns. Only a few homes had gas lights, and there were no public gas lights in the streets for another seven years. From a distance, you would have been more likely to smell London than to see its faint glow. 
We have brightened the night as if it were empty, but that is far from the truth. There are many nocturnal species among mammals. Light is a strong biological force that attracts many species. The effect is so strong that scientists say songbirds and seabirds can be ‘captured’ by searchlights or gas flares, circling until they fall. Birds that migrate at night often crash into brightly lit tall buildings, and young birds on their first journey are especially affected. Some birds, like blackbirds and nightingales, sing at odd hours because of artificial light. 
It was once believed that light pollution only bothered astronomers, who need to see the night sky clearly. While most of us may not need a perfect view of the night sky for our work, we still need darkness like other creatures do. Rejecting darkness is pointless. It is just as important for our health as light is; changing our internal clock can lead to health problems. The regular pattern of waking and sleeping is a biological reflection of the light changes on Earth. These patterns are so important to our existence that disrupting them is like changing our center of gravity. 
In the end, humans are just as affected by light pollution as frogs living near a bright highway. By living in our own bright world, we have disconnected ourselves from our natural and cultural history – the light of the stars and the natural rhythms of day and night. Light pollution makes us forget our true place in the universe and the scale of our existence, which is best understood against the backdrop of a dark night with the Milky Way shining above us."
92,C1,"
The founder of a large international company recently said that his business will no longer track how much paid holiday time employees take. This decision seems to be inspired by a similar policy from a smaller internet company. The founder mentioned that he got the idea from a cheerful email from his daughter, which many newspapers have shared. However, this way of announcing the change seems like an attempt to make the policy seem friendlier, even though it might not be very generous.
Is this idea practical? The internet company has 2,000 employees and offers one service, while the multinational corporation has 50,000 employees and many different services, such as finance, transport, and healthcare. The idea of ""take as much time off as you want"" might work better in a smaller company where employees know each other's workloads and can judge if their absence would hurt the business. The founder said employees can take as much leave as they want, as long as they feel sure that their team is up to date on projects and that their absence won't harm the business or their careers. But can anyone be that sure? No matter how much work you finish before a holiday, there will always be more waiting when you return. 
If employees follow these guidelines, they might not take any leave at all, or they could feel guilty about taking time off. This guilt can lead to stress, and if workers do not take enough leave, it could lower productivity over time. There may also be pressure from colleagues and gossip about who is off work and for how long. In many companies, there is a culture of working late, which could lead to a ""no holiday"" culture in a place with unlimited leave, where employees compete for promotions. 
If the security and rights that come with statutory leave are removed, workers might feel they cannot take the time off they need for fear of being seen as lazy. They would lose their legal rights to rely on. This policy could create a situation where employees do not feel they can take their entitled leave, or they might still use their legal rights as a guide, making the new policy useless. 
Modern technology allows us to receive work messages anytime and anywhere, which makes it hard to separate work from personal time. The internet company started its unlimited leave policy when employees asked how this new way of working could fit with the old time-off policy. If the company cannot track how much time employees work, why should it have a different standard for time off? 
However, a problem with having no set working hours is that all hours could become working hours. Employees might never know if their working hours are being watched by their employer, which could lead them to self-discipline in unhealthy ways. Employment laws exist for a reason. Workers have the right to a minimum amount of paid annual leave because rest and leisure are important for their mental and physical health. The benefits of increased morale, creativity, and productivity that the unlimited leave policy aims for can happen without sacrificing worker well-being. Therefore, I am not sure if allowing employees to ""take as much holiday as they want"" is truly the goal or likely result of this policy."
93,C1,"
Peer review is the process where experts check a scientific research paper before it is published. It is seen as a way to ensure the quality of research. Many scientists believe it is important because it helps prevent the publication of poor-quality papers. However, this process can delay publication by up to a year. Is this delay worth it to ensure the quality of published research? The answer is both yes and no. 
I still believe in peer review, but I see changes happening in scientific publishing. One important change is the use of preprints. Preprints are drafts of papers that are shared online before they have been peer-reviewed. This allows researchers to share new findings quickly so that others can read and discuss them. 
Publishing in journals has become more about gaining recognition and advancing careers, which can affect the motivations of authors and reviewers. The competition to publish in top journals can lead scientists to produce excellent work, but it can also encourage them to cut corners. Reviewers may focus more on whether a paper is good enough for a specific journal rather than its overall quality. For top journals, the decision can depend on how interesting or newsworthy the research is, rather than just its scientific merit.
Many people know about these problems, but few are willing to change the current system. However, some experts, like biologist Ron Vale, believe that preprints could help solve these issues without completely changing the way things are done. Although preprints have been around for twenty years, they are not widely accepted yet. This is partly because many scientists believe that journals will not accept papers that have been shared as preprints. There is also a concern that publishing papers without peer review could lead to poor-quality research, but this has not happened so far.
Preprints may not be peer-reviewed, but authors know that their work will be open to feedback and discussion from a global community of researchers. For example, psychology professor Tanya Elks shared her experience with a preprint. She wrote a critique of another published paper, which is often difficult to do in traditional journals. By posting a preprint, the original authors could respond to her critique, and all comments were available for everyone to see. This way, readers can judge the quality of the arguments themselves. 
Preprint archives allow for informal scientific discussions that used to happen only between individuals. They can also provide a place to share negative results, which are often ignored by journals that focus only on new discoveries. Additionally, papers on preprint archives are read and cited more often, showing that sharing research this way is effective. By using the internet's openness and encouraging collaboration, preprints can help shift the focus back to the quality of the research itself, rather than where it is published."
94,C1,"
When I ask my literature students what a poem is, they often say things like ""a painting in words."" Their answers usually do not satisfy me or them. One day, I asked a group to pick an object and write two paragraphs about it. The first paragraph was a scientific description, and the second was from the object's point of view, titled ""Poem."" One student wrote: 
**Poem**  
I may look strange or scary, but I’m a device that helps people breathe. I’m only used in emergencies and for a short time. Most people will never need to use me. The object? An oxygen mask – an unusual choice that helped the class understand how poetry works in a unique way. This exercise led to laughter and helpful discussions.
When I was in school, poetry often confused me. I thought every poem was a silly puzzle that made it hard to understand and feel. After school, most people pay less attention to poetry. Sometimes you see a poem, and it stands out because it is not continuous prose. It challenges you to read it, but often you feel let down by its simplicity or because you don’t understand it at first. Still, you feel good for trying.
What do we hope to find in poems? Isn’t a poem a place for deep feelings, beautiful images, thoughtful reflections, or sharp humor? The answer seems to be yes. But if we want tears, we watch movies; for information or sharp criticism, we read online articles. Novels let us escape, paintings please our eyes, and music – well, nothing can compare to the beauty of lyrics, instruments, and melody.
However, one useful thing a poem can offer is ambiguity. Everyday life is full of uncertainty, unlike reading sentence by sentence. But these thoughts still don’t explain what a poem really is. If you search online for ""poem,"" it leads you to ""poetry,"" which is described as ""a form of literary art that uses the beauty and rhythm of language."" This sounds nice, but it doesn’t explain the word’s origins. ""Poem"" comes from the Greek word poí?ma, meaning ""a thing made,"" and a poet is someone who ""makes things."" So, if a poem is a thing made, what kind of thing is it?
Poets sometimes compare poems to wild animals – untameable and unpredictable – or to machines – carefully designed and exact – depending on their views. But these comparisons often break down when examined closely. The most valuable part of trying to define a poem through comparison is not the comparison itself but the discussion it creates. Whether you see a poem as a machine or a wild animal, this process can change how you think about both. It can help us rethink our usual ways of thinking and see ordinary things in a new way.
A poem as a mental object is easy to understand, especially when we think about how song lyrics can stick in our minds. The mix of words and melody has a strong effect, going back to schoolyard rhymes like ""Sticks and stones may break my bones, but words can never hurt me."" But aren’t words sometimes like sticks and stones? Think about a poem on a page of a newspaper or magazine, right in front of you: A poem can touch you like nothing else, even though it is just ink on paper, like the prose around it. 
What about all the empty space around the poem – space that could have been used for a longer article or an ad? A poem is written and rewritten just like an article, story, or novel, but it is not usually made to be sold. Publishers write press releases and send out review copies of poetry collections, but few expect them to make money. A poem is not just a product for the market; it exists for its own sake. Because of its special place in a magazine or book, a poem can still surprise us, even if just for a moment."
