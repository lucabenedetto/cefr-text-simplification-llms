text_id,text_level,text
1,C2,"Some time ago, a website warned about the dangers of public check-ins, which are online posts that show where you are. The website's message was clear: when you say, ""Hey, I’m at this place,"" you are also telling everyone where you are, including people you might not want to meet. This reflects a growing understanding that sharing everything online can have negative effects. The internet gives us many chances to share our lives with a global audience, which can seem exciting: wealth, fame, and more. However, we often realize too late that the online world can be crowded and dangerous.
But don’t lose hope. There is guidance from early internet users who explored these challenges before us. In the beginning of the web, they faced many difficulties, lost jobs, and dealt with the temptations of fame, long before social media existed. These early bloggers experienced what many of us are now going through. It’s important to learn from their experiences.
In January 1994, Justin Hall, a 19-year-old student, started sharing his life online. At that time, the web was mostly used by graduate students and scientists. Hall saw a chance to share his personal story. He created a blog filled with his thoughts, photos, and art. In January 1996, he began posting daily, attracting many readers who were fascinated by his boldness. Hall was open about everything; no topic was off-limits. While some might see this as attention-seeking, there was also a creative side to his work.
However, one day, visitors to his blog found it replaced by a video called Dark Night. Hall shared that he had fallen in love, but when he wrote about it, he was told to choose between his blog and his relationship. He felt that sharing his life online made people distrust him. The blog ended, but the issue remains: sharing online can be great, but if you think it will make people want to be around you, you might be disappointed.
In 2002, Heather Armstrong, a young woman in Los Angeles, had a blog called Dooce. She sometimes wrote about her job at a software company. One day, a colleague sent her blog link to the company’s vice presidents, including some she had criticized, and she lost her job. This situation shows the ""online distribution effect,"" where people feel they can say things online that they wouldn’t say in person. However, the internet is not a separate reality; our online actions can have real-life consequences.
Armstrong’s story had a happy ending. Although she was upset and stopped blogging for a while, she later got married and started a new blog about her family. Today, she is successful as a ""mommy blogger,"" and her writing supports her family. Once known for her online mistakes, she has learned to share her life more carefully. Armstrong’s experience teaches us an important lesson: the internet allows us to say anything, but that doesn’t mean we should."
2,C2,"Some scientists recently started a campaign to warn people about a low-budget film called *What the Bleep Do We Know?* This film mixes documentary and drama to show that there is much we do not understand about our universe. While scientists can sometimes be arrogant, even the most confident physicist would not claim to know everything about the cosmos. However, some scientists felt it was important to warn the public about the film, calling it everything from “atrocious” to “very dangerous.” This made me curious to see the movie.
At first, I didn’t understand why there was so much concern. Various scientists made simple statements about how new discoveries are showing that the universe is stranger than we thought. But when the film started to explain some of these discoveries, I began to understand the fuss. One claim was that water molecules can be influenced by thoughts. I had heard before about a Japanese researcher who suggested that the shape of a water molecule could change based on the feelings of people nearby. However, the film only showed pictures of ice crystals that looked beautiful after being spoken to nicely and ugly after being exposed to negative emotions. Many people find this kind of evidence convincing, but scientists often respond with skepticism.
I understand their point. The idea that thoughts can affect water is fascinating and suggests new forces in the universe. But before we get too excited, we need solid proof that this effect is real. Pretty pictures of crystals are not enough. The real issue is that the movie’s claims were not strange enough. For example, water molecules might have properties linked to a mysterious energy that is causing the universe to expand. This idea is supported by decades of research from scientists around the world.
In fact, discoveries are being made that show the universe is indeed stranger than we ever imagined. Astronomers have found that the universe is made of an unknown type of matter and is driven by a mysterious force called “dark energy.” On a more practical level, neuroscientists have discovered that our awareness of events happens about half a second after they occur, a delay we don’t notice because our brains edit it out. Anthropologists believe they have found where modern humans first appeared and how they spread across the world. Some theorists even suggest there are connections between life on Earth and the fundamental design of the universe.
Despite what some may think, science is not close to being complete. In fact, we seem to be further from knowing everything than ever before. Many natural phenomena may never be fully understood. The ideas of chaos and quantum uncertainty show that there are limits to what we can know. Some of the world’s top physicists are trying to create a “Theory of Everything” that would explain all the forces and particles in the universe with one equation. Overall, many of us believe that the universe can be described in one word: incredible."
3,C2,"To put it simply, I find writing novels challenging, but writing short stories is a joy. If writing novels is like planting a forest, then writing short stories is more like planting a garden. Both processes work together to create a beautiful landscape that I love. The trees provide shade, and the wind moves the leaves, which can turn a bright gold. In the garden, flowers bloom, and their colorful petals attract bees and butterflies, showing us the gentle change of seasons.
Since I started my career as a fiction writer, I have often switched between writing novels and short stories. My routine is this: after finishing a novel, I want to write short stories; after completing a group of short stories, I feel ready to focus on a novel. I never write short stories while working on a novel, and I never write a novel while working on short stories. The two types of writing may use different parts of my brain, and it takes time to switch from one to the other.
I began my career with two short novels in 1975, and from 1984 to 1985, I started writing short stories. I didn’t know much about writing short stories then, so it was difficult, but I found the experience very memorable. I felt my fictional world expand, and readers seemed to enjoy this different side of me as a writer. One of my early works, “Breaking Waves,” was included in my first short-story collection, Tales from Abroad. This was my beginning as a short-story writer.
One of the joys of writing short stories is that they don’t take long to finish. Usually, it takes me about a week to shape a short story (though I can revise it many times). It’s not the same level of commitment as writing a novel, which can take a year or two. I can go into a room, finish my work, and leave. Writing a novel can feel like it goes on forever, and I sometimes wonder if I will make it through. So, writing short stories is a nice change of pace.
Another great thing about short stories is that you can create a story from the smallest ideas—like a thought, a word, or an image. Often, it feels like jazz improvisation, where the story takes me where it wants to go. Plus, with short stories, you don’t have to worry about failing. If an idea doesn’t work out, you can just accept it and move on. Even famous writers like F. Scott Fitzgerald and Raymond Carver didn’t write a masterpiece every time. This gives me comfort. You can learn from your mistakes and use that knowledge in your next story.
When I write novels, I try to learn from both the successes and failures I have in writing short stories. In this way, short stories are like a testing ground for me as a novelist. It is hard to experiment the way I want to in a novel, so without short stories, writing novels would be even more difficult.
My short stories are like soft shadows I have left in the world, gentle reminders of my feelings when I wrote them. They guide me, and it makes me happy as a writer to share these personal feelings with my readers."
4,C2,"At its core, science is closely related to philosophy, but it also has practical uses, like curing diseases. Science has made our lives easier but has also posed threats to our existence. It tries to understand everything from tiny ants to the vast universe, but it often struggles to do so. Science has influenced poets, politicians, philosophers, and even frauds. Its beauty is often seen only by those who study it deeply, while its dangers are often misunderstood. People have both overestimated and underestimated its importance, and the mistakes made by scientists are sometimes ignored or exaggerated.
The history of science is filled with conflict. Old theories are often changed or completely replaced, similar to how new music styles are sometimes mocked before becoming popular. Scientists can be jealous and angry, and conflict is a big part of scientific history. This book will show how scientific ideas have changed not just science itself but also many areas of human thought. While science has practical benefits, this book will focus more on the ideas behind it, appreciating their beauty and being ready to question them. We will recognize both the creativity and the limitations of the human mind.
Science is always changing. There is always a scientist challenging the ideas of another. Most of the time, these changes do not disrupt society, but sometimes they can shake our established beliefs. For example, in the 17th century, science described the universe as a giant clock. Now, physics has raised questions about basic ideas, leading us to understand that observing the universe can change it, and we often do not fully understand our own concepts.
Some people think that the changing nature of scientific theories shows that science cannot explain the universe. However, these changes usually help us understand and predict nature better. For instance, Isaac Newton explained more than the ancient Greek thinker Aristotle, and Albert Einstein explained even more than Newton. Science may make mistakes, but it continues to progress. 
At the end of the 19th century, many physicists believed there was nothing important left to discover. Then came breakthroughs like radioactivity, X-rays, the electron, and many new particles, along with concepts like quantum mechanics and black holes. Biology has also made significant discoveries. Today, some people claim that a complete theory explaining everything about the universe is coming soon. 
Science is not just a harmless activity. In the last 200 years, we have moved from simply observing nature to controlling it in small but growing ways. Sometimes, we have disrupted nature in ways we did not fully understand. It is important for everyone, not just scientists, to understand scientific advances because they will shape the world for future generations. Science is now a key part of how humanity thinks about and shapes its future. The decisions made in science can impact budgets, health, and even the future of life on Earth."
5,C2,"From around 2015, after many years of growth, major publishers began to notice that ebook sales had stopped increasing or even decreased in some cases. This raised new questions about the future of ebooks in the publishing industry. One anonymous publishing executive recently admitted that the excitement around ebooks may have led to poor investments, causing his company to lose faith in the value of printed books. Despite the clear idea that digital and print books can exist together, people still wonder if ebooks will replace printed books. Whether people are trying to predict or dismiss this idea, it is important to note that the thought of books disappearing continues to spark our imagination and create strong discussions.
Why is this idea so strong? Why do we see the relationship between ebooks and printed books as a battle, even when evidence shows they can coexist? The answers to these questions go beyond ebooks and reveal much about our mixed feelings of excitement and fear regarding change and innovation. In my research, I have shown that the idea of one type of media replacing another often comes up when new technologies are introduced. For example, after television was invented, many believed that radio would disappear. However, radio survived by finding new ways to be used, like listening in cars or on trains.
The idea that books might disappear is not new either. As early as 1894, people thought that the phonograph would replace books with what we now call audiobooks. This pattern has repeated many times. Movies, radio, television, hyperlinks, and smartphones have all been said to threaten the existence of printed books. Some people worried that the end of books would lead to a decline in culture, while others imagined a perfect future with ebooks and exaggerated their benefits.
The idea of books disappearing often appears during times of technological change. This story reflects our hopes and fears about new technology. To understand why we react this way, we need to recognize that we form emotional connections with different types of media as they become important in our lives. Many studies show that we develop strong feelings for objects like books, televisions, and computers, even naming our cars or getting frustrated with our laptops when they don’t work.
When new technology, like e-readers, appears, it doesn’t just mean economic and social change. It also forces us to rethink our relationship with things that have become part of our daily lives. As technology evolves, we often miss what we used to know but no longer have. This is why industries grow around older products and technologies. For example, when the printing press spread in 15th-century Europe, people began to seek out original manuscripts. The change from silent to sound movies in the 1920s made people nostalgic for the older style. The same happened when photography changed from analog to digital and from vinyl records to CDs. 
E-readers have also led to a new appreciation for the physical qualities of printed books, even their unique smell. This should comfort those who worry about printed books disappearing. However, the idea of a medium disappearing will continue to be an interesting story about the power of technology and our fear of change. One way we cope with change is by using familiar stories, like those of tragedy and endings. The story of the death of the book is easy to remember and share, reflecting both our excitement for the future and our fear of losing parts of our familiar world—and ultimately, ourselves."
6,C2,"
For a year and a half, I woke up every weekday at 5:30 AM. I brushed my teeth, made coffee, and wrote about how some of the greatest thinkers of the last four hundred years managed their time to do their best work. I focused on the everyday details of their lives, like when they slept, ate, and worked, to show their personalities and careers. The French writer Jean Anthelme Brillat-Savarin once said, “Tell me what you eat, and I shall tell you what you are.” I believe, “Tell me what time you eat, and whether you take a nap afterward.” 
This book is not just about the creative work itself, but about the daily routines that help people be productive. I also share my personal struggles: How can you do meaningful creative work while making a living? Is it better to focus completely on one project or to work a little each day? When time is short, do you have to give up things like sleep or a clean house, or can you learn to do more in less time? I don’t claim to answer these questions, but I provide examples of how many successful people have faced similar challenges. 
The title of the book is Daily Rituals, but I really focus on people’s routines. Routines can seem ordinary, but they can also help us use our time, willpower, and optimism effectively. A good routine can help us stay focused and avoid being influenced by our moods. The psychologist William James believed that having good habits allows us to focus on more interesting things. Ironically, he struggled with procrastination and could not stick to a regular schedule.
This book started because of my own procrastination. One Sunday, while trying to write a story for a small architecture magazine, I found myself cleaning my workspace and making coffee instead of working. I am a “morning person” and can concentrate well in the morning, but I struggle in the afternoon. To feel better about this, I began searching online for information about other writers’ schedules. I thought it would be interesting to collect these stories, which led to the Daily Routines blog I started that day, and now this book.
The blog was simple; I shared descriptions of people’s routines from biographies and articles. For the book, I have gathered a much larger and better-researched collection while keeping the variety of voices that made the blog enjoyable. I let my subjects speak for themselves through quotes from their letters, diaries, and interviews. In some cases, I summarized their routines from other sources. I want to acknowledge the many biographers, journalists, and scholars whose research helped me. I have listed all my sources in the Notes section for further reading."
7,C1,"Howard became a palaeontologist because of a change in interest rates when he was six years old. His father, who was careful with money and had a big loan, said that their planned holiday to Spain was no longer possible. Instead, they rented a chalet on the English coast. One rainy August afternoon, Howard found an ammonite on the beach. He had always wanted to be a palaeontologist, and by the end of university, he knew what kind he wanted to be. He was interested in the very early history of life, not in dinosaurs or the Jurassic period. He studied small creatures found in grey rocks.
When he finished his doctoral thesis, he wondered if he would get a job, especially in the kind of place he wanted. He was confident in his abilities, but he knew that hard work does not always lead to success. When a job opened up at Tavistock College in London, he applied, but did not expect much.
On the day of his interview, the professor who was supposed to lead the panel had a fight with his wife. He left home upset, crashed his car into a gatepost, and ended up in the hospital. The interview went ahead without him, and a colleague who did not like the professor took his place. This colleague helped Howard get the job, even though Howard did not know him. At first, Howard was grateful, but later he learned the truth about how he got the job. He was a bit disappointed, wishing he could believe he was chosen for his skills. However, what mattered most was that he had the job and could do the work he loved.
Howard often thought about how organized his professional life was, where he could plan and achieve goals, compared to the chaos of personal life. Sometimes, strangers could unexpectedly change everything, like when his briefcase with important lecture notes was stolen at a train station. Angry, Howard went back to college, called to postpone his lecture, and reported the theft. He then went for coffee and met a colleague who was with a visitor from the Natural History Museum in Nairobi. This conversation led Howard to learn about a new collection of fossils that needed to be studied, which would be a great opportunity for him.
Because of the theft, Howard changed his plans. He decided not to go to a conference in Stockholm or take students on a trip to Scotland. Instead, he would find a way to visit the museum in Nairobi."
8,C1,"Charles Spence is a professor at Oxford University who enjoys trying unusual foods. He mentions that he has ice cream made from bee larvae at home, which may look strange but supposedly tastes a bit like nuts and flowers. Spence and his team are working on making eating bugs more acceptable. His research focuses on how our senses work together to create the flavors we experience. This research influences what we eat and drink, from large food companies to high-end restaurants.
Spence studies many factors that affect our taste, such as who we eat with, how food is presented, and even the noise around us. In his book, ""The Perfect Meal,"" co-written with Betina Piqueras-Fiszman, he shares interesting facts about eating. For example, the first person to order in a group usually enjoys their meal more, and we tend to eat 35% more food when dining with one other person, and 75% more with three others.
Spence's lab is simple and not very modern. It has soundproof booths and old audio-visual equipment. By keeping costs low, he can work with chefs who cannot afford research on their own. Much of his funding comes from a large food company. He notes that in the past, research funded by the food industry was not taken seriously in universities. However, now it is important for universities to show that their work has real-world effects.
Spence helps food brands reduce salt and sugar in their products, which is important for keeping customers healthy. Surprisingly, many companies make these changes slowly so that customers do not notice. He explains that when people know about these changes, they often focus too much on the taste and may not enjoy it as much.
Spence first met famous chef Heston Blumenthal while working on a project for a food company. At that time, people thought combining science and food was strange, but Spence believed it could change how people think about food. Their collaboration led to the creation of a dish called ""Sound of the Sea,"" which uses sound to enhance the dining experience.
Spence has found that different sounds can change how we taste food. For example, high-pitched music can make food taste sweeter, while lower sounds can make it taste bitter. Soon, an airline will match music with the food served to passengers. Last year, a well-known brand even released an app that played music while ice cream melted, but they did not match the music to the taste, which is often the case.
At home, Spence's dinner parties can be quite unique. One time, they ate rabbit with the fur still on the cutlery, and another time, they used remote-controlled colored lights. They have even experimented with sounds and different drinks to see how they affect taste. For Spence, home, sweet shops, food conventions, and gastronomy conferences are all part of his research."
9,C1,"Our brains are working harder than ever. We are bombarded with facts, false information, chatter, and rumors, all pretending to be useful information. We have to sort through this to find what we really need to know and what we can ignore. At the same time, we are doing more tasks ourselves. Thirty years ago, travel agents booked our flights and salespeople helped us in stores. Now, we do most of these things on our own. We are trying to manage many responsibilities, including our lives, families, jobs, hobbies, and favorite TV shows, often with the help of our smartphones. These devices have become part of our busy lives, filling every free moment with tasks. 
However, there is a problem. Although we believe we are multitasking—doing several things at once—this is a misleading idea. Earl Miller, a neuroscientist at MIT, explains that our brains are not designed to multitask effectively. When we think we are multitasking, we are actually switching quickly between tasks. Each time we switch, it costs us mental energy. Instead of being expert jugglers, we are more like amateur plate spinners, quickly moving from one task to another and worrying about what we might drop. Even though we feel productive, multitasking actually makes us less efficient.
Research shows that multitasking increases stress hormones, which can lead to confusion and unclear thinking. It creates a desire for constant stimulation, rewarding our brains for losing focus. The prefrontal cortex, the part of the brain we need to stay focused, is easily distracted by new things. This means that the very area we rely on to concentrate is often pulled away by distractions.
Just the chance to multitask can hurt our ability to think clearly. Glenn Wilson, a former psychology professor, calls this problem ""info-mania."" His studies found that trying to focus on a task while seeing an unread email can lower our IQ by almost 10 points. The cognitive losses from multitasking are even worse than those caused by being tired. Russ Poldrack, a neuroscientist at Stanford, discovered that learning while multitasking can send information to the wrong part of the brain. For example, if students do homework while watching TV, the information from their studies goes to a part of the brain that stores skills instead of facts. Without distractions, the information goes to a part of the brain that organizes it better, making it easier to remember.
Additionally, multitasking often involves making decisions, like whether to reply to a text or ignore it. Making decisions is also hard on our brains, and small decisions use the same mental energy as big ones. After making many small choices, we can end up making poor decisions about important matters.
In discussions about information overload, email is often mentioned as a major issue. It’s not that email itself is the problem, but the overwhelming amount of messages we receive. When a 10-year-old boy was asked what his father does for work, he said, ""He answers emails."" His father agreed that this was quite accurate. We feel we must respond to emails, but it often feels impossible to do that and accomplish anything else."
10,C1,"In a zoo in Sweden, a chimpanzee named Santino spent his nights breaking concrete into pieces to throw at visitors during the day. Was he being mean? In caves in the US, female bats help other fruit bat mothers if they can’t find the right position to give birth. Are they being kind? Fifty years ago, these questions were not considered important. Animals had behaviors, and science focused on measuring those behaviors. The idea that animals have thoughts, feelings, and moral beliefs was seen as silly. However, this view has started to change. Research on bats, chimps, rats, dolphins, and chickens has begun to explore animal emotions, which were once a forbidden topic. This change has led to popular science books like Mark Bekoff’s ""Wild Justice"" and Victoria Braithwaite’s ""Do Fish Feel Pain?"". This has sparked a debate that may never be fully answered: do animals have consciousness? This debate raises another question about conscience, which is a person’s sense of right and wrong that guides their actions.
In a recent experiment with cows, those that opened a locked gate to get food showed more happiness—by jumping and kicking—than those that had the gate opened for them. If this research suggests that cows enjoy solving problems, what does it mean for how we produce and eat beef? While the observations are clear, their meaning is debated. Dr. Jonathan Balcombe, author of ""Second Nature,"" believes the only logical response to this research is to stop eating meat. He thinks humanity is close to a major change in ethics, similar to the end of slavery. Aubrey Manning, a professor at Edinburgh University, believes we should reconsider how we view animal intelligence. He thinks animals have a simpler understanding of the world than humans do. Professor Euan MacPhail suggests we should avoid attributing human feelings to animals. These three views may never agree because the main issue is not just scientific or moral, but philosophical. Since defining consciousness is very difficult, can we ever truly understand what it is like to be a bat, as philosopher Thomas Nagel asks?
Balcombe describes an important experiment he conducted that suggests starlings, a type of bird, can feel depressed. In a study at Newcastle University, starlings were divided into two groups. One group lived in spacious, comfortable cages, while the other lived in small, empty cages. Both groups learned to eat tasty worms from one box and avoid unpleasant worms from another. However, when only unpleasant worms were offered, only the starlings in the comfortable cages would eat. Balcombe concluded that living in a bad cage made the starlings feel pessimistic about life. Balcombe, who has worked with animal rights groups, has a strong opinion. He says, “We look back with horror at a time of racism. One day, our views on animals will be the same. We can’t support animal rights while eating a cheeseburger.” If he were the only one with this view, it might be easy to ignore him. But Professor Aubrey Manning shares similar beliefs. Manning has written a textbook called ""An Introduction to Animal Behaviour."" He says, “What we are seeing is a swing in ideas. In the early 20th century, some people thought animals thought just like us, and there was a backlash against that. Now we are moving back to the idea that animals have feelings. But this is a very controversial topic, and we should be careful of academics with strong personal opinions.”"
11,C1,"Critical thinking is a way to engage with what we read or hear to understand it better. Adrian West, a research director at the Edward de Bono Foundation in the U.K., believes that arguing can help us find the truth. While technology helps us store and process information, there are worries that it is changing how we solve complex problems and making it harder to think deeply. West points out that we are often exposed to poor but appealing ideas, which can confuse our ability to reason. He notes that having more information does not always lead to better knowledge or decision-making. 
According to the National Endowment for the Arts, fewer people are reading literature, and this decline is speeding up. Patricia Greenfield, a psychology professor, thinks that focusing more on visual media may be hurting our critical thinking skills. She says that people are now more focused on real-time media and multitasking instead of concentrating on one thing. However, we still do not have a clear answer on how technology affects critical thinking. 
Technology has made critical thinking more complicated, and experts can no longer rely on old beliefs. It is easy to see technology as either good or bad, but the truth is that it can be both, depending on how it is used. For example, a computer game might help or hurt critical thinking. Reading online can improve analysis skills, but constantly clicking on links can prevent deeper thinking. Greenfield, who studied over 50 research papers on learning and technology, says that technology changes how people think. 
She explains that reading helps develop imagination and critical thinking in ways that visual media, like video games and television, do not. However, she also found that visual media can improve some types of information processing. Unfortunately, most visual media do not give people time to reflect or think deeply. As a result, many young people may not reach their full potential. 
How society views technology affects how we think about critical thinking. This is especially clear when looking at video games and learning. James Paul Gee, a professor of educational psychology, argues that video games can actually be beneficial for children. He says that while many believe video games are harmful, they can also be a great learning tool. Evidence shows that playing video games can help children develop better reasoning skills. Games like Sim City and Civilization teach decision-making and analytical skills in engaging environments that mimic the real world. 
In the digital age, as reading and math scores decline, it is important to examine how technology affects our thinking and analysis."
12,C1,"When Matthew Crawford is not thinking and writing about how we should live, he works as a motorcycle mechanic. He chose this job after becoming unhappy with office work and his role in social policy. His first book praised the value of manual jobs, while his latest book discusses how to deal with modern life. He was inspired to write it when he noticed advertisements appearing on a credit card machine while he was shopping. Crawford realized that these distractions are hard to avoid. What we want to think about at any moment is personal, but we often cannot choose this for ourselves because of things we do not even notice. It is becoming harder to think or remember conversations. To avoid constant interruptions, people are closing themselves off and are less likely to talk to strangers. Crawford says we increasingly experience the world through things like video games and apps, which often try to manipulate us. These things reflect our desires perfectly and can take over our lives.
Crawford is not alone in his concerns. Many office workers complain about emails but still spend their free time checking them. Studies show that just seeing a phone can distract us, which many of us have felt when bored at dinner, wanting to check our phones. There is no scientific proof yet that our attention spans have changed, but people are more aware of other things they could be doing. While it is easy to blame technology for this problem, Crawford believes it has made it easier for us to focus on ourselves. With so many choices, it is hard to control our attention, which affects our social lives. People prefer to text friends instead of talking to them. By only interacting with representations of people, we risk losing important connections in society.
Crawford uses his gym as an example. In the past, there was one music player for everyone, which could lead to disagreements over music choices. Now, people listen to their own music with earbuds, and the gym has lost its social atmosphere. Real connections often happen through conflict, like discussing different music tastes.
Crawford suggests two solutions. First, we need to manage noise and distractions in public spaces. More importantly, he believes we should engage in skilled activities to connect with the world in a more meaningful way. He mentions cooks, ice-hockey players, and motorcycle racers as examples of people who deal with real materials. No representation can replace the feeling of a hockey puck on ice or gravel under a motorcycle tire. These roles require good judgment and the ability to interact with others.
Crawford argues that when we engage with the world this way, we see that manufactured experiences are not as fulfilling as real ones. This does not mean everyone should become a chef, but it is important to use our judgment in daily life. There are many benefits to this approach, including professional satisfaction. Constantly fighting distractions can be tiring and make it harder to focus on what is important. In contrast, paying attention to one thing helps us pay attention to others."
13,C1,"
""What do you do for a living?"" This is a common question in small talk, and we often define ourselves by our jobs. However, if you are a philosopher like me, it can be a bit more complicated. Saying you are a philosopher can sound pretentious. It feels better to say you study or teach philosophy, but calling yourself a philosopher might make people think you believe you know some special truth. This is not true; philosophers are just like everyone else. Still, this stereotype makes me think twice about how I describe myself.
One reason this stereotype exists is that philosophers are seen as people who judge what others do and value intellectual life. The Greek philosopher Aristotle believed that a life spent thinking deeply was the best kind of life. While not all modern philosophers agree with him, philosophy is still connected to thinking and understanding life. Another Greek philosopher, Socrates, said that ""the unexamined life is not worth living,"" meaning that simply accepting what society says can lead to an unsatisfying life. Our ability to think about the world helps us control our lives and make our own choices.
However, living an examined life does not mean you have to read many philosophical books or spend all your time thinking. It just means paying attention to the everyday experiences that shape our lives and making sure they are meaningful. You don’t have to be a wise person living away from society to do this. In fact, examining life can be very practical and should be shared with others, as it is important for a good life.
Another reason people misunderstand philosophers is that academic philosophy has become more distant from real-life experiences, especially for those who do not have formal training. This is not entirely the philosophers' fault; universities often focus on expensive academic journals, making it hard for philosophers to share their ideas with anyone outside their small group of experts. As a result, philosophy can seem disconnected from everyday life. While some philosophers have tried to change this view, many have accepted it. The academic world has created an environment that feels isolated, and as philosophy has become more focused on specific debates, it has become harder to explain these ideas to people who are not trained in the field.
I sometimes call myself an ""ethicist"" because I mainly work in ethics, which is the part of philosophy that looks at human actions. Recently, I have felt that this title does not fully describe my work because ethics is often linked to formal rules and laws. There is a new trend in philosophy where ethics is seen as applied ethics, which examines the fairness of certain social practices. The role of an ethicist today is to decide if an action is ethical or acceptable. These are important questions that I often explore.
However, philosophy is more than just this. A typical discussion might start by asking if illegally downloading movies is unethical (it is) and then move on to deeper questions about responsibility, our views on art, and the effects of consumerism. In this way, philosophy helps people think more carefully about the actions and behaviors that shape their lives. Sometimes we might confirm what we already believe; other times, we might find that our beliefs are hard to defend. Either way, by examining these ideas, we can benefit everyone."
14,C1,"Food lovers, chefs, and anyone who enjoys food might think that thinking about what and how we eat can make eating more enjoyable. However, throughout history, discussions about food in philosophy have often been less important than other philosophical topics. When philosophers talk about eating, they often use it as a metaphor for something else, like gaining knowledge. Sometimes, there are discussions that seem to be about food, but when we look closer, we see they are actually about different, but related, ideas.
One interesting example is the ancient Greek philosopher Epicurus. He believed in seeking pleasure and avoiding pain in many areas of life. However, his name has become closely linked to enjoying food and drink. We see his name used in restaurants, food shops, and recipe websites, likely because these businesses hope to attract customers by connecting with a famous philosopher.
Food is also a social and cultural experience. The food we eat comes from history and is shared with our communities. These communities provide us with people to eat with and the systems that help grow and distribute food. We interact with food more often and in more important ways than with other products, making it a clear topic for philosophical discussion.
Once food is prepared, critics begin to talk and write about it. But why do these critics have such a special status? One philosopher pointed out that tasting food is not a special skill; rather, good food critics are just better at describing flavors. This area of taste has not been studied much in philosophy, as most research has focused on vision. This needs to change.
Another important aspect of food is its beauty. We often describe paintings or music as beautiful, but we rarely talk about the taste of food in the same way. Some philosophers believe that, with the growth of modern cooking, food deserves to be discussed in the same way as art forms like sculpture or poetry. However, food is consumed and disappears when we enjoy it, unlike art, which lasts over time. Therefore, it is incorrect to think of food as a true object of beauty.
There are also many ethical questions related to food. We can ask what we should eat: organic, free-range, locally grown, vegetarian, or non-genetically modified foods? The answers often reflect our ethical beliefs, and philosophers will always question why we choose what we do. This kind of academic discussion about food is important.
Another topic is the difference between cooking at home and in restaurants. Home cooks have a special responsibility to their guests because of their personal relationships. In a home kitchen, everyone shares food and companionship. In contrast, professional cooks have responsibilities to their employers and the food itself. Their kitchens are not open to everyone, and their relationships are more about colleagues than friends.
A recent essay called ""Diplomacy of the Dish"" looks at how food and dining can help connect different cultures. This happens in two ways: first, we learn to appreciate others by enjoying their food, and second, we build personal connections with people from other cultures by sharing meals. This practice has a long history in international relations. The essay includes many interesting examples and stories that make this part of food philosophy engaging for readers."
15,C1,"Rob Daviau, from the US, creates ‘legacy’ board games. He felt that the traditional board games he played as a child were not fun or challenging enough anymore, so he started to think about how board games could change. He wondered if they could have a story and if choices made in one game could affect future games. He took a classic board game called Risk and made a new version called Risk Legacy. In this game, decisions made during play have lasting effects. Players might have to tear up cards, change the board, or open packets with new rules at important moments. The game is played over several sessions, and players’ past actions become part of the game’s story. Daviau said, “You could point to the board and say: ‘Right here! You did this!’”
Later, he was asked to work on a legacy version of Pandemic, a very popular cooperative board game where players try to cure diseases. His next project was a game called SeaFall. While Pandemic Legacy was very well received, SeaFall was seen as a true test of the legacy format because it was the first game that did not follow an earlier version. Set in the age of sailing (16th to mid-19th century), players become sailors exploring a new world. Designers of legacy board games must think about all possible player choices to keep the story from falling apart. To do this, they use testers to see how the games will play out. Jaime Barriga was a tester for SeaFall. He said, “It takes a lot of time. The first few games might be great, but then after a few more games, it can start to break. Then you have to go back and fix everything.”
Legacy board games were not expected to become very popular. When Daviau was developing the idea, he thought it would appeal to only a small group of people. He said, “I thought it was different and pretty good, but it’s weird and breaks many rules. I thought I would be known as the guy who did this strange project.” However, many players, like Russell Chapman, loved the idea. He believes it is one of the biggest improvements in game design in years. He said, “It’s a new level of commitment, intensity, and excitement. There’s nothing more thrilling for a board gamer than learning a new game, and you get that constantly with legacy games.”
Another fan, Ben Hogg, said that the excitement of a developing story was more important than worries about the game lasting a long time. He mentioned that people don’t usually watch the same movie twice in the cinema. “You’re buying into an experience,” he said. “It adds a storytelling aspect like in video games.” The legacy format is inspired not only by video games but also by the popularity of TV series that tell stories in episodes. While in college, Daviau wanted to be a television writer but later moved into advertising and game design. Still, he loved telling stories.
Matt Leacock, the creator of Pandemic, compared the legacy design process to writing a novel. He said, “You need to know how you want it to end and have a good idea of where to start, but it’s not enough to just have a general plan.” Daviau feels proud of his work but is curious to see how others will use the idea. He also thinks that gamers will soon want something new. Colby Dauch, the studio manager for the publisher of SeaFall, is not so sure. He believes the legacy format has changed how people think about board games."
16,C1,"One night not long ago, an octopus named Inky escaped from his tank at New Zealand’s National Aquarium. He crawled across the floor and squeezed into a small drain that led to the Pacific Ocean. This story was exciting and was shared widely online. Many people enjoy stories of animals escaping, like those of rats and llamas, because they like to think of animals as being similar to humans. Octopuses are especially interesting because they are very intelligent and look very different from us. They can open jars, recognize faces, use coconut shells for protection, and even play in complex ways.
Some people think that comparing animals to humans is not scientific. However, Dr. Frans de Waal, who studies primates like gorillas and chimpanzees, believes that it is actually unscientific to ignore the human-like qualities of animals. He calls this idea ""anthropodenial."" After studying many years of research on animal intelligence, he shows that animals can do many things that we thought only humans could do, like remembering the past and future, showing empathy, and understanding what others want. This means that animals are smarter than we often believe.
Dr. de Waal thinks that anthropodenial is a recent idea. In medieval and early modern Europe, people believed that animals were smart enough to be put on trial for crimes. Even in the 1800s, many scientists looked for similarities between human and animal intelligence. Charles Darwin, who changed how we see our place in the world with his theory of evolution, said that the difference between human and animal minds is one of degree, not kind.
In the 20th century, a new way of thinking called behaviorism focused on training animals with rewards and punishments. During this time, many people saw animals as simple machines or robots, and those who thought animals had feelings were often dismissed. This change in thinking happened while humans were destroying animal habitats, polluting the environment, and treating livestock poorly.
Fortunately, Dr. de Waal believes we are starting to understand animal intelligence better. He says that we are learning to see animal thinking as similar to human thinking, even if they are not the same. He notes that there is a lot of new information about animal intelligence available online.
To test animal intelligence effectively, we need to consider each species' unique skills. For example, squirrels might not do well on human memory tests, but they can remember where they hid their nuts. In her book, The Soul of an Octopus, naturalist Sy Montgomery suggests that if an octopus were to test human intelligence, it might judge us by how well we can change the colors of our skin. If we failed, it might think we are not very smart.
Dr. de Waal is not completely convinced that Inky's escape had a happy ending. He believes that while some octopuses have escaped before, it might be too hopeful to think Inky knew how to find the drain to the ocean. However, he understands that stories like Inky's help people appreciate animal intelligence. He once conducted an experiment to see if capuchin monkeys could feel envy. When some monkeys received cucumbers (a food they liked) and others received grapes (which they liked even more), the monkeys with cucumbers became very upset when they saw their friends getting grapes. This study was published in a well-known scientific journal, but what really made people believe the results was a short video of the experiment released ten years later. This shows how our understanding of animal minds can be surprising."
17,C1,"
Robotics, once just a part of science fiction, is now becoming a major change in technology, similar to what happened during industrialization. Robots have been used in car and factory production for many years, but experts believe we are about to see a big increase in the use of robots in many other areas. Many people think that robots will take over most jobs done by humans in the next 50 years. However, about 80% of people also believe their current jobs will still exist in the same way during that time. This shows a common belief that our jobs are safe, but they are not. Every industry will be affected by robots in the coming years.
For example, an Australian company called Fastbrick Robotics has created a robot named Hadrian X that can lay 1,000 bricks in one hour. This job would take two human workers a whole day or more. Another example is a robot called Tally, developed by a startup in San Francisco. Tally moves around supermarkets to check that products are in stock and correctly priced, rather than cleaning the aisles.
Supporters of robotic automation say that robots cannot fix or program themselves yet, which means new jobs will be created for skilled workers like technicians and programmers. However, critics warn that we should not forget the importance of human skills in the workplace. They believe society is not ready for the changes that will come from reducing human interaction.
Dr. Jing Bing Zhang, an expert in robotics, studies how robots are changing the workforce. His recent report predicts that in two years, nearly one-third of robots will be smarter and able to work safely with humans. In three years, many top companies will have a chief robotics officer, and some governments will create laws about robots. In five years, salaries in the robotics field will rise by at least 60%, but many jobs will still be unfilled because there aren’t enough skilled workers.
Zhang says that automation will affect lower-skilled workers, which is unfortunate. He believes these workers need to retrain themselves instead of waiting for the government to protect their jobs. People can no longer expect to do the same job for their entire lives.
New technologies in motion control, sensors, and artificial intelligence will lead to new types of robots for consumers, including robots that can live and interact with us in our homes. This presents a great opportunity for companies, but it also brings challenges, such as the need for new rules to keep us safe and protect our privacy.
With many jobs at risk and a global employment crisis approaching, it is important to focus on education to prepare for the future workforce that will include robots. Developed countries need more graduates in science, technology, engineering, and math (STEM) to stay competitive."
18,C1,"George Mallory, a famous mountaineer, is often quoted as saying he wanted to climb Mount Everest ""because it’s there."" This answer reflects a common question people have: why would anyone want to take such a dangerous risk? Some may find inspiration in his words, as they encourage people to pursue their dreams and face the unknown. However, the main point is that climbing can simply be about adventure and enjoyment.
In 1967, Bolivian writer Tejada-Flores wrote an important essay called ""Games that Climbers Play."" He described seven different types of climbing activities, from playing on small rocks to climbing high mountains, each with its own rules. He argued that the way climbers approach a climb, such as how much equipment they use, should depend on the rules of that specific activity. Over the years, this idea has become popular in climbing discussions, appearing in magazines and conversations.
Many climbers love the feeling of freedom that climbing gives them. However, climbing can also be very limiting. For example, being stuck in a tent during a storm is not what most people think of as freedom. This creates a contradiction in climbing. Yet, some argue that having fewer choices can actually simplify a climber's life, which can feel like a different kind of freedom.
When asked ""Why climb?"", US rock climber Joe Fitschen suggests a better question: ""Why do people climb?"" He believes that climbing is part of our nature; humans are built to take on challenges and test their limits, even if it involves risks. Therefore, the joy of climbing is more about our biology than logical thinking.
Another important perspective comes from US academic Brian Treanor, who says that climbing can help develop important qualities like courage, humility, and respect for nature. While he recognizes that not all climbers show these traits, he believes they are especially important in today's world, which often avoids risks. Climbing, then, can help people grow and thrive in life outside of climbing.
Additionally, some climbers believe that those who climb without help from others or technology must be fully dedicated to succeed. Experts Ebert and Robinson stirred some debate by claiming that independent climbs are harder and deserve more recognition than those done with teams or extra support, like bottled oxygen at high altitudes. This view raised concerns that it might encourage climbers to take unnecessary risks.
We can also look at climbing from a different cultural perspective. Many climbers report experiences of being fully present, or ""in the zone."" The physical effort, the focused mindset during a climb, and the intuitive problem-solving are all key aspects of climbing. Some argue that climbing techniques are similar to those in Zen philosophy, which aims to achieve a peaceful state of mind. This idea adds another layer to understanding why people are drawn to climbing."
79,C2,"One of the most interesting changes in education in the UK and around the world in recent years is the push for new students to get involved in research as early as possible. This change shows that research is not just for famous scholars at old universities or scientists who are discovering new things in fields like medicine. Instead, research is a natural and important way to build knowledge and skills. 
Research skills will help you not only in your studies but also in your future job. They teach you how to think about the world and how to approach your own work. As a student, you contribute to knowledge. You do not just learn and repeat what others say; you create new knowledge. Creating knowledge starts with asking questions instead of just accepting things as they are. You might wonder: Why? How? When? What does this mean? What if things were different? How does it work in this situation? What should we think about the facts, opinions, or beliefs we are given? Why is this important? These questions are at the heart of what we call research.
Research can be seen as a range of activities. On one end, there is complex and groundbreaking research done by highly trained experts, which leads to major changes and new knowledge. However, research can also start from simple questions and everyday inquiries. This type of research involves careful work, asking thoughtful questions about issues, and making practical suggestions.
Most students have always been researchers in some way. You have likely done research for school projects or answered questions at work since you started. You have asked questions that led you to investigate things, whether it was about planning a holiday, growing plants, fixing things at home, training a pet, or finding the right products online.
In college and higher education, you are expected to have a curious mind, identify problems, critically explore information, and create your own responses and knowledge. Some students may find this challenging because, in some cultures, knowledge is seen as fixed, and learning comes from listening to teachers and texts. It might feel disrespectful to question established knowledge, and you may think you should be told what is important to learn. 
However, in higher education in the UK, US, much of Europe, and Australia, questioning established knowledge and authorities is encouraged. This process of inquiry and knowledge creation can seem overwhelming. Critical thinking is especially important in research. The research done by others is valuable for students and professionals, but we need to engage with it, think critically, and test it. We should not just accept everything we read as true; instead, we should analyze it to see if it is logical and supported by evidence. We must avoid blindly trusting the facts and information we receive from others."
80,C2,"Cities have always been important places for ideas and creativity. In the 18th century, coffee houses in London were popular spots where people discussed science and politics. In modern Paris, cafés on the Left Bank were where artists like Pablo Picasso talked about new art. However, living in a city can be challenging. The same cafés that encouraged discussions also helped spread diseases like cholera, and even Picasso eventually moved to the countryside. 
Today, cities are known for their creativity, but they can also feel overwhelming and unnatural. Scientists are now studying how city life affects our brains, and the findings are concerning. While it has been known that city life can be tiring, new research shows that living in a city can actually make us think less clearly. One major reason for this is the lack of nature in urban environments. Studies have shown that hospital patients recover faster when they can see trees from their windows. Even small views of nature can help our brains because they give us a break from the busy city life.
For the first time in history, more people live in cities than in rural areas. Instead of being in open spaces, we are surrounded by many strangers in concrete environments. This unnatural setting can have serious effects on our mental and physical health and can change how we think. For example, when walking down a busy street, our brains have to pay attention to many things: people walking, traffic, and confusing street layouts. These small tasks can drain our mental energy because cities are full of distractions that require us to constantly shift our focus. 
The brain is like a powerful computer, but paying attention uses a lot of its energy. In nature, we don’t have to work as hard to focus. This idea is called attention restoration theory, created by psychologist Stephen Kaplan. He suggested that being in nature can help restore our attention. Natural environments have things that naturally attract our focus without causing stress, unlike loud city noises. This allows our minds to relax and recharge.
Long before scientists studied this, philosophers and landscape designers warned about the negative effects of city life and sought ways to bring nature into urban areas. Parks, like Central Park in New York, provide a break from city life. A well-designed park can quickly improve brain function. While people have tried many ways to boost mental performance, such as energy drinks or office redesigns, nothing seems to work as well as simply walking in nature.
Given the many mental challenges of city living, we must ask: Why do cities keep growing? Even in our digital age, cities remain centers of creativity. Research from the Santa Fe Institute shows that the same crowded features of cities that can hurt our attention and memory—like busy streets and close contact with many people—are also linked to innovation. The concentration of social interactions in cities is a key factor in their creativity. Just as the crowded streets of 18th-century London led to new ideas, the busy environment of 21st-century Cambridge, Massachusetts, helps it thrive as a technology hub. 
However, less crowded areas may produce fewer new ideas over time. The challenge is to find ways to reduce the negative effects of city life while keeping its unique advantages. After all, there will always be times when people feel tired of nature and want to return to the city."
81,C2,"**Where Should We Look for the Mind?**
This question might seem strange: we usually think that thinking happens inside our heads. Today, we have advanced brain imaging techniques that support this idea. However, I believe we should not limit the study of the mind to just what is inside our bodies. There is a lot of evidence, from ancient times to now, showing that objects and tools also play a role in how we think. 
From an archaeological perspective, items like stone tools, jewelry, carvings, clay tokens, and writing systems have been important in human evolution and the development of our minds. Therefore, what is outside our heads might also be part of our minds. 
It is easy to connect the mind with the brain because most of what we know about the mind comes from studying people in isolation from their surroundings. This makes sense for neuroscientists who use brain scans, but it often overlooks the fact that much of our thinking happens outside our heads. I do not want to deny the importance of the brain, but I want to emphasize that the mind is more than just the brain. 
Instead, we should explore the idea that human intelligence extends beyond our bodies into culture and the material world. This is where my new theory, called Material Engagement Theory (MET), comes in. MET looks at how objects can help us think and how they become part of our bodies. For example, when we make numbers from clay or use a stone to create a tool, these actions show how objects can extend our thinking. 
MET also examines how these interactions have changed over time and what these changes mean for our thinking. This approach helps us understand what minds are and how they are shaped by the things around us. 
Consider a blind person using a stick. Where does this person’s self begin? The connection between the blind person and the stick shows how minds and objects can be linked. The stick helps the blind person turn touch into a form of sight, and the brain treats the stick as part of the body. 
The stick reminds us that human intelligence can adapt and change by using new technologies. I see the human mind as a work in progress, always evolving. No matter what form the “stick” takes in our history—whether it’s a simple stone tool or a modern smartphone—its main purpose is to connect us to the world, helping us understand and navigate our environment.
For humans, tools are used to satisfy our curiosity. This unique ability to engage with material culture explains why we create things and how those things shape our minds. I call this metaplasticity—our minds are flexible and change as we interact with the material world. 
I want to emphasize the importance of material objects in understanding the mind. MET provides a new way to see how different forms of material culture, from ancient tools to modern devices, have influenced who we are and how we think. While the idea of technology changing our minds may sound futuristic, humans have been using such tools since the beginning of our existence."
82,C1,"Few inventions have changed our way of understanding the world as much as photography. This is especially true in the United States, where photography quickly became part of culture at all levels – from practical and scientific uses to industrial and artistic ones, as well as a new form of entertainment that came with the camera. Historians believe that photography is one of the greatest contributions of the US to the visual arts. No other art form has had such a significant impact from the US.
This claim is strong, but it shows how photography has become central to US culture and thought. To fully appreciate its impact, we need to look at its beginnings in the mid-19th century. Why was this new art so attractive? 
First, photography was a mechanical process that matched the growing interest in technology. This was part of a national attitude that accepted change as normal. Just like steam power, railroads, and electricity made communication and travel easier, photography brought the wonders of the world into people's homes in an exciting way.
Second, the camera became the preferred tool for showing one’s identity. It allowed people to create images for public viewing in a country where personal and national identities were always being formed and reformed. Third, the camera was important for families, providing a way to keep a record of their lives, even if that record was somewhat idealized. 
Finally, the realistic nature of photographs matched the trend towards realism in US art, which focused on everyday life. Every photograph draws attention to something specific, showing what the photographer wants us to see. Because photographs are made by a machine, they are records of events, people, and things, which gives them a sense of objectivity. However, since each photograph is taken by a specific person, it also carries a personal point of view.
When we look at a photograph, we might think we understand its meaning, especially if we recognize the subject. This is why photography has often been called a ""universal language."" But the meaning of a photograph is more complicated. Few things in front of us express their own meaning, and no image is shown without some context that affects how we understand it, like a caption in a newspaper or its placement in a gallery.
To understand a photograph historically, we need to consider its purpose and role in its culture: Why did the photographer take the picture? How did people first see it? The same image can be viewed in different places and times, and its meaning can change accordingly.
For a long time, the importance of the camera to artists was not widely known, but today it is clear. In recent decades, starting with artist Andy Warhol’s use of photographs in his work, many artists have included photographs in various ways. In summary, photography has become an essential part of art, closely linked to painting and our ideas of representation before the camera was invented."
83,C1,"In 1890, William James, an American philosopher and one of the founders of modern psychology, defined psychology as the ""science of mental life."" This definition is still useful today. We all have a mental life, which means we have some understanding of our thoughts and feelings. Although we can study mental life in animals like rats and monkeys, it is still a complex idea. James focused on human psychology, which he believed included basic elements: thoughts, feelings, the physical world, and a way to understand these things. Our knowledge about our mental life is mostly personal and comes from our own experiences, which may or may not match scientific facts.
Because of this, we often make judgments about psychological issues based on our own experiences. We act like amateur psychologists when we share our opinions on complicated topics, such as whether brainwashing works or why people behave in certain ways, like feeling insulted or unhappy. Problems can arise when two people understand things differently. Formal psychology tries to provide methods to determine which explanations are most likely correct in different situations. Psychologists help us separate subjective thoughts, which can be biased, from objective facts.
Psychology, as defined by William James, is about the mind and brain. While psychologists study the brain, they do not yet fully understand how it works in relation to our hopes, fears, and behaviors. It is often difficult to study the brain directly, so psychologists learn more by observing behavior and making educated guesses about what happens inside us. A challenge in psychology is that scientific facts should be objective, but the mind's workings are not easily observed. We can only see them indirectly through behavior.
Studying psychology is similar to solving a crossword puzzle. It requires evaluating and interpreting clues, which must come from careful observation and accurate measurement. These clues are analyzed with scientific rigor and logical reasoning. Psychology often involves complex interactions, and understanding them requires advanced techniques and theories. Like any science, psychology aims to describe, understand, predict, and learn how to influence the processes it studies. When these goals are met, psychology can help us understand our experiences and apply findings to real life.
Psychological research has been beneficial in many areas, such as improving teaching methods, designing safer machines, and helping people express their feelings better. Although psychological questions have been discussed for centuries, scientific investigation of these questions has only happened in the last 150 years. Early psychologists used introspection, or self-reflection, to find answers. They aimed to identify mental structures, which is still important today, but introspection has limitations. As Sir Francis Galton noted, it only allows us to see a small part of what the brain does automatically. William James compared trying to understand the mind through introspection to ""turning up the gas quickly enough to see how the darkness looks."" Therefore, modern psychologists prefer to base their theories on careful observations of others' behavior rather than on their own experiences."
84,C1,"In a storehouse in a regular business park in Michigan, USA, there is a unique collection known as the Museum of Failed Products. This museum, owned by a company called GfK, displays products that were taken off the market because very few people wanted to buy them. The owner, Carol Sherry, believes that each product has its own sad story related to the designers, marketers, and salespeople involved. 
What is surprising is that this museum exists as a successful business. Many companies do not keep samples of their failed products, showing how uncomfortable they are with failure. The museum was started by Robert McMath, who originally wanted to create a library of consumer products, not just failed ones. He began collecting new items in the 1960s, not realizing that most products fail. As a result, his collection ended up being mostly unsuccessful products.
Today’s culture of optimism may contribute to the products in the museum. Many products likely went through meetings where no one recognized they would fail. Even if they did, marketers might invest more money into a failing product to try to save face. There is often little effort to understand why the products failed, and people involved may choose to forget about it.
This focus on positivity is also common in the self-help industry, where the idea of ""positive visualization"" is popular. This means that if you imagine good outcomes, they are more likely to happen. Some researchers, like neuroscientist Tali Sharot, suggest that our brains may be wired to see things more positively than they really are. Her research shows that well-balanced people often have an overly optimistic view of their ability to influence events compared to those who are depressed.
However, psychologist Gabriele Oettingen has studied how effective these positive thoughts really are. Her findings show that thinking too much about how well things could go can actually reduce people's motivation to reach their goals. For example, people who imagined having a very successful week at work ended up achieving less.
Psychologist Carol Dweck explains that our experiences with failure depend on our beliefs about ability. People with a ""fixed mindset"" believe that ability is something you are born with, while those with a ""growth mindset"" think that ability can develop through effort and challenges. Fixed mindset individuals often find failure very frightening because it shows they did not meet expectations. For instance, a talented athlete might think of themselves as a natural but fail to practice enough to succeed. On the other hand, people with a growth mindset see failure as a sign that they are pushing their limits. Dweck compares this to weight training, where muscles grow stronger by being challenged. Overall, having a growth mindset is a healthier way to approach life, regardless of whether it leads to success."
85,C1,"Everyone knows that sports teams have an advantage when they play at home. But why is that? Many people think they understand it, but professional sports are changing quickly, and what we used to believe is now being questioned. Two main factors are challenging the idea of home advantage: science and money. Sports scientists want to find out what helps players perform their best, and they have come up with many theories about home advantage. At the same time, those who invest money in sports want to know why home advantage is still important. If players are paid well, shouldn’t they feel comfortable playing anywhere?
What does this mean for fans? For example, would it matter if Manchester United played some of their home games in the Far East to reach more fans there? It would matter to British fans who believe their team needs their support and loyalty. Fans often think that the order of events explains what happens. When a team plays well, the home fans cheer loudly, and sometimes this leads to a goal. Fans remember the times they cheered and a goal was scored, but they forget the many times they cheered and nothing happened.
However, there is one common belief among fans that science supports. Home fans not only cheer for their team but also direct a lot of their noise at the referee. In a well-known study, referees watched a match with crowd noise and another group watched it in silence. The referees who heard the crowd were less likely to call fouls against the home team. Interestingly, the crowd noise did not make them more likely to call fouls against the away team. The researchers found that referees try to avoid extra stress from making calls against the home team. They do not just do what the crowd wants; they try to avoid making the crowd angry.
Recent studies show that home advantage has decreased in all major sports, but not as much as expected. For example, in the early years of the Football League in the 1890s in England, home teams won about 70% of the points, while today they win about 60%. One reason for this change could be travel. In the past, travel was difficult, but now players travel in comfort, and if their hotel is not good enough, the club will find them a better one. Stadiums used to be very different from each other, but now many top stadiums look similar and efficient.
Despite these explanations, none fully explains the data. The mystery remains why home advantage varies by sport. Basketball has the most home advantage, followed by football, while baseball has the least. One reason could be that home advantage is more about teamwork, and baseball focuses more on individual matchups between the batter and pitcher. Team sports require players to work together, and in basketball, success depends on teamwork, which builds confidence when playing at home.
Another possibility relates to a more aggressive side of sports. Researchers found that players’ testosterone levels were normal before away games but increased by up to 67% before home games. This increase may be linked to a natural instinct to protect their home ground. In recent Rugby World Cup games, underdog teams playing at home won against stronger opponents, showing determination and a refusal to be intimidated—this is a sign of home advantage. As one referee said, “It’s the law of the jungle out there.”"
86,C1,"""‘The more I practice, the luckier I get,’ said golfer Gary Player about fifty years ago. This saying is famous among athletes and their coaches. It is easy to understand why. First, many people argue about who really said it first, as other golfers from that time are also mentioned. Second, the meaning of the quote is not straightforward. Player did not mean 'lucky' in a real sense; he was being ironic. His true message was: ‘The more I practice, the better I get.’ But there is a limit to this idea. The question of where that limit is, or if it even exists, is part of the nature-nurture debate. This debate is about whether talent is something we are born with or something we develop through practice, especially in sports.
However, it is not really a balanced debate; it seems that practice (nurture) wins most of the time. The idea that practice is more important than natural talent comes from psychologist Anders Ericsson. He is known for the theory that anyone can become an expert in any field with 10,000 hours of practice. This theory has inspired many popular books, like Malcolm Gladwell’s Outliers, and others like Talent Is Overrated and Bounce: The Myth of Talent and the Power of Practice. These books generally agree that practice is the most important factor, suggesting that ‘10,000 hours is both necessary and enough to make anyone an expert in anything.’
But, as Epstein discusses in his interesting book, this is often not the case. He points out a major problem in much of the research on what makes people excellent: it usually looks only at successful people. He asks if there are better ways to understand why some people have certain abilities while others do not, and how to separate what is natural talent from what comes from environment, support, opportunity, and hard work.
Epstein travels around the world, from Kenya to Sweden, and from the Caribbean to the Arctic, to find answers. In Alaska, he learns that the drive and desire of sled dogs in a tough race might be influenced by their genes, suggesting a link between genetics and traits we think are voluntary. This raises the question: can commitment and determination be chosen, or are they partly genetic?
Epstein makes a distinction between 'hardware' (nature) and 'software' (nurture), and he believes that both are important for elite athletes. He does not ignore the role of training or environment. For example, he suggests that if Usain Bolt had grown up in the US, he might have become a good basketball player instead of the fastest man in history. However, he also looks at cases where genetics play a significant role that cannot be overlooked. He also discusses race and gender in sports, asking: ‘If only practice matters, why do we separate men and women in competitions?’ Sometimes, the best questions are the most obvious."""
87,C1,"A recent international report shows that many children living in rich, developed countries feel lonely and unhappy. Jay Griffiths asks a simple question: why are today’s children so unhappy? In her new book, Kith, Griffiths explains that these children spend too much time indoors, stuck in front of screens like televisions and computers, and have lost touch with nature – the woods, mountains, rivers, and streams. She believes this is the main problem. 
A follow-up study interviewed some children to understand their feelings better. The children did not express a strong desire for new technology; many said they would be happier if they could just go outside more. Many adults over 40 often talk about how they enjoyed their childhoods, spending long summer days exploring, swimming, or building forts. They believe this was much better and healthier than the lives of today’s children. However, they often forget that they are the ones who have created a safer environment for their kids, which limits their freedom.
Griffiths’ book includes strong arguments about the fear of dangers in the outside world and how it connects to consumerism aimed at children. Parents’ fears lead them to keep their kids at home, which benefits the toy and gadget industry that sells entertainment for indoor kids. She also discusses trends like giving medication to restless children or requiring safety goggles for playground games. While I am not sure how common these rules are, Griffiths expresses her frustration clearly and passionately about childhood freedoms and restrictions.
However, Griffiths sometimes goes too far with her ideas and ignores important counter-arguments. At one point, she even compares the treatment of children today to a form of racism, which seems extreme. She tends to have a romantic view of children, believing they are naturally good and creative. While she acknowledges that children can be mischievous, she mostly presents an idealized view that may not match reality.
One of the main ideas in the book is that children should have the freedom to explore and take risks. Griffiths argues that children need to experience small accidents to learn how to handle bigger dangers later. However, she does not explain how to create these safe accidents. Also, not all children want to explore the wild as she suggests. What about the shy, quiet, or cautious children? Perhaps the real issue is that we try to make all children fit into the same mold, whether by keeping them too safe or pushing them too hard to explore."
88,C1,"I am a research bio-psychologist with a PhD, which means I have studied a lot. I am good at solving problems in my work and life, but this skill does not come only from my education. Many problems in life cannot be solved with complicated formulas or memorized answers from school. They need judgment, wisdom, and creativity that come from real-life experiences. For children, these experiences often happen through play. 
My recent research focuses on how important play is for children's development. All mammals, including humans, play when they are young, and those who have the most to learn tend to play the most. Carnivores, like lions, play more than herbivores, like cows, because hunting is harder to learn than grazing. Primates, like monkeys, play more than other mammals because they rely more on learning than on instincts. Children, who have a lot to learn, play more than any other young primates when they are allowed to.
Play is a natural way for both adults and mammals to learn. The most important skills children need to live happy and productive lives cannot be taught in school. These skills are learned and practiced through play. They include thinking creatively, getting along with others, cooperating, and controlling their emotions. Creativity is essential for success in today’s world. We no longer need people who just follow instructions or do simple calculations. We need people who can ask new questions and solve new problems. If we can encourage creative thinkers, we will have a strong workforce.
However, we cannot teach creativity directly. School can sometimes take away creativity by focusing on fixed questions and answers instead of encouraging children to ask their own questions. More importantly, children need to learn how to get along with others, care for them, and cooperate. Children naturally want to play with each other, and through play, they learn social skills, fairness, and morality, which are important for their future.
Play is voluntary, meaning players can choose to stop at any time. If they cannot quit, it is not really play. Players understand that to keep the game fun, they must make sure everyone is happy. This ability to quit makes play a very democratic activity.
Unfortunately, school has become more stressful. There are fewer breaks, more homework, and greater pressure for high grades. Outside of school, organized sports have taken the place of spontaneous games. Supervised playdates have replaced unsupervised neighborhood play, and adults often feel they need to step in instead of letting children solve their own problems. These changes have happened slowly but have had a big impact over time. They are caused by various social factors, including parents' fears, warnings from experts about dangers, less connected neighborhoods, and the belief that children learn more from adults than from each other.
Our children do not need more school; they need more play. If we care about our children and future generations, we must change the negative trend that has developed over the past fifty years. We need to give childhood back to children. They should be allowed to play and explore freely so they can grow into strong adults ready for an unpredictable future."
89,C1,"In many countries, more young people in their twenties are using social media to find jobs. Websites like Twitter and LinkedIn allow them to connect directly with potential employers, which is easier than the old way of standing outside an office with a sign saying ""hire me."" However, this increased access also means there is a higher chance of making mistakes.
For example, a young job seeker in the US reached out to a senior marketing executive on LinkedIn. This executive had many important contacts, and the job seeker thought she could help him. But the executive was upset by his request. She felt it was wrong to share her contacts with someone she didn’t know. Instead of just rejecting his request, she sent a harsh and sarcastic message that became very popular online. Many people were shocked by her response, and she might regret how she expressed herself. However, if this incident makes young people think more carefully about using social media for work, it could be a good thing.
Social media can be risky for job seekers who do not know how to use it properly. Many young people are making mistakes. It is ironic because social networking sites like Facebook and Twitter have been a big part of young people's social lives for years. When older generations were teenagers, social media was a way to escape from parents and teachers. It was a place to show off and be creative, often in a fantasy world. You could have long conversations online and then ignore those people in real life. You could create a different image of yourself on Facebook with the right pictures and songs.
However, using social media for professional networking is very different. Some young people do not see this difference clearly. They learned to be bold and confident online, which might lead them to think this is still a good approach. Just because many people liked your posts on Facebook does not mean you can impress employers on LinkedIn. Young people need to understand that the rules of social networking have changed, and they must meet employers' expectations to succeed in the job market.
One common complaint from employers about young job seekers on professional networking sites is that they are too casual and seem arrogant. This reinforces the idea that young people feel entitled. In reality, many young people are struggling to find jobs, which is why they are using social media. This impression of arrogance can hurt their chances, even if they have the skills and motivation to be valuable employees.
So, how should you contact someone on a professional networking site? First, clearly explain who you are and what you can offer them, like doing some research or helping in another way. This approach increases your chances of getting a helpful response. Avoid sending generic messages, and keep your tone respectful to make a good impression. Remember, social media can be a great way to make important connections, but it needs to be used carefully to avoid closing doors."
90,C1,"""Anyone who claims they can accurately predict the future of newspapers is either lying or mistaken. The numbers show that newspapers are in trouble. Since 2000, the circulation of most UK national daily newspapers has dropped by 30 to 50 percent. The Pew Research Centre in the USA reports that only 26 percent of US citizens now get their news from newspapers, down from 45 percent in 2001. Many people confidently say that the last printed newspaper will be gone within 15 years. However, history shows that old forms of media often survive. For example, in 1835, a journalist in New York claimed that books and theatre were finished and that newspapers would become the most important source of news. Yet, theatre survived not only newspapers but also cinema and television. Radio has continued to thrive even with the rise of TV, and cinema has remained popular despite videos and DVDs. Even vinyl records have made a comeback, with online sales increasing by 745 percent since 2008. 
Newspapers were once a new form of media, but it took centuries for them to become the main source of news. This change happened because producing timely news for a large audience became possible and affordable in the mid-19th century, thanks to the steam press, railways, and the telegraph. It was also important that people began to understand that the world is always changing and that they need regular updates. In medieval times, people did not think this way; they only noticed the changing seasons and major disasters like famine or disease, which they could not predict. Life was seen as a cycle, and the most important events were those that repeated over time. 
Before the 19th century, journalism was not a full-time job that could support a living. Even then, there was no clear reason why most people needed news every day or week. In some ways, the regular publication of newspapers can be a limitation. Online news allows readers to choose when to read based on what they find urgent. Advanced search engines and algorithms help us customize the news to fit our interests. When important news happens, online news sources can provide updates every minute. Mistakes and misunderstandings can be corrected quickly, and readers can access full documents or events mentioned in the news. This is very different from the limitations of newspapers. 
However, many news providers do not seem to understand how the internet can help spread knowledge and understanding. Instead, they focus on being the first to report news, getting more reader comments, and creating excitement, which can lead to confusion. In medieval times, news was often shared in busy marketplaces or taverns, where truth mixed with rumors and misunderstandings. In some ways, we seem to be going back to that situation. Newspapers have not always been very good at explaining how the world works. They might be facing extinction. Or perhaps, as the internet adds to our feeling that we live in a chaotic world, newspapers will find that they can still help us understand and gain wisdom."""
91,C1,"If humans truly felt at home under the moon and stars, we would walk happily in the dark, just like many animals that thrive at night. However, we are daytime creatures, with eyes that are made for sunlight. This fact is deeply rooted in our genes, even if we don’t often think about being daytime beings, just like we don’t think about being primates or mammals. This is the only way to understand what we have done to the night. We have changed the night by filling it with artificial light. This control is similar to building a dam on a river. While there are benefits to this, it also leads to problems known as light pollution, which scientists are just starting to study. 
Light pollution mainly comes from poor lighting design that allows artificial light to shine into the sky instead of focusing it down where it is needed. Bad lighting brightens the night, changing the light levels that many living things, including us, have adapted to. Wherever artificial light spreads into nature, it affects important life activities like migration, reproduction, and feeding. 
For most of human history, the term ‘light pollution’ would not have made sense. Imagine walking towards London on a moonlit night around 1800, when it was the most populated city in the world. Nearly a million people lived there, using candles, torches, and lanterns. Only a few homes had gas lights, and there were no public gas lights in the streets for another seven years. From a distance, you would have been more likely to smell London than to see its faint glow. 
We have brightened the night as if it were empty, but that is far from the truth. There are many nocturnal species among mammals. Light is a strong biological force that attracts many animals. Scientists say that songbirds and seabirds can be ‘captured’ by bright lights, circling around them until they fall. Birds that migrate at night often crash into brightly lit tall buildings, and young birds on their first journey are especially affected. Some birds, like blackbirds and nightingales, sing at strange hours because of artificial light.
It was once believed that light pollution only bothered astronomers, who need to see the night sky clearly. While most of us may not need a perfect view of the stars for our work, we still need darkness. Rejecting darkness is pointless. It is just as important for our health as light is. Changing our natural sleep and wake cycles can lead to health problems. The regular pattern of waking and sleeping is a biological response to the natural light changes on Earth. These rhythms are so important to our existence that disrupting them is like changing our center of balance.
In the end, humans are just as affected by light pollution as frogs living near a bright highway. By living in our own bright world, we have disconnected ourselves from our natural and cultural roots – the light of the stars and the natural rhythms of day and night. Light pollution makes us forget our true place in the universe and the vastness of our existence, which is best understood under a dark sky filled with the Milky Way, the edge of our galaxy, shining above us."
92,C1,"The founder of a large international company recently said that his business will stop tracking how much paid holiday time employees take. This decision seems to be inspired by a similar policy from an internet company. The founder mentioned that he got the idea from a cheerful email from his daughter, which many newspapers have shared. However, this way of announcing the change feels like an attempt to make a serious policy seem friendlier.
But is this idea practical? The internet company has 2,000 employees and offers one service, while the multinational corporation has 50,000 employees and many different services, like finance, transport, and healthcare. The idea of ""take as much time off as you want"" might work better in a smaller company where employees know each other's work schedules. In a big company, it can be hard to know if taking time off will hurt the business or their careers.
The founder said employees can take as much leave as they want, as long as they feel sure that their work and their team are up to date. But is it really possible to be that sure? No matter how much work you finish before a holiday, there will always be more waiting when you return. This means that employees might feel guilty about taking time off, which can lead to stress and lower productivity over time. 
There can also be pressure from coworkers and office gossip about who is taking time off and for how long. In many companies, especially in the corporate world, there is a culture of working late, which could lead to a situation where employees feel they cannot take holidays, even if they have unlimited leave. If the security of having a legal right to take leave is removed, workers might feel they cannot take the time they need for fear of being seen as lazy.
This policy could create a situation where employees do not feel they can take their entitled leave, or they might still rely on their legal rights, making the new policy useless. Modern technology allows us to receive work messages anytime and anywhere, which makes it hard to separate work from personal time. The internet company started their unlimited leave policy because employees wanted to know how to balance this new way of working with the old rules about time off.
However, having no set working hours can mean that employees feel they are always working. They might worry that their hours are being watched, which can lead to stress and unhealthy work habits. Employment laws exist to protect workers. They are entitled to a minimum amount of paid leave because rest is important for their mental and physical health. The benefits of unlimited leave, like better morale and creativity, can happen without sacrificing worker well-being. Therefore, I am not sure if allowing employees to take as much holiday as they want is really the goal or the likely result of this policy."
93,C1,"
Journal-based peer review is the process where experts in a field check a scientific research paper before it is published. This process is seen as a way to ensure the quality of research. It is believed to help prevent the publication of poor-quality or nonsensical papers. However, reviewing a paper can delay its publication by up to a year. Is this delay worth it to ensure the trustworthiness of published research? The answer is both yes and no. 
I still believe in journal-based peer review, but I see changes happening. The increasing use of preprints, which are drafts of papers shared online without peer review, is an important part of this change. Preprints allow researchers to quickly share new findings so that others can read, discuss, and build on them. Publishing in journals has become more about gaining recognition and advancing careers, which can affect the motivations of authors and reviewers. 
Competition for publication in top journals encourages scientists to produce high-quality work, and these journals do publish excellent research. However, the rewards for publishing in these journals can lead to shortcuts, where important data is left out to make the research look better. Reviewers often focus on whether a paper is good enough for a specific journal rather than its overall quality. For top journals, this can depend more on how interesting or newsworthy the research is rather than its scientific value.
These issues are well-known, but many people are hesitant to change the current system. However, as biologist Ron Vale recently suggested in a preprint, preprints could help solve these problems without completely changing the system. Although preprint archives have existed for twenty years, they are not widely used. This slow acceptance is partly due to scientists being cautious and the belief that journals will not accept papers that have been shared as preprints. There is also a concern that publishing papers without peer review could lead to poor-quality research, but this has not happened so far.
Preprints may not be peer-reviewed, but authors know that their work will be open to feedback and discussion from a global community of reviewers. Tanya Elks, a psychology professor, shared her experience: “My paper critiqued another published paper, which is not well handled by traditional journals. In their anonymous review system, the original authors might block a critical paper, or if they are not chosen as reviewers, they might complain about misrepresentation. By posting a preprint, the original authors could respond, and we could consider their feedback. All comments are public, so readers can judge the quality of the arguments. The risk of rejection by journals is less concerning because we still have the preprint and the comments available, so our work is not wasted.”
Preprint archives allow informal scientific discussions that used to happen only between individuals. They can also be a good way to share negative results, which are often ignored by journals that focus too much on new discoveries. Additionally, being on preprint archives increases the number of times papers are read and cited, showing how effective preprints can be for sharing research. By using the internet's culture of openness and collaboration, preprints can help shift the focus back to the quality of the work itself, rather than where it is published."
94,C1,"When I ask my literature students what a poem is, they often say things like ""a painting in words."" Their answers usually do not satisfy me or them. One day, I asked a group of students to pick an object and write two paragraphs about it. The first paragraph was a scientific description, and the second was from the object's point of view, titled ""Poem."" One student wrote: 
""Poem: I may look strange or scary, but I am a device that helps people breathe. I am only used in emergencies and for a short time. Most people will never need to use me."" 
The object was an oxygen mask, which helped the class understand how poetry works in a unique way. This exercise led to laughter and good discussions. 
When I was in school, I found poetry confusing. I thought every poem was a silly puzzle that made it hard to understand feelings. After school, most people pay less attention to poetry. Sometimes, you see a poem, and it stands out because it is not continuous prose. It challenges you to read it, but often you feel let down by its simplicity or because you don’t understand it at first. Still, you feel good for trying. 
What do we want to find in poems? We might expect deep feelings, beautiful images, thoughtful reflections, or sharp humor. The answer seems to be yes. But if we want tears, we watch movies; for information, we read articles. Novels let us escape, paintings please our eyes, and music is hard to beat with its mix of lyrics and melody. 
However, one thing that poems can offer is ambiguity, which is common in everyday life. But this still does not explain what a poem really is. If you look up ""poem"" online, it leads you to ""poetry,"" defined as a form of literary art that uses the beauty and rhythm of language. This is fine for English professors, but it misses the word's origins. ""Poem"" comes from the Greek word ""poí?ma,"" meaning ""a thing made,"" and a poet is someone who makes things. 
So, if a poem is a thing made, what kind of thing is it? Poets sometimes compare poems to wild animals—uncontrollable and unpredictable—or to machines—carefully designed and exact—depending on their views. But these comparisons often break down when examined closely. The most valuable part of trying to define a poem through comparison is the discussion it creates. Whether you see a poem as a machine or a wild animal, this process can change how you think about both. It can help us see ordinary things in a new way. 
Thinking of a poem as a mental object is not hard, especially since song lyrics often stick in our heads. The mix of words and melody has power, just like schoolyard rhymes such as ""Sticks and stones may break my bones, but words can never hurt me."" But aren’t words sometimes like sticks and stones? 
Think about a poem in a newspaper or magazine, right in front of you. A poem can touch you like nothing else, even though it is just ink on paper, like the prose around it. What about the empty space around the poem—space that could have been used for a longer article or an ad? A poem is written and rewritten like any article or story, but it does not become a product for sale. Publishers send out press releases and review copies of poetry collections, but few expect them to make money. A poem is not meant to be a product; it exists for its own sake. Because of its unique place in a magazine or book, a poem can still surprise us, even if just for a moment."
